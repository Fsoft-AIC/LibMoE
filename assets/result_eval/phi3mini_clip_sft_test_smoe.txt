g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.8667, 'grad_norm': 0.7956084489561719, 'learning_rate': 7.539108576140264e-09, 'flos': 49391916441600.0, 'epoch': 0.97, 'num_input_tokens_seen': 172475580}
 ... (more hidden) ...language loss:  0.8237848281860352
mlp auxiliary loss:  0.10162051022052765
clip auxiliary loss:  0.10162051022052765
language loss:  0.6263276934623718
mlp auxiliary loss:  0.10161112248897552
clip auxiliary loss:  0.10161112248897552
{'loss': 0.8809, 'grad_norm': 4.132938260506819, 'learning_rate': 7.471686893661732e-09, 'flos': 13098931077120.0, 'epoch': 0.97, 'num_input_tokens_seen': 172493595}
 ... (more hidden) ...language loss:  0.7560538053512573
mlp auxiliary loss:  0.10177432745695114
clip auxiliary loss:  0.10177432745695114
language loss:  0.7302577495574951
mlp auxiliary loss:  0.101680688560009
clip auxiliary loss:  0.101680688560009
{'loss': 0.8324, 'grad_norm': 3.5667618807148735, 'learning_rate': 7.4045674761442636e-09, 'flos': 14826984222720.0, 'epoch': 0.97, 'num_input_tokens_seen': 172510645}
 ... (more hidden) ...language loss:  0.8971186876296997
mlp auxiliary loss:  0.10155422240495682
clip auxiliary loss:  0.10155422240495682
language loss:  0.40025559067726135
mlp auxiliary loss:  0.10171347856521606
clip auxiliary loss:  0.10171347856521606
{'loss': 0.9235, 'grad_norm': 3.151946656129113, 'learning_rate': 7.337750333769488e-09, 'flos': 16898947891200.0, 'epoch': 0.97, 'num_input_tokens_seen': 172530170}
 ... (more hidden) ...language loss:  0.2015918642282486
mlp auxiliary loss:  0.10183551907539368
clip auxiliary loss:  0.10183551907539368
language loss:  0.8906632661819458
mlp auxiliary loss:  0.10188353061676025
clip auxiliary loss:  0.10188353061676025
{'loss': 0.9268, 'grad_norm': 3.6591493245420956, 'learning_rate': 7.2712354766737425e-09, 'flos': 25002733117440.0, 'epoch': 0.97, 'num_input_tokens_seen': 172550220}
 ... (more hidden) ...language loss:  0.8005557060241699
mlp auxiliary loss:  0.10170580446720123
clip auxiliary loss:  0.10170580446720123
language loss:  0.6319332122802734
mlp auxiliary loss:  0.10193586349487305
clip auxiliary loss:  0.10193586349487305
{'loss': 0.9998, 'grad_norm': 2.848011554587424, 'learning_rate': 7.2050229149469565e-09, 'flos': 14486997995520.0, 'epoch': 0.97, 'num_input_tokens_seen': 172569950}
 ... (more hidden) ...language loss:  0.33306804299354553
mlp auxiliary loss:  0.10161847621202469
clip auxiliary loss:  0.10161847621202469
language loss:  0.689261257648468
mlp auxiliary loss:  0.10183931887149811
clip auxiliary loss:  0.10183931887149811
{'loss': 0.8187, 'grad_norm': 3.3879687896335837, 'learning_rate': 7.139112658633984e-09, 'flos': 20596532674560.0, 'epoch': 0.97, 'num_input_tokens_seen': 172589820}
 ... (more hidden) ...language loss:  0.8337629437446594
mlp auxiliary loss:  0.10174160450696945
clip auxiliary loss:  0.10174160450696945
language loss:  0.6516510248184204
mlp auxiliary loss:  0.10175958275794983
clip auxiliary loss:  0.10175958275794983
{'loss': 0.8857, 'grad_norm': 4.294726025121903, 'learning_rate': 7.073504717733048e-09, 'flos': 19785117081600.0, 'epoch': 0.97, 'num_input_tokens_seen': 172609105}
 ... (more hidden) ...language loss:  0.6155750751495361
mlp auxiliary loss:  0.10035058110952377
clip auxiliary loss:  0.10035058110952377
language loss:  0.5662583112716675
mlp auxiliary loss:  0.10035058110952377
clip auxiliary loss:  0.10035058110952377
{'loss': 0.7826, 'grad_norm': 0.7255295003759892, 'learning_rate': 7.008199102196855e-09, 'flos': 49313682124800.0, 'epoch': 0.97, 'num_input_tokens_seen': 172670250}
 ... (more hidden) ...language loss:  0.6862936019897461
mlp auxiliary loss:  0.1003558337688446
clip auxiliary loss:  0.1003558337688446
language loss:  0.6215295195579529
mlp auxiliary loss:  0.1003558337688446
clip auxiliary loss:  0.1003558337688446
{'loss': 0.8103, 'grad_norm': 0.8050464459164469, 'learning_rate': 6.9431958219321464e-09, 'flos': 41675439820800.0, 'epoch': 0.97, 'num_input_tokens_seen': 172726135}
 ... (more hidden) ...language loss:  0.8764394521713257
mlp auxiliary loss:  0.10181304812431335
clip auxiliary loss:  0.10181304812431335
language loss:  0.6826258897781372
mlp auxiliary loss:  0.1016232818365097
clip auxiliary loss:  0.1016232818365097
{'loss': 0.9761, 'grad_norm': 3.0951574957837047, 'learning_rate': 6.878494886800146e-09, 'flos': 16060544655360.0, 'epoch': 0.97, 'num_input_tokens_seen': 172746630}
 ... (more hidden) ...language loss:  0.7712482810020447
mlp auxiliary loss:  0.10170236974954605
clip auxiliary loss:  0.10170236974954605
language loss:  0.6525061130523682
mlp auxiliary loss:  0.10162483900785446
clip auxiliary loss:  0.10162483900785446
{'loss': 0.9453, 'grad_norm': 3.2544106118905973, 'learning_rate': 6.814096306615669e-09, 'flos': 14198166466560.0, 'epoch': 0.97, 'num_input_tokens_seen': 172764490}
 ... (more hidden) ...language loss:  0.6163665056228638
mlp auxiliary loss:  0.10208244621753693
clip auxiliary loss:  0.10208244621753693
language loss:  0.7497673034667969
mlp auxiliary loss:  0.10179103165864944
clip auxiliary loss:  0.10179103165864944
{'loss': 0.8316, 'grad_norm': 3.26187838774763, 'learning_rate': 6.750000091148011e-09, 'flos': 12521298677760.0, 'epoch': 0.97, 'num_input_tokens_seen': 172781505}
 ... (more hidden) ...language loss:  0.2973569631576538
mlp auxiliary loss:  0.10164473205804825
clip auxiliary loss:  0.10164473205804825
language loss:  0.729106605052948
mlp auxiliary loss:  0.10195223987102509
clip auxiliary loss:  0.10195223987102509
[2024-09-18 20:26:09,926] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.9181, 'grad_norm': 5.2762008095702875, 'learning_rate': 6.686206250120729e-09, 'flos': 20990954065920.0, 'epoch': 0.97, 'num_input_tokens_seen': 172802720}
 ... (more hidden) ...language loss:  0.7385082840919495
mlp auxiliary loss:  0.10156025737524033
clip auxiliary loss:  0.10156025737524033
language loss:  0.5694522261619568
mlp auxiliary loss:  0.10196320712566376
clip auxiliary loss:  0.10196320712566376
{'loss': 0.9284, 'grad_norm': 14.181651250039058, 'learning_rate': 6.622714793210749e-09, 'flos': 13099329638400.0, 'epoch': 0.97, 'num_input_tokens_seen': 172821360}
 ... (more hidden) ...language loss:  0.7431579828262329
mlp auxiliary loss:  0.10180649161338806
clip auxiliary loss:  0.10180649161338806
language loss:  0.7664788961410522
mlp auxiliary loss:  0.10205458104610443
clip auxiliary loss:  0.10205458104610443
{'loss': 0.977, 'grad_norm': 2.9768029463763934, 'learning_rate': 6.559525730050364e-09, 'flos': 14669810442240.0, 'epoch': 0.97, 'num_input_tokens_seen': 172841180}
 ... (more hidden) ...language loss:  0.8038668036460876
mlp auxiliary loss:  0.10176511853933334
clip auxiliary loss:  0.10176511853933334
language loss:  0.6369728446006775
mlp auxiliary loss:  0.10168080031871796
clip auxiliary loss:  0.10168080031871796
{'loss': 0.9512, 'grad_norm': 3.2800171488802072, 'learning_rate': 6.496639070224574e-09, 'flos': 13098716467200.0, 'epoch': 0.98, 'num_input_tokens_seen': 172859385}
 ... (more hidden) ...language loss:  0.7966375946998596
mlp auxiliary loss:  0.10198087245225906
clip auxiliary loss:  0.10198087245225906
language loss:  0.7804906368255615
mlp auxiliary loss:  0.10169897973537445
clip auxiliary loss:  0.10169897973537445
{'loss': 1.0303, 'grad_norm': 6.715010769083416, 'learning_rate': 6.4340548232739714e-09, 'flos': 13885198540800.0, 'epoch': 0.98, 'num_input_tokens_seen': 172875305}
 ... (more hidden) ...language loss:  0.747255265712738
mlp auxiliary loss:  0.10207230597734451
clip auxiliary loss:  0.10207230597734451
language loss:  0.8897771239280701
mlp auxiliary loss:  0.101723313331604
clip auxiliary loss:  0.101723313331604
[2024-09-18 20:27:10,114] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.9825, 'grad_norm': 2.7206404679443863, 'learning_rate': 6.371772998692071e-09, 'flos': 16743705600000.0, 'epoch': 0.98, 'num_input_tokens_seen': 172894280}
 ... (more hidden) ...language loss:  0.7245643138885498
mlp auxiliary loss:  0.10185683518648148
clip auxiliary loss:  0.10185683518648148
language loss:  0.7447335720062256
mlp auxiliary loss:  0.10163021832704544
clip auxiliary loss:  0.10163021832704544
{'loss': 0.8342, 'grad_norm': 5.629973522821913, 'learning_rate': 6.309793605927094e-09, 'flos': 14410081996800.0, 'epoch': 0.98, 'num_input_tokens_seen': 172912320}
 ... (more hidden) ...language loss:  0.696052610874176
mlp auxiliary loss:  0.10149583220481873
clip auxiliary loss:  0.10149583220481873
language loss:  0.9241757392883301
mlp auxiliary loss:  0.10197228193283081
clip auxiliary loss:  0.10197228193283081
{'loss': 0.991, 'grad_norm': 4.283279494801946, 'learning_rate': 6.248116654381297e-09, 'flos': 13724989562880.0, 'epoch': 0.98, 'num_input_tokens_seen': 172930510}
 ... (more hidden) ...language loss:  0.7720367908477783
mlp auxiliary loss:  0.10151638090610504
clip auxiliary loss:  0.10151638090610504
language loss:  0.7153481245040894
mlp auxiliary loss:  0.10185196250677109
clip auxiliary loss:  0.10185196250677109
[2024-09-18 20:27:47,502] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.9114, 'grad_norm': 3.71896002681264, 'learning_rate': 6.186742153410751e-09, 'flos': 16767014215680.0, 'epoch': 0.98, 'num_input_tokens_seen': 172949725}
 ... (more hidden) ...language loss:  0.7982113361358643
mlp auxiliary loss:  0.10165902972221375
clip auxiliary loss:  0.10165902972221375
language loss:  0.9382660388946533
mlp auxiliary loss:  0.10152972489595413
clip auxiliary loss:  0.10152972489595413
{'loss': 1.0573, 'grad_norm': 4.965849407639691, 'learning_rate': 6.125670112326453e-09, 'flos': 16324473323520.0, 'epoch': 0.98, 'num_input_tokens_seen': 172968705}
 ... (more hidden) ...language loss:  0.811832070350647
mlp auxiliary loss:  0.10169630497694016
clip auxiliary loss:  0.10169630497694016
language loss:  0.5059311389923096
mlp auxiliary loss:  0.10167837142944336
clip auxiliary loss:  0.10167837142944336
{'loss': 0.8888, 'grad_norm': 8.144548579867655, 'learning_rate': 6.064900540392548e-09, 'flos': 19917296025600.0, 'epoch': 0.98, 'num_input_tokens_seen': 172990520}
 ... (more hidden) ...language loss:  0.9301657676696777
mlp auxiliary loss:  0.10142546892166138
clip auxiliary loss:  0.10142546892166138
language loss:  1.0288920402526855
mlp auxiliary loss:  0.10143114626407623
clip auxiliary loss:  0.10143114626407623
{'loss': 0.9818, 'grad_norm': 3.6644298637716433, 'learning_rate': 6.0044334468278835e-09, 'flos': 15773338030080.0, 'epoch': 0.98, 'num_input_tokens_seen': 173009585}
 ... (more hidden) ...language loss:  0.5091450214385986
mlp auxiliary loss:  0.1015615463256836
clip auxiliary loss:  0.1015615463256836
language loss:  0.919179379940033
mlp auxiliary loss:  0.10159098356962204
clip auxiliary loss:  0.10159098356962204
{'loss': 0.9096, 'grad_norm': 2.9886011687921754, 'learning_rate': 5.944268840805345e-09, 'flos': 18684502056960.0, 'epoch': 0.98, 'num_input_tokens_seen': 173030050}
 ... (more hidden) ...language loss:  0.29964974522590637
mlp auxiliary loss:  0.10154712200164795
clip auxiliary loss:  0.10154712200164795
language loss:  0.6235572695732117
mlp auxiliary loss:  0.10164690762758255
clip auxiliary loss:  0.10164690762758255
{'loss': 0.8335, 'grad_norm': 3.807756568645056, 'learning_rate': 5.88440673145163e-09, 'flos': 18917365493760.0, 'epoch': 0.98, 'num_input_tokens_seen': 173050820}
 ... (more hidden) ...language loss:  1.092808485031128
mlp auxiliary loss:  0.10155238211154938
clip auxiliary loss:  0.10155238211154938
language loss:  0.6918540000915527
mlp auxiliary loss:  0.10204444825649261
clip auxiliary loss:  0.10204444825649261
{'loss': 1.0138, 'grad_norm': 3.3008789048054235, 'learning_rate': 5.824847127848142e-09, 'flos': 12758484971520.0, 'epoch': 0.98, 'num_input_tokens_seen': 173069065}
 ... (more hidden) ...language loss:  0.7404351830482483
mlp auxiliary loss:  0.10213405638933182
clip auxiliary loss:  0.10213405638933182
language loss:  0.7285093069076538
mlp auxiliary loss:  0.10180903971195221
clip auxiliary loss:  0.10180903971195221
{'loss': 0.9693, 'grad_norm': 3.0511622856437777, 'learning_rate': 5.765590039029433e-09, 'flos': 15956978257920.0, 'epoch': 0.98, 'num_input_tokens_seen': 173088105}
 ... (more hidden) ...language loss:  0.39471495151519775
mlp auxiliary loss:  0.10216107219457626
clip auxiliary loss:  0.10216107219457626
language loss:  0.7710880637168884
mlp auxiliary loss:  0.10189405083656311
clip auxiliary loss:  0.10189405083656311
[2024-09-18 20:29:21,757] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.9078, 'grad_norm': 2.942418465080217, 'learning_rate': 5.706635473985422e-09, 'flos': 26235005890560.0, 'epoch': 0.98, 'num_input_tokens_seen': 173111695}
 ... (more hidden) ...language loss:  0.9390334486961365
mlp auxiliary loss:  0.10180337727069855
clip auxiliary loss:  0.10180337727069855
language loss:  0.8634899258613586
mlp auxiliary loss:  0.10161024332046509
clip auxiliary loss:  0.10161024332046509
{'loss': 1.0433, 'grad_norm': 3.7190560607623624, 'learning_rate': 5.6479834416591764e-09, 'flos': 15852308152320.0, 'epoch': 0.98, 'num_input_tokens_seen': 173130775}
 ... (more hidden) ...language loss:  0.9128748178482056
mlp auxiliary loss:  0.10183987021446228
clip auxiliary loss:  0.10183987021446228
language loss:  0.5768015384674072
mlp auxiliary loss:  0.10190510749816895
clip auxiliary loss:  0.10190510749816895
{'loss': 0.8778, 'grad_norm': 3.020586271158099, 'learning_rate': 5.589633950947803e-09, 'flos': 18368161689600.0, 'epoch': 0.98, 'num_input_tokens_seen': 173147995}
 ... (more hidden) ...language loss:  0.7069047093391418
mlp auxiliary loss:  0.10178736597299576
clip auxiliary loss:  0.10178736597299576
language loss:  0.5243293642997742
mlp auxiliary loss:  0.10218453407287598
clip auxiliary loss:  0.10218453407287598
{'loss': 0.8817, 'grad_norm': 7.068428994199837, 'learning_rate': 5.5315870107035535e-09, 'flos': 15196441436160.0, 'epoch': 0.98, 'num_input_tokens_seen': 173165765}
 ... (more hidden) ...language loss:  0.7957602143287659
mlp auxiliary loss:  0.10159941017627716
clip auxiliary loss:  0.10159941017627716
language loss:  0.826745867729187
mlp auxiliary loss:  0.10180538147687912
clip auxiliary loss:  0.10180538147687912
{'loss': 0.9667, 'grad_norm': 3.734676843678578, 'learning_rate': 5.473842629731607e-09, 'flos': 9873572782080.0, 'epoch': 0.98, 'num_input_tokens_seen': 173183985}
 ... (more hidden) ...language loss:  0.799065351486206
mlp auxiliary loss:  0.10181073099374771
clip auxiliary loss:  0.10181073099374771
language loss:  1.0018097162246704
mlp auxiliary loss:  0.1019284799695015
clip auxiliary loss:  0.1019284799695015
{'loss': 0.9729, 'grad_norm': 4.110469309796309, 'learning_rate': 5.416400816792066e-09, 'flos': 12705981296640.0, 'epoch': 0.98, 'num_input_tokens_seen': 173201220}
 ... (more hidden) ...language loss:  0.8968784213066101
mlp auxiliary loss:  0.10152053833007812
clip auxiliary loss:  0.10152053833007812
language loss:  0.6410643458366394
mlp auxiliary loss:  0.10186025500297546
clip auxiliary loss:  0.10186025500297546
{'loss': 0.9561, 'grad_norm': 5.867571090573142, 'learning_rate': 5.359261580598407e-09, 'flos': 14512422051840.0, 'epoch': 0.98, 'num_input_tokens_seen': 173216780}
 ... (more hidden) ...language loss:  0.5766662955284119
mlp auxiliary loss:  0.1016043946146965
clip auxiliary loss:  0.1016043946146965
language loss:  0.8006903529167175
mlp auxiliary loss:  0.10183495283126831
clip auxiliary loss:  0.10183495283126831
{'loss': 0.9697, 'grad_norm': 5.0583545053825585, 'learning_rate': 5.302424929819027e-09, 'flos': 8325634129920.0, 'epoch': 0.98, 'num_input_tokens_seen': 173230510}
 ... (more hidden) ...language loss:  0.8330194354057312
mlp auxiliary loss:  0.10183248668909073
clip auxiliary loss:  0.10183248668909073
language loss:  0.7707608938217163
mlp auxiliary loss:  0.1016879603266716
clip auxiliary loss:  0.1016879603266716
{'loss': 0.9143, 'grad_norm': 6.349231775626481, 'learning_rate': 5.24589087307592e-09, 'flos': 9506261667840.0, 'epoch': 0.98, 'num_input_tokens_seen': 173247850}
 ... (more hidden) ...language loss:  0.25024470686912537
mlp auxiliary loss:  0.10186693072319031
clip auxiliary loss:  0.10186693072319031
language loss:  0.5936100482940674
mlp auxiliary loss:  0.10179124027490616
clip auxiliary loss:  0.10179124027490616
[2024-09-18 20:31:09,496] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.8445, 'grad_norm': 2.501694200564575, 'learning_rate': 5.189659418944891e-09, 'flos': 42604226273280.0, 'epoch': 0.98, 'num_input_tokens_seen': 173277745}
 ... (more hidden) ...language loss:  0.8824512362480164
mlp auxiliary loss:  0.10180553793907166
clip auxiliary loss:  0.10180553793907166
language loss:  0.6827753186225891
mlp auxiliary loss:  0.10182268917560577
clip auxiliary loss:  0.10182268917560577
{'loss': 0.9643, 'grad_norm': 4.0610184535565645, 'learning_rate': 5.133730575956674e-09, 'flos': 15039175680000.0, 'epoch': 0.98, 'num_input_tokens_seen': 173297135}
 ... (more hidden) ...language loss:  0.8858287334442139
mlp auxiliary loss:  0.1017545759677887
clip auxiliary loss:  0.1017545759677887
language loss:  0.6680481433868408
mlp auxiliary loss:  0.10188239067792892
clip auxiliary loss:  0.10188239067792892
[2024-09-18 20:31:33,860] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.9071, 'grad_norm': 3.270232386366425, 'learning_rate': 5.0781043525953696e-09, 'flos': 14829467566080.0, 'epoch': 0.98, 'num_input_tokens_seen': 173314920}
 ... (more hidden) ...language loss:  0.8776683211326599
mlp auxiliary loss:  0.10179638862609863
clip auxiliary loss:  0.10179638862609863
language loss:  0.55963534116745
mlp auxiliary loss:  0.10181812196969986
clip auxiliary loss:  0.10181812196969986
{'loss': 0.9191, 'grad_norm': 4.927632576113583, 'learning_rate': 5.0227807572995605e-09, 'flos': 16664827453440.0, 'epoch': 0.98, 'num_input_tokens_seen': 173336615}
 ... (more hidden) ...language loss:  0.6714997291564941
mlp auxiliary loss:  0.10200578719377518
clip auxiliary loss:  0.10200578719377518
language loss:  0.8306176662445068
mlp auxiliary loss:  0.10168850421905518
clip auxiliary loss:  0.10168850421905518
{'loss': 0.8678, 'grad_norm': 4.392668007778701, 'learning_rate': 4.967759798461646e-09, 'flos': 14855658086400.0, 'epoch': 0.98, 'num_input_tokens_seen': 173354680}
 ... (more hidden) ...language loss:  0.13797177374362946
mlp auxiliary loss:  0.10159467905759811
clip auxiliary loss:  0.10159467905759811
language loss:  0.8478021025657654
mlp auxiliary loss:  0.10181838274002075
clip auxiliary loss:  0.10181838274002075
[2024-09-18 20:32:10,729] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.9375, 'grad_norm': 4.343260154273538, 'learning_rate': 4.913041484428282e-09, 'flos': 20152274903040.0, 'epoch': 0.98, 'num_input_tokens_seen': 173374875}
 ... (more hidden) ...language loss:  0.13066215813159943
mlp auxiliary loss:  0.10175728797912598
clip auxiliary loss:  0.10175728797912598
language loss:  0.7763561010360718
mlp auxiliary loss:  0.10194355249404907
clip auxiliary loss:  0.10194355249404907
{'loss': 0.9453, 'grad_norm': 7.791093965042338, 'learning_rate': 4.858625823500384e-09, 'flos': 18182406021120.0, 'epoch': 0.98, 'num_input_tokens_seen': 173392295}
 ... (more hidden) ...language loss:  0.4096537232398987
mlp auxiliary loss:  0.1017867922782898
clip auxiliary loss:  0.1017867922782898
language loss:  0.7724577188491821
mlp auxiliary loss:  0.10208828747272491
clip auxiliary loss:  0.10208828747272491
{'loss': 0.9339, 'grad_norm': 4.917538818764689, 'learning_rate': 4.80451282393246e-09, 'flos': 21353697054720.0, 'epoch': 0.98, 'num_input_tokens_seen': 173412000}
 ... (more hidden) ...language loss:  0.22674718499183655
mlp auxiliary loss:  0.10159485787153244
clip auxiliary loss:  0.10159485787153244
language loss:  0.5646200180053711
mlp auxiliary loss:  0.1015147790312767
clip auxiliary loss:  0.1015147790312767
[2024-09-18 20:32:46,221] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.8707, 'grad_norm': 4.250654506774446, 'learning_rate': 4.750702493933722e-09, 'flos': 23063285637120.0, 'epoch': 0.98, 'num_input_tokens_seen': 173431605}
 ... (more hidden) ...language loss:  0.7488747239112854
mlp auxiliary loss:  0.10160307586193085
clip auxiliary loss:  0.10160307586193085
language loss:  0.6909518241882324
mlp auxiliary loss:  0.1017908975481987
clip auxiliary loss:  0.1017908975481987
{'loss': 1.0395, 'grad_norm': 3.8477770791227703, 'learning_rate': 4.697194841666974e-09, 'flos': 16586378526720.0, 'epoch': 0.98, 'num_input_tokens_seen': 173450250}
 ... (more hidden) ...language loss:  0.9326826333999634
mlp auxiliary loss:  0.1017327755689621
clip auxiliary loss:  0.1017327755689621
language loss:  0.89897620677948
mlp auxiliary loss:  0.10180948674678802
clip auxiliary loss:  0.10180948674678802
{'loss': 1.0091, 'grad_norm': 8.428796418814741, 'learning_rate': 4.6439898752492764e-09, 'flos': 15247289548800.0, 'epoch': 0.98, 'num_input_tokens_seen': 173470110}
 ... (more hidden) ...language loss:  0.655510425567627
mlp auxiliary loss:  0.10035787522792816
clip auxiliary loss:  0.10035787522792816
language loss:  0.6821247935295105
mlp auxiliary loss:  0.10035787522792816
clip auxiliary loss:  0.10035787522792816
{'loss': 0.829, 'grad_norm': 0.7423469372204974, 'learning_rate': 4.591087602751731e-09, 'flos': 49338125107200.0, 'epoch': 0.98, 'num_input_tokens_seen': 173531690}
 ... (more hidden) ...language loss:  0.7360946536064148
mlp auxiliary loss:  0.10183213651180267
clip auxiliary loss:  0.10183213651180267
language loss:  0.811994731426239
mlp auxiliary loss:  0.10163813084363937
clip auxiliary loss:  0.10163813084363937
{'loss': 0.9132, 'grad_norm': 2.9535291284222955, 'learning_rate': 4.538488032199916e-09, 'flos': 15220056637440.0, 'epoch': 0.98, 'num_input_tokens_seen': 173549510}
 ... (more hidden) ...language loss:  0.7394677996635437
mlp auxiliary loss:  0.10157376527786255
clip auxiliary loss:  0.10157376527786255
language loss:  0.8539310693740845
mlp auxiliary loss:  0.10161051899194717
clip auxiliary loss:  0.10161051899194717
{'loss': 0.8694, 'grad_norm': 4.547780998654042, 'learning_rate': 4.486191171572784e-09, 'flos': 14301579571200.0, 'epoch': 0.98, 'num_input_tokens_seen': 173566500}
 ... (more hidden) ...language loss:  0.8425529599189758
mlp auxiliary loss:  0.10214748978614807
clip auxiliary loss:  0.10214748978614807
language loss:  0.8234732747077942
mlp auxiliary loss:  0.10184246301651001
clip auxiliary loss:  0.10184246301651001
{'loss': 0.9738, 'grad_norm': 2.403329042421162, 'learning_rate': 4.434197028803766e-09, 'flos': 16870917857280.0, 'epoch': 0.98, 'num_input_tokens_seen': 173585445}
 ... (more hidden) ...language loss:  0.8179796934127808
mlp auxiliary loss:  0.10166119784116745
clip auxiliary loss:  0.10166119784116745
language loss:  0.6900853514671326
mlp auxiliary loss:  0.10170231759548187
clip auxiliary loss:  0.10170231759548187
{'loss': 1.0031, 'grad_norm': 4.597617461138946, 'learning_rate': 4.3825056117805514e-09, 'flos': 16375934607360.0, 'epoch': 0.98, 'num_input_tokens_seen': 173601050}
 ... (more hidden) ...language loss:  0.9392915368080139
mlp auxiliary loss:  0.10170391201972961
clip auxiliary loss:  0.10170391201972961
language loss:  0.7759689688682556
mlp auxiliary loss:  0.10175320506095886
clip auxiliary loss:  0.10175320506095886
{'loss': 0.9882, 'grad_norm': 4.262906392432812, 'learning_rate': 4.331116928344425e-09, 'flos': 10109134172160.0, 'epoch': 0.98, 'num_input_tokens_seen': 173617085}
 ... (more hidden) ...language loss:  0.730743944644928
mlp auxiliary loss:  0.10191824287176132
clip auxiliary loss:  0.10191824287176132
language loss:  0.5697262287139893
mlp auxiliary loss:  0.10172227025032043
clip auxiliary loss:  0.10172227025032043
{'loss': 0.819, 'grad_norm': 3.637493178108871, 'learning_rate': 4.28003098629115e-09, 'flos': 11840283832320.0, 'epoch': 0.98, 'num_input_tokens_seen': 173632940}
 ... (more hidden) ...language loss:  0.8123552203178406
mlp auxiliary loss:  0.10150081664323807
clip auxiliary loss:  0.10150081664323807
language loss:  0.797607421875
mlp auxiliary loss:  0.10167447477579117
clip auxiliary loss:  0.10167447477579117
[2024-09-18 20:34:44,642] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.9872, 'grad_norm': 4.2681289422132505, 'learning_rate': 4.229247793370305e-09, 'flos': 17449316720640.0, 'epoch': 0.98, 'num_input_tokens_seen': 173651785}
 ... (more hidden) ...language loss:  0.675460934638977
mlp auxiliary loss:  0.10225000977516174
clip auxiliary loss:  0.10225000977516174
language loss:  0.911584198474884
mlp auxiliary loss:  0.10204808413982391
clip auxiliary loss:  0.10204808413982391
{'loss': 0.8919, 'grad_norm': 3.588569024604117, 'learning_rate': 4.178767357285951e-09, 'flos': 19444916244480.0, 'epoch': 0.98, 'num_input_tokens_seen': 173673135}
 ... (more hidden) ...language loss:  0.6948042511940002
mlp auxiliary loss:  0.10206528007984161
clip auxiliary loss:  0.10206528007984161
language loss:  0.7568150758743286
mlp auxiliary loss:  0.10168865323066711
clip auxiliary loss:  0.10168865323066711
{'loss': 0.8897, 'grad_norm': 3.113587406282351, 'learning_rate': 4.128589685695516e-09, 'flos': 18710171381760.0, 'epoch': 0.98, 'num_input_tokens_seen': 173693280}
 ... (more hidden) ...language loss:  0.9229686260223389
mlp auxiliary loss:  0.1017814576625824
clip auxiliary loss:  0.1017814576625824
language loss:  0.7326335310935974
mlp auxiliary loss:  0.10186628997325897
clip auxiliary loss:  0.10186628997325897
{'loss': 1.0439, 'grad_norm': 3.3313385398793014, 'learning_rate': 4.078714786211135e-09, 'flos': 11836972707840.0, 'epoch': 0.98, 'num_input_tokens_seen': 173708850}
 ... (more hidden) ...language loss:  0.6576834321022034
mlp auxiliary loss:  0.10167517513036728
clip auxiliary loss:  0.10167517513036728
language loss:  0.46017494797706604
mlp auxiliary loss:  0.1016106903553009
clip auxiliary loss:  0.1016106903553009
{'loss': 0.9597, 'grad_norm': 2.7965270097650077, 'learning_rate': 4.029142666398977e-09, 'flos': 17714042511360.0, 'epoch': 0.98, 'num_input_tokens_seen': 173728735}
 ... (more hidden) ...language loss:  0.9007863402366638
mlp auxiliary loss:  0.10171753913164139
clip auxiliary loss:  0.10171753913164139
language loss:  0.522773265838623
mlp auxiliary loss:  0.10162291675806046
clip auxiliary loss:  0.10162291675806046
{'loss': 0.991, 'grad_norm': 3.0350381050043054, 'learning_rate': 3.979873333778805e-09, 'flos': 16035488501760.0, 'epoch': 0.98, 'num_input_tokens_seen': 173746630}
 ... (more hidden) ...language loss:  0.1261039823293686
mlp auxiliary loss:  0.10158716142177582
clip auxiliary loss:  0.10158716142177582
language loss:  0.7521995306015015
mlp auxiliary loss:  0.10182816535234451
clip auxiliary loss:  0.10182816535234451
[2024-09-18 20:35:57,114] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.9265, 'grad_norm': 3.65148315082305, 'learning_rate': 3.930906795824862e-09, 'flos': 27778345758720.0, 'epoch': 0.98, 'num_input_tokens_seen': 173767025}
 ... (more hidden) ...language loss:  0.6240741610527039
mlp auxiliary loss:  0.10169344395399094
clip auxiliary loss:  0.10169344395399094
language loss:  0.9424092769622803
mlp auxiliary loss:  0.10194580256938934
clip auxiliary loss:  0.10194580256938934
{'loss': 0.9683, 'grad_norm': 4.8870807516671375, 'learning_rate': 3.882243059965207e-09, 'flos': 12626857881600.0, 'epoch': 0.98, 'num_input_tokens_seen': 173784460}
 ... (more hidden) ...language loss:  0.6680580377578735
mlp auxiliary loss:  0.10197966545820236
clip auxiliary loss:  0.10197966545820236
language loss:  0.5521990060806274
mlp auxiliary loss:  0.10178307443857193
clip auxiliary loss:  0.10178307443857193
{'loss': 0.8553, 'grad_norm': 3.8191605217723508, 'learning_rate': 3.833882133582156e-09, 'flos': 9558765342720.0, 'epoch': 0.98, 'num_input_tokens_seen': 173799840}
 ... (more hidden) ...language loss:  0.8182685971260071
mlp auxiliary loss:  0.1017606109380722
clip auxiliary loss:  0.1017606109380722
language loss:  0.814166784286499
mlp auxiliary loss:  0.10181126743555069
clip auxiliary loss:  0.10181126743555069
{'loss': 0.9658, 'grad_norm': 5.184118248650508, 'learning_rate': 3.785824024012285e-09, 'flos': 15406609428480.0, 'epoch': 0.98, 'num_input_tokens_seen': 173818560}
 ... (more hidden) ...language loss:  0.8631603121757507
mlp auxiliary loss:  0.1015901193022728
clip auxiliary loss:  0.1015901193022728
language loss:  0.5812548995018005
mlp auxiliary loss:  0.10154089331626892
clip auxiliary loss:  0.10154089331626892
{'loss': 0.9712, 'grad_norm': 2.5399810537998326, 'learning_rate': 3.738068738545541e-09, 'flos': 16560096030720.0, 'epoch': 0.98, 'num_input_tokens_seen': 173837365}
 ... (more hidden) ...language loss:  0.6401666402816772
mlp auxiliary loss:  0.1020582988858223
clip auxiliary loss:  0.1020582988858223
language loss:  0.9900712370872498
mlp auxiliary loss:  0.1015954464673996
clip auxiliary loss:  0.1015954464673996
{'loss': 0.9701, 'grad_norm': 4.972987065844397, 'learning_rate': 3.6906162844265733e-09, 'flos': 12993310556160.0, 'epoch': 0.98, 'num_input_tokens_seen': 173854170}
 ... (more hidden) ...language loss:  0.8237859606742859
mlp auxiliary loss:  0.10168848931789398
clip auxiliary loss:  0.10168848931789398
language loss:  0.8409532308578491
mlp auxiliary loss:  0.1018642708659172
clip auxiliary loss:  0.1018642708659172
{'loss': 0.9059, 'grad_norm': 3.447460102074498, 'learning_rate': 3.643466668853845e-09, 'flos': 16062169559040.0, 'epoch': 0.98, 'num_input_tokens_seen': 173871915}
 ... (more hidden) ...language loss:  0.6524198055267334
mlp auxiliary loss:  0.10199226438999176
clip auxiliary loss:  0.10199226438999176
language loss:  0.32128235697746277
mlp auxiliary loss:  0.10161431133747101
clip auxiliary loss:  0.10161431133747101
{'loss': 0.9347, 'grad_norm': 4.567650800782676, 'learning_rate': 3.59661989898008e-09, 'flos': 18082917212160.0, 'epoch': 0.98, 'num_input_tokens_seen': 173892690}
 ... (more hidden) ...language loss:  0.6323961615562439
mlp auxiliary loss:  0.10146552324295044
clip auxiliary loss:  0.10146552324295044
language loss:  0.9153795838356018
mlp auxiliary loss:  0.10184149444103241
clip auxiliary loss:  0.10184149444103241
{'loss': 0.9609, 'grad_norm': 3.1071787990251427, 'learning_rate': 3.5500759819115934e-09, 'flos': 17790375997440.0, 'epoch': 0.98, 'num_input_tokens_seen': 173912775}
 ... (more hidden) ...language loss:  0.9924512505531311
mlp auxiliary loss:  0.10168850421905518
clip auxiliary loss:  0.10168850421905518
language loss:  0.9236899614334106
mlp auxiliary loss:  0.10220833122730255
clip auxiliary loss:  0.10220833122730255
{'loss': 1.0123, 'grad_norm': 7.338336653831759, 'learning_rate': 3.5038349247094034e-09, 'flos': 14667633684480.0, 'epoch': 0.98, 'num_input_tokens_seen': 173929755}
 ... (more hidden) ...language loss:  0.7038294076919556
mlp auxiliary loss:  0.10188913345336914
clip auxiliary loss:  0.10188913345336914
language loss:  0.5278650522232056
mlp auxiliary loss:  0.10186352580785751
clip auxiliary loss:  0.10186352580785751
{'loss': 0.964, 'grad_norm': 3.97948703508102, 'learning_rate': 3.4578967343878994e-09, 'flos': 12547918417920.0, 'epoch': 0.98, 'num_input_tokens_seen': 173945680}
 ... (more hidden) ...language loss:  0.9297208786010742
mlp auxiliary loss:  0.10208199918270111
clip auxiliary loss:  0.10208199918270111
language loss:  0.6773655414581299
mlp auxiliary loss:  0.10163190960884094
clip auxiliary loss:  0.10163190960884094
{'loss': 0.999, 'grad_norm': 3.47849369202799, 'learning_rate': 3.4122614179161733e-09, 'flos': 16010616299520.0, 'epoch': 0.98, 'num_input_tokens_seen': 173965360}
 ... (more hidden) ...language loss:  0.7272967100143433
mlp auxiliary loss:  0.10194055736064911
clip auxiliary loss:  0.10194055736064911
language loss:  1.016054630279541
mlp auxiliary loss:  0.10184335708618164
clip auxiliary loss:  0.10184335708618164
{'loss': 0.9557, 'grad_norm': 2.397884242956872, 'learning_rate': 3.36692898221691e-09, 'flos': 14200220590080.0, 'epoch': 0.98, 'num_input_tokens_seen': 173983445}
 ... (more hidden) ...language loss:  1.0352269411087036
mlp auxiliary loss:  0.10189490765333176
clip auxiliary loss:  0.10189490765333176
language loss:  0.7517309188842773
mlp auxiliary loss:  0.10155193507671356
clip auxiliary loss:  0.10155193507671356
{'loss': 0.9204, 'grad_norm': 3.075137164770593, 'learning_rate': 3.3218994341668305e-09, 'flos': 13335013662720.0, 'epoch': 0.98, 'num_input_tokens_seen': 174002095}
 ... (more hidden) ...language loss:  0.8511064648628235
mlp auxiliary loss:  0.10190439224243164
clip auxiliary loss:  0.10190439224243164
language loss:  0.7027174830436707
mlp auxiliary loss:  0.10226896405220032
clip auxiliary loss:  0.10226896405220032
{'loss': 0.9408, 'grad_norm': 2.627893669479806, 'learning_rate': 3.2771727805971373e-09, 'flos': 18919572910080.0, 'epoch': 0.98, 'num_input_tokens_seen': 174023200}
 ... (more hidden) ...language loss:  0.8926777243614197
mlp auxiliary loss:  0.1018720343708992
clip auxiliary loss:  0.1018720343708992
language loss:  0.8860374093055725
mlp auxiliary loss:  0.10200180113315582
clip auxiliary loss:  0.10200180113315582
{'loss': 0.9629, 'grad_norm': 2.916479504347628, 'learning_rate': 3.232749028292847e-09, 'flos': 15639442206720.0, 'epoch': 0.98, 'num_input_tokens_seen': 174039885}
 ... (more hidden) ...language loss:  0.8596325516700745
mlp auxiliary loss:  0.10204894095659256
clip auxiliary loss:  0.10204894095659256
language loss:  0.8697236776351929
mlp auxiliary loss:  0.10154454410076141
clip auxiliary loss:  0.10154454410076141
[2024-09-18 20:39:05,204] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 1.0746, 'grad_norm': 3.1545768306491997, 'learning_rate': 3.188628183992792e-09, 'flos': 15563476623360.0, 'epoch': 0.98, 'num_input_tokens_seen': 174059870}
 ... (more hidden) ...language loss:  0.8961433172225952
mlp auxiliary loss:  0.10037568211555481
clip auxiliary loss:  0.10037568211555481
language loss:  0.5942988991737366
mlp auxiliary loss:  0.10037568211555481
clip auxiliary loss:  0.10037568211555481
[2024-09-18 20:39:18,284] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.849, 'grad_norm': 0.7789299658069909, 'learning_rate': 3.1448102543902844e-09, 'flos': 42581890621440.0, 'epoch': 0.98, 'num_input_tokens_seen': 174123505}
 ... (more hidden) ...language loss:  0.8319649696350098
mlp auxiliary loss:  0.10188084840774536
clip auxiliary loss:  0.10188084840774536
language loss:  0.8448076844215393
mlp auxiliary loss:  0.10188166797161102
clip auxiliary loss:  0.10188166797161102
{'loss': 0.8573, 'grad_norm': 4.009327027887939, 'learning_rate': 3.1012952461324515e-09, 'flos': 11368517222400.0, 'epoch': 0.98, 'num_input_tokens_seen': 174142200}
 ... (more hidden) ...language loss:  0.36374396085739136
mlp auxiliary loss:  0.10194393992424011
clip auxiliary loss:  0.10194393992424011
language loss:  1.0283223390579224
mlp auxiliary loss:  0.10173211246728897
clip auxiliary loss:  0.10173211246728897
{'loss': 0.9387, 'grad_norm': 4.411522058284141, 'learning_rate': 3.0580831658204575e-09, 'flos': 14380488376320.0, 'epoch': 0.98, 'num_input_tokens_seen': 174159500}
 ... (more hidden) ...language loss:  0.5804266929626465
mlp auxiliary loss:  0.10194562375545502
clip auxiliary loss:  0.10194562375545502
language loss:  0.6948372721672058
mlp auxiliary loss:  0.10181675106287003
clip auxiliary loss:  0.10181675106287003
{'loss': 0.9763, 'grad_norm': 5.899633829424421, 'learning_rate': 3.015174020009281e-09, 'flos': 15353584558080.0, 'epoch': 0.98, 'num_input_tokens_seen': 174178545}
 ... (more hidden) ...language loss:  0.813205897808075
mlp auxiliary loss:  0.10147510468959808
clip auxiliary loss:  0.10147510468959808
language loss:  0.6623983979225159
mlp auxiliary loss:  0.10204818099737167
clip auxiliary loss:  0.10204818099737167
{'loss': 0.9428, 'grad_norm': 3.788772414596439, 'learning_rate': 2.9725678152086043e-09, 'flos': 16896740474880.0, 'epoch': 0.98, 'num_input_tokens_seen': 174196835}
 ... (more hidden) ...language loss:  0.8038623332977295
mlp auxiliary loss:  0.10152465105056763
clip auxiliary loss:  0.10152465105056763
language loss:  0.9484617710113525
mlp auxiliary loss:  0.1020166277885437
clip auxiliary loss:  0.1020166277885437
{'loss': 1.0094, 'grad_norm': 5.392171355991299, 'learning_rate': 2.930264557881257e-09, 'flos': 7953754890240.0, 'epoch': 0.98, 'num_input_tokens_seen': 174211740}
 ... (more hidden) ...language loss:  0.6210135221481323
mlp auxiliary loss:  0.10034245252609253
clip auxiliary loss:  0.10034245252609253
language loss:  0.611811637878418
mlp auxiliary loss:  0.10034245252609253
clip auxiliary loss:  0.10034245252609253
[2024-09-18 20:40:30,570] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.8208, 'grad_norm': 0.8009325210231973, 'learning_rate': 2.8882642544452163e-09, 'flos': 42944633610240.0, 'epoch': 0.98, 'num_input_tokens_seen': 174276185}
 ... (more hidden) ...language loss:  0.7943441867828369
mlp auxiliary loss:  0.10151326656341553
clip auxiliary loss:  0.10151326656341553
language loss:  0.6835355758666992
mlp auxiliary loss:  0.10183987021446228
clip auxiliary loss:  0.10183987021446228
{'loss': 0.9348, 'grad_norm': 3.2683533582863378, 'learning_rate': 2.8465669112716083e-09, 'flos': 9611851530240.0, 'epoch': 0.98, 'num_input_tokens_seen': 174293430}
 ... (more hidden) ...language loss:  0.8509040474891663
mlp auxiliary loss:  0.10195336490869522
clip auxiliary loss:  0.10195336490869522
language loss:  0.902036190032959
mlp auxiliary loss:  0.1017182469367981
clip auxiliary loss:  0.1017182469367981
{'loss': 0.9391, 'grad_norm': 6.046477548711342, 'learning_rate': 2.8051725346858177e-09, 'flos': 16295186288640.0, 'epoch': 0.98, 'num_input_tokens_seen': 174313410}
 ... (more hidden) ...language loss:  0.3577784299850464
mlp auxiliary loss:  0.1019207090139389
clip auxiliary loss:  0.1019207090139389
language loss:  0.7565001845359802
mlp auxiliary loss:  0.10170196741819382
clip auxiliary loss:  0.10170196741819382
{'loss': 0.8913, 'grad_norm': 4.417097442540127, 'learning_rate': 2.7640811309674883e-09, 'flos': 19706913423360.0, 'epoch': 0.98, 'num_input_tokens_seen': 174332630}
 ... (more hidden) ...language loss:  0.8155795335769653
mlp auxiliary loss:  0.10185851156711578
clip auxiliary loss:  0.10185851156711578
language loss:  0.5634671449661255
mlp auxiliary loss:  0.10144782066345215
clip auxiliary loss:  0.10144782066345215
{'loss': 1.0004, 'grad_norm': 2.4607052648047953, 'learning_rate': 2.7232927063498557e-09, 'flos': 20834148188160.0, 'epoch': 0.98, 'num_input_tokens_seen': 174352725}
 ... (more hidden) ...language loss:  0.2705100476741791
mlp auxiliary loss:  0.10184809565544128
clip auxiliary loss:  0.10184809565544128
language loss:  0.5784624218940735
mlp auxiliary loss:  0.10164353251457214
clip auxiliary loss:  0.10164353251457214
[2024-09-18 20:41:29,780] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.8725, 'grad_norm': 3.4802909146311127, 'learning_rate': 2.682807267020859e-09, 'flos': 28644349808640.0, 'epoch': 0.98, 'num_input_tokens_seen': 174375205}
 ... (more hidden) ...language loss:  0.9014992117881775
mlp auxiliary loss:  0.10180032253265381
clip auxiliary loss:  0.10180032253265381
language loss:  0.6360332369804382
mlp auxiliary loss:  0.10163562744855881
clip auxiliary loss:  0.10163562744855881
{'loss': 0.8172, 'grad_norm': 2.6321527124121693, 'learning_rate': 2.642624819121808e-09, 'flos': 17188453908480.0, 'epoch': 0.98, 'num_input_tokens_seen': 174395075}
 ... (more hidden) ...language loss:  0.8500993251800537
mlp auxiliary loss:  0.10172230750322342
clip auxiliary loss:  0.10172230750322342
language loss:  0.5682904124259949
mlp auxiliary loss:  0.10165491700172424
clip auxiliary loss:  0.10165491700172424
{'loss': 0.8054, 'grad_norm': 2.9162537055353788, 'learning_rate': 2.6027453687487154e-09, 'flos': 10372020449280.0, 'epoch': 0.98, 'num_input_tokens_seen': 174411885}
 ... (more hidden) ...language loss:  0.18441249430179596
mlp auxiliary loss:  0.10177815705537796
clip auxiliary loss:  0.10177815705537796
language loss:  0.7438497543334961
mlp auxiliary loss:  0.1020776554942131
clip auxiliary loss:  0.1020776554942131
{'loss': 0.7022, 'grad_norm': 4.039779746972938, 'learning_rate': 2.5631689219509643e-09, 'flos': 15877088378880.0, 'epoch': 0.98, 'num_input_tokens_seen': 174430285}
 ... (more hidden) ...language loss:  0.7489490509033203
mlp auxiliary loss:  0.10168520361185074
clip auxiliary loss:  0.10168520361185074
language loss:  0.7540951371192932
mlp auxiliary loss:  0.10182171314954758
clip auxiliary loss:  0.10182171314954758
{'loss': 1.03, 'grad_norm': 3.1135682628249066, 'learning_rate': 2.523895484732197e-09, 'flos': 15486008770560.0, 'epoch': 0.98, 'num_input_tokens_seen': 174449460}
 ... (more hidden) ...language loss:  0.7247669696807861
mlp auxiliary loss:  0.1018863171339035
clip auxiliary loss:  0.1018863171339035
language loss:  0.6106788516044617
mlp auxiliary loss:  0.10173933207988739
clip auxiliary loss:  0.10173933207988739
{'loss': 0.9457, 'grad_norm': 3.2674056313743054, 'learning_rate': 2.4849250630505357e-09, 'flos': 12779463536640.0, 'epoch': 0.98, 'num_input_tokens_seen': 174467425}
 ... (more hidden) ...language loss:  0.830488383769989
mlp auxiliary loss:  0.1018078476190567
clip auxiliary loss:  0.1018078476190567
language loss:  0.4403184652328491
mlp auxiliary loss:  0.10172002017498016
clip auxiliary loss:  0.10172002017498016
{'loss': 0.9206, 'grad_norm': 2.8131980029848243, 'learning_rate': 2.4462576628172528e-09, 'flos': 17949849169920.0, 'epoch': 0.98, 'num_input_tokens_seen': 174485775}
 ... (more hidden) ...language loss:  0.6996665000915527
mlp auxiliary loss:  0.10161834955215454
clip auxiliary loss:  0.10161834955215454
language loss:  0.6526772975921631
mlp auxiliary loss:  0.1017606183886528
clip auxiliary loss:  0.1017606183886528
{'loss': 0.9359, 'grad_norm': 3.515123255211489, 'learning_rate': 2.407893289898766e-09, 'flos': 13151680020480.0, 'epoch': 0.98, 'num_input_tokens_seen': 174504525}
 ... (more hidden) ...language loss:  0.8920103907585144
mlp auxiliary loss:  0.1016034483909607
clip auxiliary loss:  0.1016034483909607
language loss:  0.6949235200881958
mlp auxiliary loss:  0.10183250904083252
clip auxiliary loss:  0.10183250904083252
{'loss': 1.0317, 'grad_norm': 5.686400590211991, 'learning_rate': 2.3698319501144202e-09, 'flos': 19471260057600.0, 'epoch': 0.98, 'num_input_tokens_seen': 174525230}
 ... (more hidden) ...language loss:  0.9245444536209106
mlp auxiliary loss:  0.1022367998957634
clip auxiliary loss:  0.1022367998957634
language loss:  0.6903080940246582
mlp auxiliary loss:  0.10181140154600143
clip auxiliary loss:  0.10181140154600143
{'loss': 0.9228, 'grad_norm': 4.526354440549743, 'learning_rate': 2.3320736492382644e-09, 'flos': 13282326036480.0, 'epoch': 0.99, 'num_input_tokens_seen': 174543785}
 ... (more hidden) ...language loss:  0.4892522692680359
mlp auxiliary loss:  0.1017761081457138
clip auxiliary loss:  0.1017761081457138
language loss:  0.31478941440582275
mlp auxiliary loss:  0.10210640728473663
clip auxiliary loss:  0.10210640728473663
{'loss': 0.8765, 'grad_norm': 3.8962235824771247, 'learning_rate': 2.29461839299816e-09, 'flos': 15850836541440.0, 'epoch': 0.99, 'num_input_tokens_seen': 174563220}
 ... (more hidden) ...language loss:  0.820242166519165
mlp auxiliary loss:  0.10159987956285477
clip auxiliary loss:  0.10159987956285477
language loss:  0.9258843660354614
mlp auxiliary loss:  0.10183537006378174
clip auxiliary loss:  0.10183537006378174
{'loss': 0.9952, 'grad_norm': 2.1827824540688363, 'learning_rate': 2.257466187076229e-09, 'flos': 18757616394240.0, 'epoch': 0.99, 'num_input_tokens_seen': 174582145}
 ... (more hidden) ...language loss:  0.8050583600997925
mlp auxiliary loss:  0.10176621377468109
clip auxiliary loss:  0.10176621377468109
language loss:  0.7445859313011169
mlp auxiliary loss:  0.10193729400634766
clip auxiliary loss:  0.10193729400634766
{'loss': 0.8895, 'grad_norm': 2.816674419445075, 'learning_rate': 2.2206170371081854e-09, 'flos': 14826738954240.0, 'epoch': 0.99, 'num_input_tokens_seen': 174600450}
 ... (more hidden) ...language loss:  0.9296846389770508
mlp auxiliary loss:  0.10195285826921463
clip auxiliary loss:  0.10195285826921463
language loss:  1.0698670148849487
mlp auxiliary loss:  0.1018969714641571
clip auxiliary loss:  0.1018969714641571
{'loss': 1.0421, 'grad_norm': 3.489240730130659, 'learning_rate': 2.1840709486842247e-09, 'flos': 17974660055040.0, 'epoch': 0.99, 'num_input_tokens_seen': 174619790}
 ... (more hidden) ...language loss:  0.8225876092910767
mlp auxiliary loss:  0.10175348818302155
clip auxiliary loss:  0.10175348818302155
language loss:  0.6795787811279297
mlp auxiliary loss:  0.1018994152545929
clip auxiliary loss:  0.1018994152545929
[2024-09-18 20:44:16,639] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.973, 'grad_norm': 3.181739838366401, 'learning_rate': 2.1478279273481335e-09, 'flos': 13518040719360.0, 'epoch': 0.99, 'num_input_tokens_seen': 174637995}
 ... (more hidden) ...language loss:  0.9096230268478394
mlp auxiliary loss:  0.10177955776453018
clip auxiliary loss:  0.10177955776453018
language loss:  0.6520209908485413
mlp auxiliary loss:  0.10164761543273926
clip auxiliary loss:  0.10164761543273926
{'loss': 0.9833, 'grad_norm': 12.481536657167608, 'learning_rate': 2.1118879785981815e-09, 'flos': 24347602206720.0, 'epoch': 0.99, 'num_input_tokens_seen': 174657855}
 ... (more hidden) ...language loss:  0.9564751386642456
mlp auxiliary loss:  0.10180345922708511
clip auxiliary loss:  0.10180345922708511
language loss:  0.9316081404685974
mlp auxiliary loss:  0.10171014815568924
clip auxiliary loss:  0.10171014815568924
{'loss': 0.9847, 'grad_norm': 8.045217991561092, 'learning_rate': 2.0762511078862288e-09, 'flos': 18500401950720.0, 'epoch': 0.99, 'num_input_tokens_seen': 174677920}
 ... (more hidden) ...language loss:  0.7760103940963745
mlp auxiliary loss:  0.10171464830636978
clip auxiliary loss:  0.10171464830636978
language loss:  0.7319290637969971
mlp auxiliary loss:  0.1018214225769043
clip auxiliary loss:  0.1018214225769043
{'loss': 0.8521, 'grad_norm': 6.846518039495457, 'learning_rate': 2.0409173206186183e-09, 'flos': 16848559656960.0, 'epoch': 0.99, 'num_input_tokens_seen': 174696880}
 ... (more hidden) ...language loss:  0.8235682249069214
mlp auxiliary loss:  0.10187765955924988
clip auxiliary loss:  0.10187765955924988
language loss:  0.5244046449661255
mlp auxiliary loss:  0.10187854617834091
clip auxiliary loss:  0.10187854617834091
{'loss': 1.0562, 'grad_norm': 3.1194605749824165, 'learning_rate': 2.0058866221550617e-09, 'flos': 14147808890880.0, 'epoch': 0.99, 'num_input_tokens_seen': 174714840}
 ... (more hidden) ...language loss:  0.8738095164299011
mlp auxiliary loss:  0.10172654688358307
clip auxiliary loss:  0.10172654688358307
language loss:  0.5883619785308838
mlp auxiliary loss:  0.10168726742267609
clip auxiliary loss:  0.10168726742267609
{'loss': 0.9465, 'grad_norm': 4.6270646306873235, 'learning_rate': 1.971159017809976e-09, 'flos': 14069145354240.0, 'epoch': 0.99, 'num_input_tokens_seen': 174732850}
 ... (more hidden) ...language loss:  0.7396944165229797
mlp auxiliary loss:  0.10183733701705933
clip auxiliary loss:  0.10183733701705933
language loss:  0.8372325897216797
mlp auxiliary loss:  0.1017797440290451
clip auxiliary loss:  0.1017797440290451
{'loss': 0.9652, 'grad_norm': 7.613308557624833, 'learning_rate': 1.93673451285159e-09, 'flos': 15379315200000.0, 'epoch': 0.99, 'num_input_tokens_seen': 174751620}
 ... (more hidden) ...language loss:  0.6150950193405151
mlp auxiliary loss:  0.10036347806453705
clip auxiliary loss:  0.10036347806453705
language loss:  0.6012290716171265
mlp auxiliary loss:  0.10036347806453705
clip auxiliary loss:  0.10036347806453705
{'loss': 0.7902, 'grad_norm': 0.7620971131230421, 'learning_rate': 1.9026131125019495e-09, 'flos': 37747505602560.0, 'epoch': 0.99, 'num_input_tokens_seen': 174808710}
 ... (more hidden) ...language loss:  0.9618591070175171
mlp auxiliary loss:  0.10161226242780685
clip auxiliary loss:  0.10161226242780685
language loss:  0.940446138381958
mlp auxiliary loss:  0.10153398662805557
clip auxiliary loss:  0.10153398662805557
{'loss': 1.0516, 'grad_norm': 2.613778516047396, 'learning_rate': 1.8687948219371363e-09, 'flos': 16609809776640.0, 'epoch': 0.99, 'num_input_tokens_seen': 174827655}
 ... (more hidden) ...language loss:  0.9865803122520447
mlp auxiliary loss:  0.10187403112649918
clip auxiliary loss:  0.10187403112649918
language loss:  0.7193305492401123
mlp auxiliary loss:  0.10161761194467545
clip auxiliary loss:  0.10161761194467545
{'loss': 1.0682, 'grad_norm': 6.283214963949835, 'learning_rate': 1.835279646287491e-09, 'flos': 15354044436480.0, 'epoch': 0.99, 'num_input_tokens_seen': 174845385}
 ... (more hidden) ...language loss:  0.9625454545021057
mlp auxiliary loss:  0.10186707228422165
clip auxiliary loss:  0.10186707228422165
language loss:  0.9380030035972595
mlp auxiliary loss:  0.10170899331569672
clip auxiliary loss:  0.10170899331569672
{'loss': 0.9656, 'grad_norm': 3.013256506685989, 'learning_rate': 1.8020675906371685e-09, 'flos': 15824308776960.0, 'epoch': 0.99, 'num_input_tokens_seen': 174864500}
 ... (more hidden) ...language loss:  0.6571877598762512
mlp auxiliary loss:  0.10168163478374481
clip auxiliary loss:  0.10168163478374481
language loss:  0.5993611216545105
mlp auxiliary loss:  0.10154136270284653
clip auxiliary loss:  0.10154136270284653
{'loss': 0.9387, 'grad_norm': 3.2944804530742746, 'learning_rate': 1.7691586600243612e-09, 'flos': 18366996664320.0, 'epoch': 0.99, 'num_input_tokens_seen': 174883120}
 ... (more hidden) ...language loss:  0.6922405958175659
mlp auxiliary loss:  0.10188252478837967
clip auxiliary loss:  0.10188252478837967
language loss:  0.7395755648612976
mlp auxiliary loss:  0.10197246074676514
clip auxiliary loss:  0.10197246074676514
{'loss': 1.0579, 'grad_norm': 4.700207993795224, 'learning_rate': 1.7365528594415202e-09, 'flos': 11603741368320.0, 'epoch': 0.99, 'num_input_tokens_seen': 174896910}
 ... (more hidden) ...language loss:  0.08480856567621231
mlp auxiliary loss:  0.10150803625583649
clip auxiliary loss:  0.10150803625583649
language loss:  0.7651011347770691
mlp auxiliary loss:  0.10208272933959961
clip auxiliary loss:  0.10208272933959961
[2024-09-18 20:47:35,719] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.8671, 'grad_norm': 5.652862546109711, 'learning_rate': 1.7042501938346888e-09, 'flos': 25318337679360.0, 'epoch': 0.99, 'num_input_tokens_seen': 174919360}
 ... (more hidden) ...language loss:  0.7507892847061157
mlp auxiliary loss:  0.10164042562246323
clip auxiliary loss:  0.10164042562246323
language loss:  0.6448763012886047
mlp auxiliary loss:  0.10172919929027557
clip auxiliary loss:  0.10172919929027557
{'loss': 0.9642, 'grad_norm': 3.6724845218007673, 'learning_rate': 1.6722506681043913e-09, 'flos': 15222907883520.0, 'epoch': 0.99, 'num_input_tokens_seen': 174938040}
 ... (more hidden) ...language loss:  0.7893203496932983
mlp auxiliary loss:  0.10182687640190125
clip auxiliary loss:  0.10182687640190125
language loss:  0.9199650287628174
mlp auxiliary loss:  0.1015138179063797
clip auxiliary loss:  0.1015138179063797
{'loss': 0.8804, 'grad_norm': 4.437710179469012, 'learning_rate': 1.640554287104745e-09, 'flos': 11552126791680.0, 'epoch': 0.99, 'num_input_tokens_seen': 174956035}
 ... (more hidden) ...language loss:  0.9263575077056885
mlp auxiliary loss:  0.1017562672495842
clip auxiliary loss:  0.1017562672495842
language loss:  0.9197960495948792
mlp auxiliary loss:  0.10161011666059494
clip auxiliary loss:  0.10161011666059494
{'loss': 0.972, 'grad_norm': 4.862801181124768, 'learning_rate': 1.609161055644348e-09, 'flos': 12647529861120.0, 'epoch': 0.99, 'num_input_tokens_seen': 174971680}
 ... (more hidden) ...language loss:  0.2562144100666046
mlp auxiliary loss:  0.1021711528301239
clip auxiliary loss:  0.1021711528301239
language loss:  0.7812578678131104
mlp auxiliary loss:  0.10167668014764786
clip auxiliary loss:  0.10167668014764786
{'loss': 0.8661, 'grad_norm': 4.023706993780953, 'learning_rate': 1.5780709784849467e-09, 'flos': 18598664417280.0, 'epoch': 0.99, 'num_input_tokens_seen': 174988420}
 ... (more hidden) ...language loss:  0.8544842004776001
mlp auxiliary loss:  0.10222166031599045
clip auxiliary loss:  0.10222166031599045
language loss:  0.9001945853233337
mlp auxiliary loss:  0.10197880864143372
clip auxiliary loss:  0.10197880864143372
{'loss': 1.0131, 'grad_norm': 5.217870032765574, 'learning_rate': 1.5472840603436565e-09, 'flos': 11310341713920.0, 'epoch': 0.99, 'num_input_tokens_seen': 175005370}
 ... (more hidden) ...language loss:  0.798474133014679
mlp auxiliary loss:  0.10182983428239822
clip auxiliary loss:  0.10182983428239822
language loss:  0.8188219666481018
mlp auxiliary loss:  0.10190057754516602
clip auxiliary loss:  0.10190057754516602
[2024-09-18 20:49:30,889] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.9861, 'grad_norm': 3.658369705146834, 'learning_rate': 1.5168003058900757e-09, 'flos': 13334553784320.0, 'epoch': 0.99, 'num_input_tokens_seen': 175023090}
 ... (more hidden) ...language loss:  0.84575355052948
mlp auxiliary loss:  0.1019626185297966
clip auxiliary loss:  0.1019626185297966
language loss:  0.753157913684845
mlp auxiliary loss:  0.10176602005958557
clip auxiliary loss:  0.10176602005958557
[2024-09-18 20:49:48,932] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 1.1108, 'grad_norm': 3.4528512975698624, 'learning_rate': 1.4866197197491715e-09, 'flos': 15903984046080.0, 'epoch': 0.99, 'num_input_tokens_seen': 175042170}
 ... (more hidden) ...language loss:  0.6939688920974731
mlp auxiliary loss:  0.10190916061401367
clip auxiliary loss:  0.10190916061401367
language loss:  0.956378698348999
mlp auxiliary loss:  0.10178393125534058
clip auxiliary loss:  0.10178393125534058
{'loss': 0.9436, 'grad_norm': 7.036142774650304, 'learning_rate': 1.4567423064988371e-09, 'flos': 11079348449280.0, 'epoch': 0.99, 'num_input_tokens_seen': 175059240}
 ... (more hidden) ...language loss:  0.6399515867233276
mlp auxiliary loss:  0.10193100571632385
clip auxiliary loss:  0.10193100571632385
language loss:  0.8153350949287415
mlp auxiliary loss:  0.10176627337932587
clip auxiliary loss:  0.10176627337932587
{'loss': 0.9758, 'grad_norm': 4.029605027146543, 'learning_rate': 1.4271680706718913e-09, 'flos': 15269310504960.0, 'epoch': 0.99, 'num_input_tokens_seen': 175076635}
 ... (more hidden) ...language loss:  0.6975179314613342
mlp auxiliary loss:  0.1017681434750557
clip auxiliary loss:  0.1017681434750557
language loss:  0.9521669149398804
mlp auxiliary loss:  0.10196754336357117
clip auxiliary loss:  0.10196754336357117
{'loss': 1.013, 'grad_norm': 3.900581458346409, 'learning_rate': 1.3978970167543013e-09, 'flos': 19965998039040.0, 'epoch': 0.99, 'num_input_tokens_seen': 175096535}
 ... (more hidden) ...language loss:  1.0134962797164917
mlp auxiliary loss:  0.1018165647983551
clip auxiliary loss:  0.1018165647983551
language loss:  0.9249233603477478
mlp auxiliary loss:  0.10212245583534241
clip auxiliary loss:  0.10212245583534241
[2024-09-18 20:50:41,590] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.9573, 'grad_norm': 4.7905388652028025, 'learning_rate': 1.3689291491867372e-09, 'flos': 9950672732160.0, 'epoch': 0.99, 'num_input_tokens_seen': 175114570}
 ... (more hidden) ...language loss:  0.8788212537765503
mlp auxiliary loss:  0.10164867341518402
clip auxiliary loss:  0.10164867341518402
language loss:  0.6088854670524597
mlp auxiliary loss:  0.10179588198661804
clip auxiliary loss:  0.10179588198661804
{'loss': 0.932, 'grad_norm': 2.987542034795154, 'learning_rate': 1.3402644723636836e-09, 'flos': 18814626877440.0, 'epoch': 0.99, 'num_input_tokens_seen': 175136320}
 ... (more hidden) ...language loss:  0.6446244716644287
mlp auxiliary loss:  0.10181443393230438
clip auxiliary loss:  0.10181443393230438
language loss:  0.9324046969413757
mlp auxiliary loss:  0.10163304954767227
clip auxiliary loss:  0.10163304954767227
{'loss': 1.0137, 'grad_norm': 5.599556748695414, 'learning_rate': 1.311902990633218e-09, 'flos': 17950278389760.0, 'epoch': 0.99, 'num_input_tokens_seen': 175155005}
 ... (more hidden) ...language loss:  0.9115538597106934
mlp auxiliary loss:  0.10175613313913345
clip auxiliary loss:  0.10175613313913345
language loss:  0.7963956594467163
mlp auxiliary loss:  0.10169927775859833
clip auxiliary loss:  0.10169927775859833
{'loss': 0.8986, 'grad_norm': 3.2497611324854154, 'learning_rate': 1.2838447082978987e-09, 'flos': 18762184519680.0, 'epoch': 0.99, 'num_input_tokens_seen': 175175880}
 ... (more hidden) ...language loss:  0.9031389355659485
mlp auxiliary loss:  0.10161340236663818
clip auxiliary loss:  0.10161340236663818
language loss:  0.8570011258125305
mlp auxiliary loss:  0.10174619406461716
clip auxiliary loss:  0.10174619406461716
{'loss': 1.0189, 'grad_norm': 4.880498084304851, 'learning_rate': 1.2560896296143208e-09, 'flos': 17294013112320.0, 'epoch': 0.99, 'num_input_tokens_seen': 175194065}
 ... (more hidden) ...language loss:  0.7459500432014465
mlp auxiliary loss:  0.10175146162509918
clip auxiliary loss:  0.10175146162509918
language loss:  0.6625244617462158
mlp auxiliary loss:  0.10192257165908813
clip auxiliary loss:  0.10192257165908813
{'loss': 0.9976, 'grad_norm': 3.8529114749895883, 'learning_rate': 1.2286377587926722e-09, 'flos': 13438028206080.0, 'epoch': 0.99, 'num_input_tokens_seen': 175210575}
 ... (more hidden) ...language loss:  0.968977689743042
mlp auxiliary loss:  0.10165578126907349
clip auxiliary loss:  0.10165578126907349
language loss:  0.6628618836402893
mlp auxiliary loss:  0.1016116663813591
clip auxiliary loss:  0.1016116663813591
{'loss': 0.9514, 'grad_norm': 7.096430738310127, 'learning_rate': 1.2014890999973992e-09, 'flos': 18631047966720.0, 'epoch': 0.99, 'num_input_tokens_seen': 175227215}
 ... (more hidden) ...language loss:  0.7936140894889832
mlp auxiliary loss:  0.10163373500108719
clip auxiliary loss:  0.10163373500108719
language loss:  0.8572148084640503
mlp auxiliary loss:  0.10189692676067352
clip auxiliary loss:  0.10189692676067352
{'loss': 0.9714, 'grad_norm': 3.309709489180432, 'learning_rate': 1.1746436573472073e-09, 'flos': 18108954439680.0, 'epoch': 0.99, 'num_input_tokens_seen': 175248670}
 ... (more hidden) ...language loss:  0.9421617984771729
mlp auxiliary loss:  0.10171490907669067
clip auxiliary loss:  0.10171490907669067
language loss:  0.7157149910926819
mlp auxiliary loss:  0.10211269557476044
clip auxiliary loss:  0.10211269557476044
{'loss': 0.8823, 'grad_norm': 3.6640233279739105, 'learning_rate': 1.1481014349141726e-09, 'flos': 14327616798720.0, 'epoch': 0.99, 'num_input_tokens_seen': 175265610}
 ... (more hidden) ...language loss:  0.601630449295044
mlp auxiliary loss:  0.10181562602519989
clip auxiliary loss:  0.10181562602519989
language loss:  0.7332149147987366
mlp auxiliary loss:  0.10183893889188766
clip auxiliary loss:  0.10183893889188766
{'loss': 1.0281, 'grad_norm': 3.257639782029758, 'learning_rate': 1.121862436724852e-09, 'flos': 17529850429440.0, 'epoch': 0.99, 'num_input_tokens_seen': 175284170}
 ... (more hidden) ...language loss:  0.40469300746917725
mlp auxiliary loss:  0.10217113792896271
clip auxiliary loss:  0.10217113792896271
language loss:  0.7307627201080322
mlp auxiliary loss:  0.1022430807352066
clip auxiliary loss:  0.1022430807352066
{'loss': 0.9061, 'grad_norm': 3.7346187496041705, 'learning_rate': 1.0959266667598388e-09, 'flos': 15485089013760.0, 'epoch': 0.99, 'num_input_tokens_seen': 175302705}
 ... (more hidden) ...language loss:  0.6373423337936401
mlp auxiliary loss:  0.10187040269374847
clip auxiliary loss:  0.10187040269374847
language loss:  0.7304834723472595
mlp auxiliary loss:  0.10179074853658676
clip auxiliary loss:  0.10179074853658676
{'loss': 0.9339, 'grad_norm': 6.010062680544377, 'learning_rate': 1.0702941289533196e-09, 'flos': 15144642908160.0, 'epoch': 0.99, 'num_input_tokens_seen': 175321100}
 ... (more hidden) ...language loss:  0.7136552333831787
mlp auxiliary loss:  0.10192699730396271
clip auxiliary loss:  0.10192699730396271
language loss:  0.8111530542373657
mlp auxiliary loss:  0.10182639211416245
clip auxiliary loss:  0.10182639211416245
{'loss': 1.0765, 'grad_norm': 2.9512058056661283, 'learning_rate': 1.0449648271939615e-09, 'flos': 13145578967040.0, 'epoch': 0.99, 'num_input_tokens_seen': 175337165}
 ... (more hidden) ...language loss:  0.9042172431945801
mlp auxiliary loss:  0.10177012532949448
clip auxiliary loss:  0.10177012532949448
language loss:  0.5354532599449158
mlp auxiliary loss:  0.10184185951948166
clip auxiliary loss:  0.10184185951948166
{'loss': 0.9289, 'grad_norm': 2.4110856451378284, 'learning_rate': 1.0199387653240243e-09, 'flos': 16898886574080.0, 'epoch': 0.99, 'num_input_tokens_seen': 175356575}
 ... (more hidden) ...language loss:  0.7085410952568054
mlp auxiliary loss:  0.1019042357802391
clip auxiliary loss:  0.1019042357802391
language loss:  0.7862721681594849
mlp auxiliary loss:  0.10176832973957062
clip auxiliary loss:  0.10176832973957062
[2024-09-18 20:53:27,303] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.8994, 'grad_norm': 2.5375038410026156, 'learning_rate': 9.952159471400267e-10, 'flos': 11604323880960.0, 'epoch': 0.99, 'num_input_tokens_seen': 175373335}
 ... (more hidden) ...language loss:  0.7290453314781189
mlp auxiliary loss:  0.10169357806444168
clip auxiliary loss:  0.10169357806444168
language loss:  0.8586394190788269
mlp auxiliary loss:  0.10175274312496185
clip auxiliary loss:  0.10175274312496185
{'loss': 1.0276, 'grad_norm': 4.335962133939032, 'learning_rate': 9.707963763923022e-10, 'flos': 16030981693440.0, 'epoch': 0.99, 'num_input_tokens_seen': 175392105}
 ... (more hidden) ...language loss:  0.9922709465026855
mlp auxiliary loss:  0.10161146521568298
clip auxiliary loss:  0.10161146521568298
language loss:  0.7878332734107971
mlp auxiliary loss:  0.10162283480167389
clip auxiliary loss:  0.10162283480167389
{'loss': 0.9816, 'grad_norm': 2.8937512787060147, 'learning_rate': 9.466800567854427e-10, 'flos': 11420959580160.0, 'epoch': 0.99, 'num_input_tokens_seen': 175410425}
 ... (more hidden) ...language loss:  0.7642356157302856
mlp auxiliary loss:  0.10151885449886322
clip auxiliary loss:  0.10151885449886322
language loss:  0.4241805374622345
mlp auxiliary loss:  0.10206722468137741
clip auxiliary loss:  0.10206722468137741
[2024-09-18 20:54:02,497] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.8789, 'grad_norm': 5.529196665188271, 'learning_rate': 9.228669919778553e-10, 'flos': 18972444487680.0, 'epoch': 0.99, 'num_input_tokens_seen': 175429070}
 ... (more hidden) ...language loss:  0.8424791693687439
mlp auxiliary loss:  0.10167005658149719
clip auxiliary loss:  0.10167005658149719
language loss:  0.8013060688972473
mlp auxiliary loss:  0.10205558687448502
clip auxiliary loss:  0.10205558687448502
{'loss': 0.9857, 'grad_norm': 4.381384849921883, 'learning_rate': 8.993571855817617e-10, 'flos': 16428468940800.0, 'epoch': 0.99, 'num_input_tokens_seen': 175447620}
 ... (more hidden) ...language loss:  0.9584898352622986
mlp auxiliary loss:  0.10166159272193909
clip auxiliary loss:  0.10166159272193909
language loss:  0.5350157618522644
mlp auxiliary loss:  0.10148398578166962
clip auxiliary loss:  0.10148398578166962
{'loss': 0.9278, 'grad_norm': 7.493563115046118, 'learning_rate': 8.761506411638642e-10, 'flos': 15694582517760.0, 'epoch': 0.99, 'num_input_tokens_seen': 175466805}
 ... (more hidden) ...language loss:  0.6827113032341003
mlp auxiliary loss:  0.10160021483898163
clip auxiliary loss:  0.10160021483898163
language loss:  0.9206518530845642
mlp auxiliary loss:  0.1021231934428215
clip auxiliary loss:  0.1021231934428215
{'loss': 0.9253, 'grad_norm': 4.867040543151623, 'learning_rate': 8.53247362244236e-10, 'flos': 13647797637120.0, 'epoch': 0.99, 'num_input_tokens_seen': 175485335}
 ... (more hidden) ...language loss:  0.8485426306724548
mlp auxiliary loss:  0.10181817412376404
clip auxiliary loss:  0.10181817412376404
language loss:  0.619458019733429
mlp auxiliary loss:  0.1017761081457138
clip auxiliary loss:  0.1017761081457138
{'loss': 0.8769, 'grad_norm': 2.8264568319649648, 'learning_rate': 8.306473522976532e-10, 'flos': 16794369761280.0, 'epoch': 0.99, 'num_input_tokens_seen': 175504460}
 ... (more hidden) ...language loss:  0.5061819553375244
mlp auxiliary loss:  0.10171989351511002
clip auxiliary loss:  0.10171989351511002
language loss:  0.6314454078674316
mlp auxiliary loss:  0.10171300172805786
clip auxiliary loss:  0.10171300172805786
{'loss': 0.9147, 'grad_norm': 7.294574644513878, 'learning_rate': 8.083506147522623e-10, 'flos': 16140802437120.0, 'epoch': 0.99, 'num_input_tokens_seen': 175523575}
 ... (more hidden) ...language loss:  0.8786004185676575
mlp auxiliary loss:  0.10189962387084961
clip auxiliary loss:  0.10189962387084961
language loss:  0.7234837412834167
mlp auxiliary loss:  0.10162493586540222
clip auxiliary loss:  0.10162493586540222
{'loss': 1.0456, 'grad_norm': 4.408388092527791, 'learning_rate': 7.863571529906909e-10, 'flos': 9532942725120.0, 'epoch': 0.99, 'num_input_tokens_seen': 175538880}
 ... (more hidden) ...language loss:  0.6303240060806274
mlp auxiliary loss:  0.10037204623222351
clip auxiliary loss:  0.10037204623222351
language loss:  0.5799724459648132
mlp auxiliary loss:  0.10037204623222351
clip auxiliary loss:  0.10037204623222351
[2024-09-18 20:55:27,146] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.8425, 'grad_norm': 0.7819015456342807, 'learning_rate': 7.646669703489372e-10, 'flos': 44260751216640.0, 'epoch': 0.99, 'num_input_tokens_seen': 175602910}
 ... (more hidden) ...language loss:  0.5095893144607544
mlp auxiliary loss:  0.10183864086866379
clip auxiliary loss:  0.10183864086866379
language loss:  0.5060045719146729
mlp auxiliary loss:  0.10187534242868423
clip auxiliary loss:  0.10187534242868423
[2024-09-18 20:55:40,289] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.7694, 'grad_norm': 3.1733878901410626, 'learning_rate': 7.432800701177023e-10, 'flos': 13308209971200.0, 'epoch': 0.99, 'num_input_tokens_seen': 175620630}
 ... (more hidden) ...language loss:  0.6519559621810913
mlp auxiliary loss:  0.10034412145614624
clip auxiliary loss:  0.10034412145614624
language loss:  0.5554578900337219
mlp auxiliary loss:  0.10034412145614624
clip auxiliary loss:  0.10034412145614624
{'loss': 0.7931, 'grad_norm': 0.8142790025061584, 'learning_rate': 7.221964555415017e-10, 'flos': 47209518858240.0, 'epoch': 0.99, 'num_input_tokens_seen': 175680010}
 ... (more hidden) ...language loss:  0.7141616940498352
mlp auxiliary loss:  0.10161696374416351
clip auxiliary loss:  0.10161696374416351
language loss:  0.4164174497127533
mlp auxiliary loss:  0.10149045288562775
clip auxiliary loss:  0.10149045288562775
{'loss': 0.9402, 'grad_norm': 4.983124674805779, 'learning_rate': 7.01416129818222e-10, 'flos': 11735399116800.0, 'epoch': 0.99, 'num_input_tokens_seen': 175697350}
 ... (more hidden) ...language loss:  0.1692284643650055
mlp auxiliary loss:  0.10183914005756378
clip auxiliary loss:  0.10183914005756378
language loss:  0.630980908870697
mlp auxiliary loss:  0.10211709141731262
clip auxiliary loss:  0.10211709141731262
{'loss': 0.7755, 'grad_norm': 3.1916296144198952, 'learning_rate': 6.809390961006745e-10, 'flos': 18081016381440.0, 'epoch': 0.99, 'num_input_tokens_seen': 175717200}
 ... (more hidden) ...language loss:  0.7807610034942627
mlp auxiliary loss:  0.1020829826593399
clip auxiliary loss:  0.1020829826593399
language loss:  0.36472538113594055
mlp auxiliary loss:  0.10183157026767731
clip auxiliary loss:  0.10183157026767731
{'loss': 0.881, 'grad_norm': 2.904740300347585, 'learning_rate': 6.607653574948191e-10, 'flos': 17819325788160.0, 'epoch': 0.99, 'num_input_tokens_seen': 175737700}
 ... (more hidden) ...language loss:  0.8991373777389526
mlp auxiliary loss:  0.10172189772129059
clip auxiliary loss:  0.10172189772129059
language loss:  0.7699614763259888
mlp auxiliary loss:  0.10164356976747513
clip auxiliary loss:  0.10164356976747513
{'loss': 1.0082, 'grad_norm': 5.107913306902348, 'learning_rate': 6.408949170613187e-10, 'flos': 15506404823040.0, 'epoch': 0.99, 'num_input_tokens_seen': 175756685}
 ... (more hidden) ...language loss:  0.7542969584465027
mlp auxiliary loss:  0.10164668411016464
clip auxiliary loss:  0.10164668411016464
language loss:  0.7823947668075562
mlp auxiliary loss:  0.10189440846443176
clip auxiliary loss:  0.10189440846443176
{'loss': 1.0106, 'grad_norm': 2.215471128385389, 'learning_rate': 6.213277778144288e-10, 'flos': 17688035942400.0, 'epoch': 0.99, 'num_input_tokens_seen': 175778050}
 ... (more hidden) ...language loss:  0.7400389909744263
mlp auxiliary loss:  0.10181787610054016
clip auxiliary loss:  0.10181787610054016
language loss:  0.7888426780700684
mlp auxiliary loss:  0.10167834907770157
clip auxiliary loss:  0.10167834907770157
{'loss': 0.8656, 'grad_norm': 4.662744076964819, 'learning_rate': 6.020639427224416e-10, 'flos': 15350978580480.0, 'epoch': 0.99, 'num_input_tokens_seen': 175795415}
 ... (more hidden) ...language loss:  0.2520100176334381
mlp auxiliary loss:  0.101667620241642
clip auxiliary loss:  0.101667620241642
language loss:  0.8193351030349731
mlp auxiliary loss:  0.10192593187093735
clip auxiliary loss:  0.10192593187093735
{'loss': 0.9122, 'grad_norm': 3.4788884643675724, 'learning_rate': 5.831034147076864e-10, 'flos': 17785777213440.0, 'epoch': 0.99, 'num_input_tokens_seen': 175812385}
 ... (more hidden) ...language loss:  0.5880971550941467
mlp auxiliary loss:  0.1003655195236206
clip auxiliary loss:  0.1003655195236206
language loss:  0.6106744408607483
mlp auxiliary loss:  0.1003655195236206
clip auxiliary loss:  0.1003655195236206
[2024-09-18 20:57:26,859] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.7663, 'grad_norm': 0.7100594096781775, 'learning_rate': 5.644461966463065e-10, 'flos': 49351001702400.0, 'epoch': 0.99, 'num_input_tokens_seen': 175879715}
 ... (more hidden) ...language loss:  0.7961734533309937
mlp auxiliary loss:  0.10154468566179276
clip auxiliary loss:  0.10154468566179276
language loss:  0.7513974905014038
mlp auxiliary loss:  0.10157230496406555
clip auxiliary loss:  0.10157230496406555
{'loss': 0.9471, 'grad_norm': 3.487690532770987, 'learning_rate': 5.460922913687049e-10, 'flos': 14856025989120.0, 'epoch': 0.99, 'num_input_tokens_seen': 175898525}
 ... (more hidden) ...language loss:  0.8128840923309326
mlp auxiliary loss:  0.10173629969358444
clip auxiliary loss:  0.10173629969358444
language loss:  0.7027768492698669
mlp auxiliary loss:  0.10195659846067429
clip auxiliary loss:  0.10195659846067429
{'loss': 0.9371, 'grad_norm': 3.8021030930654485, 'learning_rate': 5.280417016593208e-10, 'flos': 15850805882880.0, 'epoch': 0.99, 'num_input_tokens_seen': 175918035}
 ... (more hidden) ...language loss:  0.8488767743110657
mlp auxiliary loss:  0.10205243527889252
clip auxiliary loss:  0.10205243527889252
language loss:  0.6496641635894775
mlp auxiliary loss:  0.10175072401762009
clip auxiliary loss:  0.10175072401762009
{'loss': 0.9419, 'grad_norm': 2.5305929390799964, 'learning_rate': 5.102944302559642e-10, 'flos': 12311958466560.0, 'epoch': 0.99, 'num_input_tokens_seen': 175935250}
 ... (more hidden) ...language loss:  0.5922713279724121
mlp auxiliary loss:  0.10180702805519104
clip auxiliary loss:  0.10180702805519104
language loss:  0.6163218021392822
mlp auxiliary loss:  0.1018458902835846
clip auxiliary loss:  0.1018458902835846
{'loss': 0.9693, 'grad_norm': 4.195494351272862, 'learning_rate': 4.9285047985137e-10, 'flos': 16114397306880.0, 'epoch': 0.99, 'num_input_tokens_seen': 175954390}
 ... (more hidden) ...language loss:  0.42824429273605347
mlp auxiliary loss:  0.1018541008234024
clip auxiliary loss:  0.1018541008234024
language loss:  0.9215446710586548
mlp auxiliary loss:  0.10192541033029556
clip auxiliary loss:  0.10192541033029556
{'loss': 0.9419, 'grad_norm': 3.0924726218859657, 'learning_rate': 4.757098530916436e-10, 'flos': 20047850065920.0, 'epoch': 0.99, 'num_input_tokens_seen': 175974555}
 ... (more hidden) ...language loss:  0.3971434235572815
mlp auxiliary loss:  0.10209758579730988
clip auxiliary loss:  0.10209758579730988
language loss:  0.7275771498680115
mlp auxiliary loss:  0.10163195431232452
clip auxiliary loss:  0.10163195431232452
{'loss': 0.9767, 'grad_norm': 6.848607956272625, 'learning_rate': 4.5887255257670563e-10, 'flos': 14304676085760.0, 'epoch': 0.99, 'num_input_tokens_seen': 175991315}
 ... (more hidden) ...language loss:  0.6029962301254272
mlp auxiliary loss:  0.10162495821714401
clip auxiliary loss:  0.10162495821714401
language loss:  0.8568475246429443
mlp auxiliary loss:  0.10184842348098755
clip auxiliary loss:  0.10184842348098755
{'loss': 0.9664, 'grad_norm': 8.394129527381677, 'learning_rate': 4.4233858086117906e-10, 'flos': 15171140014080.0, 'epoch': 0.99, 'num_input_tokens_seen': 176009560}
 ... (more hidden) ...language loss:  1.006905436515808
mlp auxiliary loss:  0.10173060745000839
clip auxiliary loss:  0.10173060745000839
language loss:  0.8429479002952576
mlp auxiliary loss:  0.10166498273611069
clip auxiliary loss:  0.10166498273611069
{'loss': 0.8631, 'grad_norm': 3.8369529607582225, 'learning_rate': 4.261079404528356e-10, 'flos': 14016856289280.0, 'epoch': 0.99, 'num_input_tokens_seen': 176028760}
 ... (more hidden) ...language loss:  0.856005072593689
mlp auxiliary loss:  0.10181508958339691
clip auxiliary loss:  0.10181508958339691
language loss:  0.9664579033851624
mlp auxiliary loss:  0.1016991138458252
clip auxiliary loss:  0.1016991138458252
{'loss': 0.8774, 'grad_norm': 5.130798595055399, 'learning_rate': 4.1018063381437205e-10, 'flos': 15613956833280.0, 'epoch': 0.99, 'num_input_tokens_seen': 176048865}
 ... (more hidden) ...language loss:  0.7189452648162842
mlp auxiliary loss:  0.1003415659070015
clip auxiliary loss:  0.1003415659070015
language loss:  0.5421813726425171
mlp auxiliary loss:  0.1003415659070015
clip auxiliary loss:  0.1003415659070015
[2024-09-18 20:59:28,857] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.8518, 'grad_norm': 0.9117838505349187, 'learning_rate': 3.9455666336141167e-10, 'flos': 49994574336000.0, 'epoch': 0.99, 'num_input_tokens_seen': 176112365}
 ... (more hidden) ...language loss:  0.8663771152496338
mlp auxiliary loss:  0.10167859494686127
clip auxiliary loss:  0.10167859494686127
language loss:  0.8980871438980103
mlp auxiliary loss:  0.10174205154180527
clip auxiliary loss:  0.10174205154180527
{'loss': 1.0038, 'grad_norm': 7.533664173974738, 'learning_rate': 3.7923603146450267e-10, 'flos': 10659043123200.0, 'epoch': 0.99, 'num_input_tokens_seen': 176128145}
 ... (more hidden) ...language loss:  0.7999305129051208
mlp auxiliary loss:  0.10164415091276169
clip auxiliary loss:  0.10164415091276169
language loss:  0.7908806204795837
mlp auxiliary loss:  0.10165294259786606
clip auxiliary loss:  0.10165294259786606
{'loss': 0.9812, 'grad_norm': 3.4206882317482252, 'learning_rate': 3.642187404473418e-10, 'flos': 12548041052160.0, 'epoch': 0.99, 'num_input_tokens_seen': 176146025}
 ... (more hidden) ...language loss:  0.8864966630935669
mlp auxiliary loss:  0.10173176229000092
clip auxiliary loss:  0.10173176229000092
language loss:  0.9230712652206421
mlp auxiliary loss:  0.1017681360244751
clip auxiliary loss:  0.1017681360244751
{'loss': 1.0402, 'grad_norm': 4.420817218335281, 'learning_rate': 3.495047925885508e-10, 'flos': 13596704256000.0, 'epoch': 0.99, 'num_input_tokens_seen': 176164080}
 ... (more hidden) ...language loss:  0.9775359034538269
mlp auxiliary loss:  0.10155545175075531
clip auxiliary loss:  0.10155545175075531
language loss:  1.1100163459777832
mlp auxiliary loss:  0.10185547173023224
clip auxiliary loss:  0.10185547173023224
{'loss': 1.0278, 'grad_norm': 3.0308367260660547, 'learning_rate': 3.350941901199e-10, 'flos': 12647192616960.0, 'epoch': 0.99, 'num_input_tokens_seen': 176180720}
 ... (more hidden) ...language loss:  0.9188073873519897
mlp auxiliary loss:  0.1017901822924614
clip auxiliary loss:  0.1017901822924614
language loss:  0.8872581124305725
mlp auxiliary loss:  0.10173045098781586
clip auxiliary loss:  0.10173045098781586
{'loss': 1.026, 'grad_norm': 4.9174678386805954, 'learning_rate': 3.2098693522764066e-10, 'flos': 13325846753280.0, 'epoch': 0.99, 'num_input_tokens_seen': 176193640}
 ... (more hidden) ...language loss:  1.044098138809204
mlp auxiliary loss:  0.10167442262172699
clip auxiliary loss:  0.10167442262172699
language loss:  0.7273674011230469
mlp auxiliary loss:  0.10189172625541687
clip auxiliary loss:  0.10189172625541687
{'loss': 1.0133, 'grad_norm': 6.785406710889019, 'learning_rate': 3.071830300516165e-10, 'flos': 14908744273920.0, 'epoch': 0.99, 'num_input_tokens_seen': 176211190}
 ... (more hidden) ...language loss:  0.7874584197998047
mlp auxiliary loss:  0.10182011872529984
clip auxiliary loss:  0.10182011872529984
language loss:  0.787612795829773
mlp auxiliary loss:  0.10190211236476898
clip auxiliary loss:  0.10190211236476898
{'loss': 0.8882, 'grad_norm': 4.140611641340479, 'learning_rate': 2.9368247668615234e-10, 'flos': 10424033587200.0, 'epoch': 0.99, 'num_input_tokens_seen': 176229500}
 ... (more hidden) ...language loss:  0.9572766423225403
mlp auxiliary loss:  0.10186479985713959
clip auxiliary loss:  0.10186479985713959
language loss:  0.2053072601556778
mlp auxiliary loss:  0.10174510627985
clip auxiliary loss:  0.10174510627985
{'loss': 0.7999, 'grad_norm': 6.898956728583757, 'learning_rate': 2.804852771789434e-10, 'flos': 8923141386240.0, 'epoch': 0.99, 'num_input_tokens_seen': 176242520}
 ... (more hidden) ...language loss:  0.5219339728355408
mlp auxiliary loss:  0.10181161761283875
clip auxiliary loss:  0.10181161761283875
language loss:  0.40586423873901367
mlp auxiliary loss:  0.10181094706058502
clip auxiliary loss:  0.10181094706058502
{'loss': 0.754, 'grad_norm': 5.669566964327018, 'learning_rate': 2.675914335321661e-10, 'flos': 13360928256000.0, 'epoch': 0.99, 'num_input_tokens_seen': 176260995}
 ... (more hidden) ...language loss:  0.9163792133331299
mlp auxiliary loss:  0.10181864351034164
clip auxiliary loss:  0.10181864351034164
language loss:  0.7253971099853516
mlp auxiliary loss:  0.101997010409832
clip auxiliary loss:  0.101997010409832
{'loss': 0.9761, 'grad_norm': 5.20504716073107, 'learning_rate': 2.550009477018111e-10, 'flos': 17713827901440.0, 'epoch': 1.0, 'num_input_tokens_seen': 176279485}
 ... (more hidden) ...language loss:  0.3982119858264923
mlp auxiliary loss:  0.10183484852313995
clip auxiliary loss:  0.10183484852313995
language loss:  0.4309440553188324
mlp auxiliary loss:  0.10199442505836487
clip auxiliary loss:  0.10199442505836487
{'loss': 0.823, 'grad_norm': 2.607968710945691, 'learning_rate': 2.4271382159790634e-10, 'flos': 16875547299840.0, 'epoch': 1.0, 'num_input_tokens_seen': 176296635}
 ... (more hidden) ...language loss:  0.9326422214508057
mlp auxiliary loss:  0.10176703333854675
clip auxiliary loss:  0.10176703333854675
language loss:  0.6703107357025146
mlp auxiliary loss:  0.10169613361358643
clip auxiliary loss:  0.10169613361358643
{'loss': 1.0501, 'grad_norm': 2.9474764294122724, 'learning_rate': 2.3073005708429406e-10, 'flos': 15799773818880.0, 'epoch': 1.0, 'num_input_tokens_seen': 176316000}
 ... (more hidden) ...language loss:  0.7272653579711914
mlp auxiliary loss:  0.10205862671136856
clip auxiliary loss:  0.10205862671136856
language loss:  0.9062919020652771
mlp auxiliary loss:  0.10145896673202515
clip auxiliary loss:  0.10145896673202515
{'loss': 0.9114, 'grad_norm': 3.1974805271489273, 'learning_rate': 2.190496559788535e-10, 'flos': 15061932441600.0, 'epoch': 1.0, 'num_input_tokens_seen': 176334005}
 ... (more hidden) ...language loss:  0.6751589775085449
mlp auxiliary loss:  0.10178054869174957
clip auxiliary loss:  0.10178054869174957
language loss:  0.6980088353157043
mlp auxiliary loss:  0.10184939205646515
clip auxiliary loss:  0.10184939205646515
{'loss': 0.9594, 'grad_norm': 5.722904218177138, 'learning_rate': 2.0767262005372265e-10, 'flos': 10502973050880.0, 'epoch': 1.0, 'num_input_tokens_seen': 176351240}
 ... (more hidden) ...language loss:  0.7342566847801208
mlp auxiliary loss:  0.10171352326869965
clip auxiliary loss:  0.10171352326869965
language loss:  0.5410054326057434
mlp auxiliary loss:  0.10181205719709396
clip auxiliary loss:  0.10181205719709396
{'loss': 0.9427, 'grad_norm': 3.7866538031005863, 'learning_rate': 1.965989510346322e-10, 'flos': 13623109386240.0, 'epoch': 1.0, 'num_input_tokens_seen': 176370080}
 ... (more hidden) ...language loss:  0.865737795829773
mlp auxiliary loss:  0.10170523822307587
clip auxiliary loss:  0.10170523822307587
language loss:  0.475758820772171
mlp auxiliary loss:  0.10183027386665344
clip auxiliary loss:  0.10183027386665344
{'loss': 0.891, 'grad_norm': 3.5585625288061666, 'learning_rate': 1.8582865060134955e-10, 'flos': 14225889914880.0, 'epoch': 1.0, 'num_input_tokens_seen': 176387990}
 ... (more hidden) ...language loss:  0.6798021197319031
mlp auxiliary loss:  0.10034561902284622
clip auxiliary loss:  0.10034561902284622
language loss:  0.5380783081054688
mlp auxiliary loss:  0.10034561902284622
clip auxiliary loss:  0.10034561902284622
{'loss': 0.78, 'grad_norm': 0.7894437265568093, 'learning_rate': 1.7536172038790098e-10, 'flos': 41135740170240.0, 'epoch': 1.0, 'num_input_tokens_seen': 176448020}
 ... (more hidden) ...language loss:  0.11592701822519302
mlp auxiliary loss:  0.10159759968519211
clip auxiliary loss:  0.10159759968519211
language loss:  0.7782914042472839
mlp auxiliary loss:  0.10188542306423187
clip auxiliary loss:  0.10188542306423187
{'loss': 0.8844, 'grad_norm': 19.278158623228094, 'learning_rate': 1.651981619819054e-10, 'flos': 19785607618560.0, 'epoch': 1.0, 'num_input_tokens_seen': 176464890}
 ... (more hidden) ...language loss:  0.21982066333293915
mlp auxiliary loss:  0.10186174511909485
clip auxiliary loss:  0.10186174511909485
language loss:  0.6043362021446228
mlp auxiliary loss:  0.10199324041604996
clip auxiliary loss:  0.10199324041604996
[2024-09-18 21:03:14,959] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.8837, 'grad_norm': 4.292183162880044, 'learning_rate': 1.5533797692546257e-10, 'flos': 17084121047040.0, 'epoch': 1.0, 'num_input_tokens_seen': 176483345}
 ... (more hidden) ...language loss:  0.7556624412536621
mlp auxiliary loss:  0.10160095989704132
clip auxiliary loss:  0.10160095989704132
language loss:  0.8178033828735352
mlp auxiliary loss:  0.10182734578847885
clip auxiliary loss:  0.10182734578847885
{'loss': 1.0159, 'grad_norm': 7.897077343321785, 'learning_rate': 1.4578116671404296e-10, 'flos': 13255951564800.0, 'epoch': 1.0, 'num_input_tokens_seen': 176501345}
 ... (more hidden) ...language loss:  0.6600841879844666
mlp auxiliary loss:  0.101666659116745
clip auxiliary loss:  0.101666659116745
language loss:  0.7000731825828552
mlp auxiliary loss:  0.101656474173069
clip auxiliary loss:  0.101656474173069
{'loss': 0.9095, 'grad_norm': 6.145826781533407, 'learning_rate': 1.3652773279759777e-10, 'flos': 14199822028800.0, 'epoch': 1.0, 'num_input_tokens_seen': 176517715}
 ... (more hidden) ...language loss:  0.21003776788711548
mlp auxiliary loss:  0.10204696655273438
clip auxiliary loss:  0.10204696655273438
language loss:  0.7151172757148743
mlp auxiliary loss:  0.10169278830289841
clip auxiliary loss:  0.10169278830289841
{'loss': 0.8154, 'grad_norm': 2.5123089086637553, 'learning_rate': 1.2757767657989305e-10, 'flos': 23612826685440.0, 'epoch': 1.0, 'num_input_tokens_seen': 176541225}
 ... (more hidden) ...language loss:  0.9417633414268494
mlp auxiliary loss:  0.10175804793834686
clip auxiliary loss:  0.10175804793834686
language loss:  0.646842896938324
mlp auxiliary loss:  0.10172202438116074
clip auxiliary loss:  0.10172202438116074
{'loss': 1.0576, 'grad_norm': 4.8650436455865345, 'learning_rate': 1.1893099941850948e-10, 'flos': 16426353500160.0, 'epoch': 1.0, 'num_input_tokens_seen': 176559840}
 ... (more hidden) ...language loss:  0.5171918272972107
mlp auxiliary loss:  0.10198776423931122
clip auxiliary loss:  0.10198776423931122
language loss:  0.6760607361793518
mlp auxiliary loss:  0.10180826485157013
clip auxiliary loss:  0.10180826485157013
{'loss': 0.9564, 'grad_norm': 4.283049713323645, 'learning_rate': 1.105877026252866e-10, 'flos': 16323553566720.0, 'epoch': 1.0, 'num_input_tokens_seen': 176577890}
 ... (more hidden) ...language loss:  0.8352277278900146
mlp auxiliary loss:  0.10173238813877106
clip auxiliary loss:  0.10173238813877106
language loss:  0.53258216381073
mlp auxiliary loss:  0.10202857851982117
clip auxiliary loss:  0.10202857851982117
{'loss': 0.9127, 'grad_norm': 4.105996951778895, 'learning_rate': 1.0254778746565663e-10, 'flos': 9321180487680.0, 'epoch': 1.0, 'num_input_tokens_seen': 176592885}
 ... (more hidden) ...language loss:  0.8698342442512512
mlp auxiliary loss:  0.10168316960334778
clip auxiliary loss:  0.10168316960334778
language loss:  0.6246870756149292
mlp auxiliary loss:  0.10174474120140076
clip auxiliary loss:  0.10174474120140076
{'loss': 0.9282, 'grad_norm': 6.017663387399047, 'learning_rate': 9.481125515953259e-11, 'flos': 10345676636160.0, 'epoch': 1.0, 'num_input_tokens_seen': 176610665}
 ... (more hidden) ...language loss:  0.9102741479873657
mlp auxiliary loss:  0.10203652083873749
clip auxiliary loss:  0.10203652083873749
language loss:  0.6021220684051514
mlp auxiliary loss:  0.10214825719594955
clip auxiliary loss:  0.10214825719594955
{'loss': 0.9935, 'grad_norm': 3.042013192142571, 'learning_rate': 8.737810688064228e-11, 'flos': 18313818501120.0, 'epoch': 1.0, 'num_input_tokens_seen': 176630220}
 ... (more hidden) ...language loss:  0.7548324465751648
mlp auxiliary loss:  0.10190090537071228
clip auxiliary loss:  0.10190090537071228
language loss:  0.8068941831588745
mlp auxiliary loss:  0.10194770246744156
clip auxiliary loss:  0.10194770246744156
{'loss': 0.982, 'grad_norm': 3.2057530484625683, 'learning_rate': 8.024834375608414e-11, 'flos': 15248730501120.0, 'epoch': 1.0, 'num_input_tokens_seen': 176648530}
 ... (more hidden) ...language loss:  0.8558873534202576
mlp auxiliary loss:  0.10037112236022949
clip auxiliary loss:  0.10037112236022949
language loss:  0.5399214625358582
mlp auxiliary loss:  0.10037112236022949
clip auxiliary loss:  0.10037112236022949
[2024-09-18 21:05:15,037] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.8539, 'grad_norm': 0.8351667897743201, 'learning_rate': 7.342196686788149e-11, 'flos': 51718795161600.0, 'epoch': 1.0, 'num_input_tokens_seen': 176701415}
 ... (more hidden) ...language loss:  0.8105598092079163
mlp auxiliary loss:  0.10182594507932663
clip auxiliary loss:  0.10182594507932663
language loss:  0.780452311038971
mlp auxiliary loss:  0.10178850591182709
clip auxiliary loss:  0.10178850591182709
{'loss': 0.8652, 'grad_norm': 3.3004119847071864, 'learning_rate': 6.689897725142834e-11, 'flos': 13960336343040.0, 'epoch': 1.0, 'num_input_tokens_seen': 176720610}
 ... (more hidden) ...language loss:  1.0832704305648804
mlp auxiliary loss:  0.10152462124824524
clip auxiliary loss:  0.10152462124824524
language loss:  0.7691201567649841
mlp auxiliary loss:  0.10184317827224731
clip auxiliary loss:  0.10184317827224731
{'loss': 1.0648, 'grad_norm': 4.7624036563930385, 'learning_rate': 6.067937589615545e-11, 'flos': 11289209856000.0, 'epoch': 1.0, 'num_input_tokens_seen': 176738405}
 ... (more hidden) ...language loss:  0.545995831489563
mlp auxiliary loss:  0.10035157203674316
clip auxiliary loss:  0.10035157203674316
language loss:  0.6224477887153625
mlp auxiliary loss:  0.10035157203674316
clip auxiliary loss:  0.10035157203674316
{'loss': 0.7727, 'grad_norm': 0.7559463490366001, 'learning_rate': 5.476316374575241e-11, 'flos': 42916756869120.0, 'epoch': 1.0, 'num_input_tokens_seen': 176801610}
 ... (more hidden) ...language loss:  0.6767476201057434
mlp auxiliary loss:  0.10168055444955826
clip auxiliary loss:  0.10168055444955826
language loss:  0.8965644240379333
mlp auxiliary loss:  0.10193905234336853
clip auxiliary loss:  0.10193905234336853
{'loss': 0.928, 'grad_norm': 3.6294021185533, 'learning_rate': 4.9150341697723476e-11, 'flos': 15979551068160.0, 'epoch': 1.0, 'num_input_tokens_seen': 176821220}
 ... (more hidden) ...language loss:  0.15146254003047943
mlp auxiliary loss:  0.10201093554496765
clip auxiliary loss:  0.10201093554496765
language loss:  0.6355696320533752
mlp auxiliary loss:  0.10154937207698822
clip auxiliary loss:  0.10154937207698822
[2024-09-18 21:06:16,585] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.8522, 'grad_norm': 3.1709145280331663, 'learning_rate': 4.384091060338768e-11, 'flos': 18525856665600.0, 'epoch': 1.0, 'num_input_tokens_seen': 176841410}
 ... (more hidden) ...language loss:  0.7767942547798157
mlp auxiliary loss:  0.1016320213675499
clip auxiliary loss:  0.1016320213675499
language loss:  0.6536591053009033
mlp auxiliary loss:  0.10175544768571854
clip auxiliary loss:  0.10175544768571854
{'loss': 0.9215, 'grad_norm': 7.069308349791625, 'learning_rate': 3.883487126810081e-11, 'flos': 16087869542400.0, 'epoch': 1.0, 'num_input_tokens_seen': 176860390}
 ... (more hidden) ...language loss:  0.8180040121078491
mlp auxiliary loss:  0.10168969631195068
clip auxiliary loss:  0.10168969631195068
language loss:  0.9519729614257812
mlp auxiliary loss:  0.10152724385261536
clip auxiliary loss:  0.10152724385261536
{'loss': 0.9914, 'grad_norm': 4.066689077430218, 'learning_rate': 3.41322244516995e-11, 'flos': 12915382824960.0, 'epoch': 1.0, 'num_input_tokens_seen': 176878055}
 ... (more hidden) ...language loss:  0.3211483359336853
mlp auxiliary loss:  0.10169988870620728
clip auxiliary loss:  0.10169988870620728
language loss:  0.5795828104019165
mlp auxiliary loss:  0.10179731249809265
clip auxiliary loss:  0.10179731249809265
{'loss': 0.8169, 'grad_norm': 2.685453123363579, 'learning_rate': 2.9732970866946925e-11, 'flos': 23875682304000.0, 'epoch': 1.0, 'num_input_tokens_seen': 176897655}
 ... (more hidden) ...language loss:  0.8951787352561951
mlp auxiliary loss:  0.10171794891357422
clip auxiliary loss:  0.10171794891357422
language loss:  0.6799082159996033
mlp auxiliary loss:  0.10172861069440842
clip auxiliary loss:  0.10172861069440842
{'loss': 0.9708, 'grad_norm': 5.627009223676883, 'learning_rate': 2.563711118175327e-11, 'flos': 10974954270720.0, 'epoch': 1.0, 'num_input_tokens_seen': 176914260}
 ... (more hidden) ...language loss:  1.0332274436950684
mlp auxiliary loss:  0.1019226461648941
clip auxiliary loss:  0.1019226461648941
language loss:  0.8316393494606018
mlp auxiliary loss:  0.10169067233800888
clip auxiliary loss:  0.10169067233800888
{'loss': 1.0204, 'grad_norm': 3.6722746935885526, 'learning_rate': 2.184464601717728e-11, 'flos': 14173692825600.0, 'epoch': 1.0, 'num_input_tokens_seen': 176932295}
 ... (more hidden) ...language loss:  0.5551173686981201
mlp auxiliary loss:  0.10182063281536102
clip auxiliary loss:  0.10182063281536102
language loss:  0.7834030985832214
mlp auxiliary loss:  0.10199590027332306
clip auxiliary loss:  0.10199590027332306
{'loss': 0.9603, 'grad_norm': 3.9696053611955953, 'learning_rate': 1.8355575948758585e-11, 'flos': 14462156451840.0, 'epoch': 1.0, 'num_input_tokens_seen': 176950000}
 ... (more hidden) ...language loss:  0.7889484763145447
mlp auxiliary loss:  0.10161039233207703
clip auxiliary loss:  0.10161039233207703
language loss:  0.27591633796691895
mlp auxiliary loss:  0.10227660089731216
clip auxiliary loss:  0.10227660089731216
[2024-09-18 21:07:41,675] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.9238, 'grad_norm': 3.424974126787681, 'learning_rate': 1.5169901505407424e-11, 'flos': 16870672588800.0, 'epoch': 1.0, 'num_input_tokens_seen': 176966785}
 ... (more hidden) ...language loss:  0.5447383522987366
mlp auxiliary loss:  0.1018608883023262
clip auxiliary loss:  0.1018608883023262
language loss:  0.984382688999176
mlp auxiliary loss:  0.10179807245731354
clip auxiliary loss:  0.10179807245731354
{'loss': 0.931, 'grad_norm': 2.9116190875627206, 'learning_rate': 1.228762317073695e-11, 'flos': 17816903761920.0, 'epoch': 1.0, 'num_input_tokens_seen': 176985335}
 ... (more hidden) ...language loss:  0.7340132594108582
mlp auxiliary loss:  0.10176500678062439
clip auxiliary loss:  0.10176500678062439
language loss:  0.9078848361968994
mlp auxiliary loss:  0.10171733796596527
clip auxiliary loss:  0.10171733796596527
{'loss': 0.9731, 'grad_norm': 12.738876576858326, 'learning_rate': 9.70874138195299e-12, 'flos': 22302963425280.0, 'epoch': 1.0, 'num_input_tokens_seen': 177006965}
 ... (more hidden) ...language loss:  0.8240806460380554
mlp auxiliary loss:  0.10175701975822449
clip auxiliary loss:  0.10175701975822449
language loss:  0.7228362560272217
mlp auxiliary loss:  0.10158710181713104
clip auxiliary loss:  0.10158710181713104
{'loss': 0.9335, 'grad_norm': 3.0918832406604477, 'learning_rate': 7.433256530076093e-12, 'flos': 13885566443520.0, 'epoch': 1.0, 'num_input_tokens_seen': 177026640}
 ... (more hidden) ...language loss:  0.8001128435134888
mlp auxiliary loss:  0.1018289253115654
clip auxiliary loss:  0.1018289253115654
language loss:  0.5867890119552612
mlp auxiliary loss:  0.1017620861530304
clip auxiliary loss:  0.1017620861530304
{'loss': 0.9443, 'grad_norm': 4.172858990127663, 'learning_rate': 5.46116896038562e-12, 'flos': 12179319644160.0, 'epoch': 1.0, 'num_input_tokens_seen': 177040770}
 ... (more hidden) ...language loss:  0.26416879892349243
mlp auxiliary loss:  0.10163817554712296
clip auxiliary loss:  0.10163817554712296
language loss:  0.609991729259491
mlp auxiliary loss:  0.10187827050685883
clip auxiliary loss:  0.10187827050685883
[2024-09-18 21:08:41,315] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happenin
g frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure th
at all ranks flush their caches at the same time
{'loss': 0.8226, 'grad_norm': 3.268094967623681, 'learning_rate': 3.792478972197699e-12, 'flos': 33235386163200.0, 'epoch': 1.0, 'num_input_tokens_seen': 177061075}
 ... (more hidden) ...language loss:  0.8758731484413147
mlp auxiliary loss:  0.1016998440027237
clip auxiliary loss:  0.1016998440027237
language loss:  0.5735718011856079
mlp auxiliary loss:  0.10219642519950867
clip auxiliary loss:  0.10219642519950867
{'loss': 0.8749, 'grad_norm': 4.4113668397005, 'learning_rate': 2.4271868181990895e-12, 'flos': 10712435896320.0, 'epoch': 1.0, 'num_input_tokens_seen': 177077960}
 ... (more hidden) ...language loss:  0.37069642543792725
mlp auxiliary loss:  0.10172589123249054
clip auxiliary loss:  0.10172589123249054
language loss:  0.7846509218215942
mlp auxiliary loss:  0.10187702625989914
clip auxiliary loss:  0.10187702625989914
{'loss': 0.9954, 'grad_norm': 3.4663349714295175, 'learning_rate': 1.3652927060014973e-12, 'flos': 8824633651200.0, 'epoch': 1.0, 'num_input_tokens_seen': 177093275}
 ... (more hidden) ...language loss:  0.5479332804679871
mlp auxiliary loss:  0.1018773764371872
clip auxiliary loss:  0.1018773764371872
language loss:  0.6252668499946594
mlp auxiliary loss:  0.1017894446849823
clip auxiliary loss:  0.1017894446849823
{'loss': 0.8326, 'grad_norm': 4.977373042190333, 'learning_rate': 6.067967965872612e-13, 'flos': 13649698467840.0, 'epoch': 1.0, 'num_input_tokens_seen': 177112605}
 ... (more hidden) ...language loss:  0.10568268597126007
mlp auxiliary loss:  0.10148774832487106
clip auxiliary loss:  0.10148774832487106
language loss:  0.9064153432846069
mlp auxiliary loss:  0.1018008291721344
clip auxiliary loss:  0.1018008291721344
{'loss': 0.9626, 'grad_norm': 3.149551535274832, 'learning_rate': 1.5169920497548615e-13, 'flos': 45061229813760.0, 'epoch': 1.0, 'num_input_tokens_seen': 177136945}
 ... (more hidden) ...language loss:  0.6851313710212708
mlp auxiliary loss:  0.10132160782814026
clip auxiliary loss:  0.10132160782814026
language loss:  0.5696003437042236
mlp auxiliary loss:  0.10056132078170776
clip auxiliary loss:  0.10056132078170776
{'loss': 0.7646, 'grad_norm': 1.7974536793222466, 'learning_rate': 0.0, 'flos': 36421446512640.0, 'epoch': 1.0, 'num_input_tokens_seen': 177185545}
 ... (more hidden) .../cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://
pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f3dcca8000b7f92'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f40c624000b7f8c'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f3662b4000b7f93'
Traceback (most recent call last):
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
OSError: [Errno 16] Device or resource busy: '.nfs000000014f3dccaa000b7f90'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f3662b3000b7f8e'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f40c623000b7f8d'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f5e350d000b7f8f'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f40c626000b7f91'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f3dccac000b7f94'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f40c629000b7f96'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f3dccae000b7f95'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f40c62a000b7f97'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f40c62b000b7f99'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f3dccb0000b7f98'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f3662b5000b7f9a'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f5e3510000b7f9b'
{'train_runtime': 106760.1564, 'train_samples_per_second': 3.116, 'train_steps_per_second': 0.078, 'train_loss': 0.9624144118843656, 'epoch': 1.0, 'num_input_tokens_seen': 177185545}
 ... (more hidden) ...
[2024-09-18 21:17:03,819] [INFO] [launch.py:348:main] Process 2254767 exits successfully.
[2024-09-18 21:17:05,821] [INFO] [launch.py:348:main] Process 2254766 exits successfully.
[2024-09-18 21:17:05,821] [INFO] [launch.py:348:main] Process 2254764 exits successfully.
[2024-09-18 21:18:02,880] [INFO] [launch.py:348:main] Process 2254763 exits successfully.

