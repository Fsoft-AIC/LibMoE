mlp auxiliary loss:  0.1190352737903595
clip auxiliary loss:  0.1190352737903595
{'loss': 1.0659, 'grad_norm': 6.295913509293461, 'learning_rate': 6.4340548232739714e-09, 'flos': 13885201516800.0, 'epoch': 0.98, 'num_input_tokens_seen': 172875305}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 8110/8316 [26:25:38<38:17, 11.15s/it]language loss:  0.7390673756599426
mlp auxiliary loss:  0.11906572431325912
clip auxiliary loss:  0.11906572431325912
language loss:  0.8839266300201416
mlp auxiliary loss:  0.11905111372470856
clip auxiliary loss:  0.11905111372470856
{'loss': 1.013, 'grad_norm': 5.550626447909587, 'learning_rate': 6.371772998692071e-09, 'flos': 16743708942000.0, 'epoch': 0.98, 'num_input_tokens_seen': 172894280}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 8111/8316 [26:25:49<37:52, 11.09s/it]language loss:  0.7269362807273865
mlp auxiliary loss:  0.11905916035175323
clip auxiliary loss:  0.11905916035175323
language loss:  0.7516366243362427
mlp auxiliary loss:  0.11907248944044113
clip auxiliary loss:  0.11907248944044113
{'loss': 0.8709, 'grad_norm': 7.858316824266305, 'learning_rate': 6.309793605927094e-09, 'flos': 14410085077800.0, 'epoch': 0.98, 'num_input_tokens_seen': 172912320}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 8112/8316 [26:26:00<37:32, 11.04s/it]language loss:  0.695649266242981
mlp auxiliary loss:  0.11903926730155945
clip auxiliary loss:  0.11903926730155945
language loss:  0.9103096127510071
mlp auxiliary loss:  0.11907514184713364
clip auxiliary loss:  0.11907514184713364
{'loss': 1.0194, 'grad_norm': 7.657558780834372, 'learning_rate': 6.248116654381297e-09, 'flos': 13724992238880.0, 'epoch': 0.98, 'num_input_tokens_seen': 172930510}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 8113/8316 [26:26:12<37:41, 11.14s/it]language loss:  0.7721073031425476
mlp auxiliary loss:  0.11905303597450256
clip auxiliary loss:  0.11905303597450256
language loss:  0.7208002805709839
mlp auxiliary loss:  0.11907461285591125
clip auxiliary loss:  0.11907461285591125
{'loss': 0.9456, 'grad_norm': 4.962051963278831, 'learning_rate': 6.186742153410751e-09, 'flos': 16767017275680.0, 'epoch': 0.98, 'num_input_tokens_seen': 172949725}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 8114/8316 [26:26:23<37:17, 11.08s/it]language loss:  0.8013227581977844
mlp auxiliary loss:  0.11907128989696503
clip auxiliary loss:  0.11907128989696503
language loss:  0.9215150475502014
mlp auxiliary loss:  0.11904626339673996
clip auxiliary loss:  0.11904626339673996
{'loss': 1.0925, 'grad_norm': 4.516032164277516, 'learning_rate': 6.125670112326453e-09, 'flos': 16324476647520.0, 'epoch': 0.98, 'num_input_tokens_seen': 172968705}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 8115/8316 [26:26:33<36:50, 11.00s/it]language loss:  0.8108067512512207
mlp auxiliary loss:  0.11902987211942673
clip auxiliary loss:  0.11902987211942673
language loss:  0.5074056386947632
mlp auxiliary loss:  0.11905837059020996
clip auxiliary loss:  0.11905837059020996
{'loss': 0.926, 'grad_norm': 4.193468537528825, 'learning_rate': 6.064900540392548e-09, 'flos': 19917299811600.0, 'epoch': 0.98, 'num_input_tokens_seen': 172990520}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 8116/8316 [26:26:45<37:42, 11.31s/it]language loss:  0.9307294487953186
mlp auxiliary loss:  0.1190311461687088
clip auxiliary loss:  0.1190311461687088
language loss:  1.0231915712356567
mlp auxiliary loss:  0.11903929710388184
clip auxiliary loss:  0.11903929710388184
{'loss': 1.0155, 'grad_norm': 5.446102484560758, 'learning_rate': 6.0044334468278835e-09, 'flos': 15773341243080.0, 'epoch': 0.98, 'num_input_tokens_seen': 173009585}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 8117/8316 [26:26:57<37:31, 11.32s/it]language loss:  0.5136576294898987
mlp auxiliary loss:  0.11904406547546387
clip auxiliary loss:  0.11904406547546387
language loss:  0.9159870147705078
mlp auxiliary loss:  0.11905018985271454
clip auxiliary loss:  0.11905018985271454
{'loss': 0.9493, 'grad_norm': 5.39781315639379, 'learning_rate': 5.944268840805345e-09, 'flos': 18684505662960.0, 'epoch': 0.98, 'num_input_tokens_seen': 173030050}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 8118/8316 [26:27:08<37:00, 11.22s/it]language loss:  0.30023831129074097
mlp auxiliary loss:  0.11905549466609955
clip auxiliary loss:  0.11905549466609955
language loss:  0.622808039188385
mlp auxiliary loss:  0.11907260864973068
clip auxiliary loss:  0.11907260864973068
{'loss': 0.8682, 'grad_norm': 4.0337954033186865, 'learning_rate': 5.88440673145163e-09, 'flos': 18917368820760.0, 'epoch': 0.98, 'num_input_tokens_seen': 173050820}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 8119/8316 [26:27:19<36:34, 11.14s/it]language loss:  1.0967206954956055
mlp auxiliary loss:  0.1190325990319252
clip auxiliary loss:  0.1190325990319252
language loss:  0.6921126842498779
mlp auxiliary loss:  0.11905491352081299
clip auxiliary loss:  0.11905491352081299
{'loss': 1.0527, 'grad_norm': 3.861069033337813, 'learning_rate': 5.824847127848142e-09, 'flos': 12758487896520.0, 'epoch': 0.98, 'num_input_tokens_seen': 173069065}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 8120/8316 [26:27:30<36:04, 11.04s/it]language loss:  0.7416037321090698
mlp auxiliary loss:  0.11904571950435638
clip auxiliary loss:  0.11904571950435638
language loss:  0.7170704007148743
mlp auxiliary loss:  0.11905543506145477
clip auxiliary loss:  0.11905543506145477
{'loss': 1.0066, 'grad_norm': 4.4539843349172745, 'learning_rate': 5.765590039029433e-09, 'flos': 15956981500920.0, 'epoch': 0.98, 'num_input_tokens_seen': 173088105}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 8121/8316 [26:27:41<36:04, 11.10s/it]language loss:  0.3853379487991333
mlp auxiliary loss:  0.11907472461462021
clip auxiliary loss:  0.11907472461462021
language loss:  0.7763755917549133
mlp auxiliary loss:  0.1190434992313385
clip auxiliary loss:  0.1190434992313385
{'loss': 0.943, 'grad_norm': 3.4009683239882555, 'learning_rate': 5.706635473985422e-09, 'flos': 26235010243560.0, 'epoch': 0.98, 'num_input_tokens_seen': 173111695}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 8122/8316 [26:27:53<36:57, 11.43s/it]language loss:  0.94071364402771
mlp auxiliary loss:  0.11905977874994278
clip auxiliary loss:  0.11905977874994278
language loss:  0.8662570118904114
mlp auxiliary loss:  0.11905577778816223
clip auxiliary loss:  0.11905577778816223
{'loss': 1.0777, 'grad_norm': 5.372494313133641, 'learning_rate': 5.6479834416591764e-09, 'flos': 15852311404320.0, 'epoch': 0.98, 'num_input_tokens_seen': 173130775}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 8123/8316 [26:28:04<36:19, 11.29s/it]language loss:  0.915888249874115
mlp auxiliary loss:  0.11907018721103668
clip auxiliary loss:  0.11907018721103668
language loss:  0.5671039819717407
mlp auxiliary loss:  0.11905495077371597
clip auxiliary loss:  0.11905495077371597
{'loss': 0.9099, 'grad_norm': 5.455280741347685, 'learning_rate': 5.589633950947803e-09, 'flos': 18368165094600.0, 'epoch': 0.98, 'num_input_tokens_seen': 173147995}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 8124/8316 [26:28:15<35:59, 11.25s/it]language loss:  0.7125033140182495
mlp auxiliary loss:  0.11906261742115021
clip auxiliary loss:  0.11906261742115021
language loss:  0.521675169467926
mlp auxiliary loss:  0.11906671524047852
clip auxiliary loss:  0.11906671524047852
{'loss': 0.9186, 'grad_norm': 3.7300527929911365, 'learning_rate': 5.5315870107035535e-09, 'flos': 15196444580160.0, 'epoch': 0.98, 'num_input_tokens_seen': 173165765}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 8125/8316 [26:28:26<35:47, 11.24s/it]language loss:  0.7975990176200867
mlp auxiliary loss:  0.11906050145626068
clip auxiliary loss:  0.11906050145626068
language loss:  0.8464173674583435
mlp auxiliary loss:  0.11907988786697388
clip auxiliary loss:  0.11907988786697388
{'loss': 1.0061, 'grad_norm': 7.576203596221295, 'learning_rate': 5.473842629731607e-09, 'flos': 9873575320080.0, 'epoch': 0.98, 'num_input_tokens_seen': 173183985}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 8126/8316 [26:28:37<35:17, 11.14s/it]language loss:  0.7974438667297363
mlp auxiliary loss:  0.11905578523874283
clip auxiliary loss:  0.11905578523874283
language loss:  0.9940329194068909
mlp auxiliary loss:  0.11907796561717987
clip auxiliary loss:  0.11907796561717987
{'loss': 1.0044, 'grad_norm': 15.830750318277666, 'learning_rate': 5.416400816792066e-09, 'flos': 12705984209640.0, 'epoch': 0.98, 'num_input_tokens_seen': 173201220}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 8127/8316 [26:28:48<34:48, 11.05s/it]language loss:  0.898746907711029
mlp auxiliary loss:  0.11906220018863678
clip auxiliary loss:  0.11906220018863678
language loss:  0.6447927951812744
mlp auxiliary loss:  0.1190616711974144
clip auxiliary loss:  0.1190616711974144
{'loss': 0.9909, 'grad_norm': 4.758541658899544, 'learning_rate': 5.359261580598407e-09, 'flos': 14512424895840.0, 'epoch': 0.98, 'num_input_tokens_seen': 173216780}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 8128/8316 [26:29:00<35:31, 11.34s/it]language loss:  0.5729695558547974
mlp auxiliary loss:  0.11906333267688751
clip auxiliary loss:  0.11906333267688751
language loss:  0.8087571859359741
mlp auxiliary loss:  0.11906880140304565
clip auxiliary loss:  0.11906880140304565
{'loss': 1.0, 'grad_norm': 11.182330594160138, 'learning_rate': 5.302424929819027e-09, 'flos': 8325636403920.0, 'epoch': 0.98, 'num_input_tokens_seen': 173230510}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 8129/8316 [26:29:11<35:14, 11.31s/it]language loss:  0.8260596990585327
mlp auxiliary loss:  0.11907044053077698
clip auxiliary loss:  0.11907044053077698
language loss:  0.785240113735199
mlp auxiliary loss:  0.11906987428665161
clip auxiliary loss:  0.11906987428665161
{'loss': 0.9509, 'grad_norm': 5.547068417897594, 'learning_rate': 5.24589087307592e-09, 'flos': 9506264142840.0, 'epoch': 0.98, 'num_input_tokens_seen': 173247850}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 8130/8316 [26:29:22<34:37, 11.17s/it]language loss:  0.25554779171943665
mlp auxiliary loss:  0.11907870322465897
clip auxiliary loss:  0.11907870322465897
language loss:  0.6085515022277832
mlp auxiliary loss:  0.11904480308294296
clip auxiliary loss:  0.11904480308294296
[2024-09-21 22:31:27,426] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.8792, 'grad_norm': 6.115300497686271, 'learning_rate': 5.189659418944891e-09, 'flos': 42604233203280.0, 'epoch': 0.98, 'num_input_tokens_seen': 173277745}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 8131/8316 [26:29:35<35:49, 11.62s/it]language loss:  0.8863731026649475
mlp auxiliary loss:  0.11906124651432037
clip auxiliary loss:  0.11906124651432037
language loss:  0.6790120005607605
mlp auxiliary loss:  0.1190710961818695
clip auxiliary loss:  0.1190710961818695
{'loss': 1.0014, 'grad_norm': 5.535542347430845, 'learning_rate': 5.133730575956674e-09, 'flos': 15039178812000.0, 'epoch': 0.98, 'num_input_tokens_seen': 173297135}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 8132/8316 [26:29:46<35:20, 11.52s/it]language loss:  0.8774082660675049
mlp auxiliary loss:  0.1190774068236351
clip auxiliary loss:  0.1190774068236351
language loss:  0.6819373965263367
mlp auxiliary loss:  0.11907555162906647
clip auxiliary loss:  0.11907555162906647
{'loss': 0.9455, 'grad_norm': 4.039429166520067, 'learning_rate': 5.0781043525953696e-09, 'flos': 14829470680080.0, 'epoch': 0.98, 'num_input_tokens_seen': 173314920}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 8133/8316 [26:29:57<34:46, 11.40s/it]language loss:  0.8763899207115173
mlp auxiliary loss:  0.11907202005386353
clip auxiliary loss:  0.11907202005386353
language loss:  0.563228189945221
mlp auxiliary loss:  0.11905096471309662
clip auxiliary loss:  0.11905096471309662
{'loss': 0.9563, 'grad_norm': 3.8722268302618845, 'learning_rate': 5.0227807572995605e-09, 'flos': 16664830765440.0, 'epoch': 0.98, 'num_input_tokens_seen': 173336615}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 8134/8316 [26:30:08<34:10, 11.26s/it]language loss:  0.6684712767601013
mlp auxiliary loss:  0.11906669288873672
clip auxiliary loss:  0.11906669288873672
language loss:  0.8313561081886292
mlp auxiliary loss:  0.11907101422548294
clip auxiliary loss:  0.11907101422548294
{'loss': 0.902, 'grad_norm': 7.583279144876004, 'learning_rate': 4.967759798461646e-09, 'flos': 14855661200400.0, 'epoch': 0.98, 'num_input_tokens_seen': 173354680}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 8135/8316 [26:30:19<33:47, 11.20s/it]language loss:  0.13041947782039642
mlp auxiliary loss:  0.11904589086771011
clip auxiliary loss:  0.11904589086771011
language loss:  0.8572918176651001
mlp auxiliary loss:  0.11908216774463654
clip auxiliary loss:  0.11908216774463654
{'loss': 0.9726, 'grad_norm': 8.534881824821705, 'learning_rate': 4.913041484428282e-09, 'flos': 20152278617040.0, 'epoch': 0.98, 'num_input_tokens_seen': 173374875}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 8136/8316 [26:30:31<33:49, 11.27s/it]language loss:  0.16038388013839722
mlp auxiliary loss:  0.119069904088974
clip auxiliary loss:  0.119069904088974
language loss:  0.7750173807144165
mlp auxiliary loss:  0.1190527155995369
clip auxiliary loss:  0.1190527155995369
{'loss': 0.9838, 'grad_norm': 10.401187432798576, 'learning_rate': 4.858625823500384e-09, 'flos': 18182409189120.0, 'epoch': 0.98, 'num_input_tokens_seen': 173392295}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 8137/8316 [26:30:42<33:17, 11.16s/it]language loss:  0.3923133611679077
mlp auxiliary loss:  0.1190621629357338
clip auxiliary loss:  0.1190621629357338
language loss:  0.7815404534339905
mlp auxiliary loss:  0.11907773464918137
clip auxiliary loss:  0.11907773464918137
{'loss': 0.9674, 'grad_norm': 5.4773723531358955, 'learning_rate': 4.80451282393246e-09, 'flos': 21353700441720.0, 'epoch': 0.98, 'num_input_tokens_seen': 173412000}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 8138/8316 [26:30:54<34:14, 11.54s/it]language loss:  0.2232309728860855
mlp auxiliary loss:  0.1190732941031456
clip auxiliary loss:  0.1190732941031456
language loss:  0.5683935284614563
mlp auxiliary loss:  0.11904142796993256
clip auxiliary loss:  0.11904142796993256
{'loss': 0.9033, 'grad_norm': 6.1003088450808045, 'learning_rate': 4.750702493933722e-09, 'flos': 23063289729120.0, 'epoch': 0.98, 'num_input_tokens_seen': 173431605}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 8139/8316 [26:31:05<33:39, 11.41s/it]language loss:  0.7371671795845032
mlp auxiliary loss:  0.11905671656131744
clip auxiliary loss:  0.11905671656131744
language loss:  0.6889846920967102
mlp auxiliary loss:  0.11906558275222778
clip auxiliary loss:  0.11906558275222778
{'loss': 1.0773, 'grad_norm': 5.20392755957345, 'learning_rate': 4.697194841666974e-09, 'flos': 16586381850720.0, 'epoch': 0.98, 'num_input_tokens_seen': 173450250}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 8140/8316 [26:31:16<33:22, 11.38s/it]language loss:  0.9390583634376526
mlp auxiliary loss:  0.11905176937580109
clip auxiliary loss:  0.11905176937580109
language loss:  0.9011860489845276
mlp auxiliary loss:  0.11908175051212311
clip auxiliary loss:  0.11908175051212311
{'loss': 1.0431, 'grad_norm': 6.3573285683436955, 'learning_rate': 4.6439898752492764e-09, 'flos': 15247292542800.0, 'epoch': 0.98, 'num_input_tokens_seen': 173470110}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 8141/8316 [26:31:28<33:07, 11.36s/it]language loss:  0.6554861068725586
mlp auxiliary loss:  0.11899705231189728
clip auxiliary loss:  0.11899705231189728
language loss:  0.6825873255729675
mlp auxiliary loss:  0.11899705231189728
clip auxiliary loss:  0.11899705231189728
{'loss': 0.8669, 'grad_norm': 0.7487155724254264, 'learning_rate': 4.591087602751731e-09, 'flos': 49338134887200.0, 'epoch': 0.98, 'num_input_tokens_seen': 173531690}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 8142/8316 [26:31:39<33:03, 11.40s/it]language loss:  0.7373876571655273
mlp auxiliary loss:  0.11907586455345154
clip auxiliary loss:  0.11907586455345154
language loss:  0.8084041476249695
mlp auxiliary loss:  0.11904363334178925
clip auxiliary loss:  0.11904363334178925
{'loss': 0.9484, 'grad_norm': 3.871944822800995, 'learning_rate': 4.538488032199916e-09, 'flos': 15220059529440.0, 'epoch': 0.98, 'num_input_tokens_seen': 173549510}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 8143/8316 [26:31:50<32:30, 11.28s/it]language loss:  0.7384272217750549
mlp auxiliary loss:  0.11906034499406815
clip auxiliary loss:  0.11906034499406815
language loss:  0.8548691868782043
mlp auxiliary loss:  0.11906303465366364
clip auxiliary loss:  0.11906303465366364
{'loss': 0.9092, 'grad_norm': 7.4573990504829855, 'learning_rate': 4.486191171572784e-09, 'flos': 14301582286200.0, 'epoch': 0.98, 'num_input_tokens_seen': 173566500}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 8144/8316 [26:32:01<32:06, 11.20s/it]language loss:  0.8409755825996399
mlp auxiliary loss:  0.1190539076924324
clip auxiliary loss:  0.1190539076924324
language loss:  0.8278311491012573
mlp auxiliary loss:  0.11907864362001419
clip auxiliary loss:  0.11907864362001419
{'loss': 1.0083, 'grad_norm': 4.403299195828441, 'learning_rate': 4.434197028803766e-09, 'flos': 16870920833280.0, 'epoch': 0.98, 'num_input_tokens_seen': 173585445}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 8145/8316 [26:32:14<33:08, 11.63s/it]language loss:  0.8246289491653442
mlp auxiliary loss:  0.11903849989175797
clip auxiliary loss:  0.11903849989175797
language loss:  0.673194169998169
mlp auxiliary loss:  0.11907493323087692
clip auxiliary loss:  0.11907493323087692
{'loss': 1.0411, 'grad_norm': 7.192428889130244, 'learning_rate': 4.3825056117805514e-09, 'flos': 16375937841360.0, 'epoch': 0.98, 'num_input_tokens_seen': 173601050}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 8146/8316 [26:32:25<32:19, 11.41s/it]language loss:  0.9169283509254456
mlp auxiliary loss:  0.11904966831207275
clip auxiliary loss:  0.11904966831207275
language loss:  0.7796613574028015
mlp auxiliary loss:  0.11905389279127121
clip auxiliary loss:  0.11905389279127121
{'loss': 1.0197, 'grad_norm': 7.403131399800025, 'learning_rate': 4.331116928344425e-09, 'flos': 10109136695160.0, 'epoch': 0.98, 'num_input_tokens_seen': 173617085}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 8147/8316 [26:32:36<31:34, 11.21s/it]language loss:  0.7238426208496094
mlp auxiliary loss:  0.11907383799552917
clip auxiliary loss:  0.11907383799552917
language loss:  0.5809201002120972
mlp auxiliary loss:  0.11906340718269348
clip auxiliary loss:  0.11906340718269348
{'loss': 0.8534, 'grad_norm': 7.504917058823132, 'learning_rate': 4.28003098629115e-09, 'flos': 11840286607320.0, 'epoch': 0.98, 'num_input_tokens_seen': 173632940}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 8148/8316 [26:32:46<31:05, 11.10s/it]language loss:  0.8119391798973083
mlp auxiliary loss:  0.11904989182949066
clip auxiliary loss:  0.11904989182949066
language loss:  0.7865832448005676
mlp auxiliary loss:  0.11906028538942337
clip auxiliary loss:  0.11906028538942337
{'loss': 1.0139, 'grad_norm': 5.239732238392885, 'learning_rate': 4.229247793370305e-09, 'flos': 17449319912640.0, 'epoch': 0.98, 'num_input_tokens_seen': 173651785}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 8149/8316 [26:32:58<31:00, 11.14s/it]language loss:  0.6780785322189331
mlp auxiliary loss:  0.1190616637468338
clip auxiliary loss:  0.1190616637468338
language loss:  0.9036078453063965
mlp auxiliary loss:  0.11905115097761154
clip auxiliary loss:  0.11905115097761154
{'loss': 0.9279, 'grad_norm': 3.244337430768424, 'learning_rate': 4.178767357285951e-09, 'flos': 19444919937480.0, 'epoch': 0.98, 'num_input_tokens_seen': 173673135}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 8150/8316 [26:33:10<31:30, 11.39s/it]language loss:  0.6942055821418762
mlp auxiliary loss:  0.11905697733163834
clip auxiliary loss:  0.11905697733163834
language loss:  0.758141279220581
mlp auxiliary loss:  0.11903838068246841
clip auxiliary loss:  0.11903838068246841
{'loss': 0.9225, 'grad_norm': 4.339348848075362, 'learning_rate': 4.128589685695516e-09, 'flos': 18710174936760.0, 'epoch': 0.98, 'num_input_tokens_seen': 173693280}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 8151/8316 [26:33:21<31:03, 11.29s/it]language loss:  0.9208865165710449
mlp auxiliary loss:  0.11905490607023239
clip auxiliary loss:  0.11905490607023239
language loss:  0.7306492924690247
mlp auxiliary loss:  0.11907065659761429
clip auxiliary loss:  0.11907065659761429
{'loss': 1.0765, 'grad_norm': 8.4081747694789, 'learning_rate': 4.078714786211135e-09, 'flos': 11836975158840.0, 'epoch': 0.98, 'num_input_tokens_seen': 173708850}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 8152/8316 [26:33:31<30:26, 11.14s/it]language loss:  0.6562487483024597
mlp auxiliary loss:  0.11905832588672638
clip auxiliary loss:  0.11905832588672638
language loss:  0.461576372385025
mlp auxiliary loss:  0.1190684363245964
clip auxiliary loss:  0.1190684363245964
{'loss': 0.9947, 'grad_norm': 11.36181681544181, 'learning_rate': 4.029142666398977e-09, 'flos': 17714045979360.0, 'epoch': 0.98, 'num_input_tokens_seen': 173728735}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 8153/8316 [26:33:43<30:18, 11.16s/it]language loss:  0.9122594594955444
mlp auxiliary loss:  0.1190568059682846
clip auxiliary loss:  0.1190568059682846
language loss:  0.5337218046188354
mlp auxiliary loss:  0.11907300353050232
clip auxiliary loss:  0.11907300353050232
[2024-09-21 22:35:47,150] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 1.03, 'grad_norm': 6.848548913667799, 'learning_rate': 3.979873333778805e-09, 'flos': 16035491738760.0, 'epoch': 0.98, 'num_input_tokens_seen': 173746630}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 8154/8316 [26:33:55<30:45, 11.39s/it]language loss:  0.1290254443883896
mlp auxiliary loss:  0.11906632781028748
clip auxiliary loss:  0.11906632781028748
language loss:  0.7562613487243652
mlp auxiliary loss:  0.11906479299068451
clip auxiliary loss:  0.11906479299068451
{'loss': 0.9591, 'grad_norm': 3.993405150806305, 'learning_rate': 3.930906795824862e-09, 'flos': 27778349925720.0, 'epoch': 0.98, 'num_input_tokens_seen': 173767025}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 8155/8316 [26:34:06<30:11, 11.25s/it]language loss:  0.6240407824516296
mlp auxiliary loss:  0.11906825751066208
clip auxiliary loss:  0.11906825751066208
language loss:  0.9286041855812073
mlp auxiliary loss:  0.11907313019037247
clip auxiliary loss:  0.11907313019037247
{'loss': 0.9988, 'grad_norm': 4.1114062956493695, 'learning_rate': 3.882243059965207e-09, 'flos': 12626860740600.0, 'epoch': 0.98, 'num_input_tokens_seen': 173784460}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 8156/8316 [26:34:16<29:42, 11.14s/it]language loss:  0.6810107231140137
mlp auxiliary loss:  0.11906880885362625
clip auxiliary loss:  0.11906880885362625
language loss:  0.5327101349830627
mlp auxiliary loss:  0.1190725788474083
clip auxiliary loss:  0.1190725788474083
{'loss': 0.8878, 'grad_norm': 4.720933421668449, 'learning_rate': 3.833882133582156e-09, 'flos': 9558767829720.0, 'epoch': 0.98, 'num_input_tokens_seen': 173799840}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 8157/8316 [26:34:28<29:44, 11.22s/it]language loss:  0.8193919658660889
mlp auxiliary loss:  0.11907591670751572
clip auxiliary loss:  0.11907591670751572
language loss:  0.8108222484588623
mlp auxiliary loss:  0.11903845518827438
clip auxiliary loss:  0.11903845518827438
{'loss': 1.0014, 'grad_norm': 4.870450954041888, 'learning_rate': 3.785824024012285e-09, 'flos': 15406612635480.0, 'epoch': 0.98, 'num_input_tokens_seen': 173818560}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 8158/8316 [26:34:39<29:12, 11.09s/it]language loss:  0.8691164255142212
mlp auxiliary loss:  0.11903323233127594
clip auxiliary loss:  0.11903323233127594
language loss:  0.5714841485023499
mlp auxiliary loss:  0.11905714869499207
clip auxiliary loss:  0.11905714869499207
[2024-09-21 22:36:43,091] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 1.0073, 'grad_norm': 4.33145090933517, 'learning_rate': 3.738068738545541e-09, 'flos': 16560099345720.0, 'epoch': 0.98, 'num_input_tokens_seen': 173837365}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 8159/8316 [26:34:51<29:41, 11.34s/it]language loss:  0.6350895762443542
mlp auxiliary loss:  0.11907245218753815
clip auxiliary loss:  0.11907245218753815
language loss:  1.003067970275879
mlp auxiliary loss:  0.11904703080654144
clip auxiliary loss:  0.11904703080654144
{'loss': 1.004, 'grad_norm': 5.5265929881585, 'learning_rate': 3.6906162844265733e-09, 'flos': 12993313394160.0, 'epoch': 0.98, 'num_input_tokens_seen': 173854170}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 8160/8316 [26:35:02<29:19, 11.28s/it]language loss:  0.8234202861785889
mlp auxiliary loss:  0.11903420090675354
clip auxiliary loss:  0.11903420090675354
language loss:  0.8430925607681274
mlp auxiliary loss:  0.1190689206123352
clip auxiliary loss:  0.1190689206123352
{'loss': 0.9395, 'grad_norm': 5.615655724680747, 'learning_rate': 3.643466668853845e-09, 'flos': 16062172844040.0, 'epoch': 0.98, 'num_input_tokens_seen': 173871915}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 8161/8316 [26:35:13<28:52, 11.18s/it]language loss:  0.6562754511833191
mlp auxiliary loss:  0.11906298249959946
clip auxiliary loss:  0.11906298249959946
language loss:  0.32792919874191284
mlp auxiliary loss:  0.11907108128070831
clip auxiliary loss:  0.11907108128070831
[2024-09-21 22:37:16,575] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.9709, 'grad_norm': 7.908510390116218, 'learning_rate': 3.59661989898008e-09, 'flos': 18082920896160.0, 'epoch': 0.98, 'num_input_tokens_seen': 173892690}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 8162/8316 [26:35:24<28:52, 11.25s/it]language loss:  0.6316124796867371
mlp auxiliary loss:  0.1190502941608429
clip auxiliary loss:  0.1190502941608429
language loss:  0.9211276769638062
mlp auxiliary loss:  0.11905748397111893
clip auxiliary loss:  0.11905748397111893
{'loss': 0.9927, 'grad_norm': 6.109560131965599, 'learning_rate': 3.5500759819115934e-09, 'flos': 17790379246440.0, 'epoch': 0.98, 'num_input_tokens_seen': 173912775}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 8163/8316 [26:35:35<28:23, 11.13s/it]language loss:  0.9832584857940674
mlp auxiliary loss:  0.1190648302435875
clip auxiliary loss:  0.1190648302435875
language loss:  0.9278402924537659
mlp auxiliary loss:  0.11906875669956207
clip auxiliary loss:  0.11906875669956207
{'loss': 1.0467, 'grad_norm': 25.60013918838809, 'learning_rate': 3.5038349247094034e-09, 'flos': 14667636339480.0, 'epoch': 0.98, 'num_input_tokens_seen': 173929755}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 8164/8316 [26:35:46<28:02, 11.07s/it]language loss:  0.7085073590278625
mlp auxiliary loss:  0.11905806511640549
clip auxiliary loss:  0.11905806511640549
language loss:  0.5327031016349792
mlp auxiliary loss:  0.11905913800001144
clip auxiliary loss:  0.11905913800001144
{'loss': 0.997, 'grad_norm': 4.243997711008445, 'learning_rate': 3.4578967343878994e-09, 'flos': 12547921240920.0, 'epoch': 0.98, 'num_input_tokens_seen': 173945680}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 8165/8316 [26:35:57<27:51, 11.07s/it]language loss:  0.9244294762611389
mlp auxiliary loss:  0.11907932907342911
clip auxiliary loss:  0.11907932907342911
language loss:  0.6725701689720154
mlp auxiliary loss:  0.11903458833694458
clip auxiliary loss:  0.11903458833694458
{'loss': 1.0343, 'grad_norm': 7.446294889854277, 'learning_rate': 3.4122614179161733e-09, 'flos': 16010619665520.0, 'epoch': 0.98, 'num_input_tokens_seen': 173965360}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 8166/8316 [26:36:08<27:55, 11.17s/it]language loss:  0.7275418043136597
mlp auxiliary loss:  0.11905691027641296
clip auxiliary loss:  0.11905691027641296
language loss:  1.0063852071762085
mlp auxiliary loss:  0.11905328184366226
clip auxiliary loss:  0.11905328184366226
{'loss': 0.9938, 'grad_norm': 4.407067838482113, 'learning_rate': 3.36692898221691e-09, 'flos': 14200223638080.0, 'epoch': 0.98, 'num_input_tokens_seen': 173983445}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 8167/8316 [26:36:20<28:08, 11.33s/it]language loss:  1.035935401916504
mlp auxiliary loss:  0.11906331777572632
clip auxiliary loss:  0.11906331777572632
language loss:  0.7657938599586487
mlp auxiliary loss:  0.11905479431152344
clip auxiliary loss:  0.11905479431152344
{'loss': 0.9584, 'grad_norm': 3.4640509591880373, 'learning_rate': 3.3218994341668305e-09, 'flos': 13335016620720.0, 'epoch': 0.98, 'num_input_tokens_seen': 174002095}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 8168/8316 [26:36:31<27:35, 11.18s/it]language loss:  0.8593006730079651
mlp auxiliary loss:  0.11906478554010391
clip auxiliary loss:  0.11906478554010391
language loss:  0.6993727684020996
mlp auxiliary loss:  0.11906902492046356
clip auxiliary loss:  0.11906902492046356
{'loss': 0.9803, 'grad_norm': 5.392459457733168, 'learning_rate': 3.2771727805971373e-09, 'flos': 18919576453080.0, 'epoch': 0.98, 'num_input_tokens_seen': 174023200}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 8169/8316 [26:36:42<27:14, 11.12s/it]language loss:  0.8904008269309998
mlp auxiliary loss:  0.11907044053077698
clip auxiliary loss:  0.11907044053077698
language loss:  0.8614857792854309
mlp auxiliary loss:  0.11907687038183212
clip auxiliary loss:  0.11907687038183212
{'loss': 0.9927, 'grad_norm': 6.621528147623147, 'learning_rate': 3.232749028292847e-09, 'flos': 15639445131720.0, 'epoch': 0.98, 'num_input_tokens_seen': 174039885}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 8170/8316 [26:36:53<27:00, 11.10s/it]language loss:  0.8597317337989807
mlp auxiliary loss:  0.11908186227083206
clip auxiliary loss:  0.11908186227083206
language loss:  0.8668020963668823
mlp auxiliary loss:  0.11905596405267715
clip auxiliary loss:  0.11905596405267715
{'loss': 1.1071, 'grad_norm': 4.580234289048773, 'learning_rate': 3.188628183992792e-09, 'flos': 15563479803360.0, 'epoch': 0.98, 'num_input_tokens_seen': 174059870}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 8171/8316 [26:37:05<27:25, 11.35s/it]language loss:  0.8950181603431702
mlp auxiliary loss:  0.1189970076084137
clip auxiliary loss:  0.1189970076084137
language loss:  0.5952213406562805
mlp auxiliary loss:  0.1189970076084137
clip auxiliary loss:  0.1189970076084137
[2024-09-21 22:39:08,945] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.8862, 'grad_norm': 0.8346755582982794, 'learning_rate': 3.1448102543902844e-09, 'flos': 42581900491440.0, 'epoch': 0.98, 'num_input_tokens_seen': 174123505}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 8172/8316 [26:37:16<27:25, 11.43s/it]language loss:  0.8279796838760376
mlp auxiliary loss:  0.11906316131353378
clip auxiliary loss:  0.11906316131353378
language loss:  0.8559681177139282
mlp auxiliary loss:  0.11905769258737564
clip auxiliary loss:  0.11905769258737564
{'loss': 0.8902, 'grad_norm': 5.6302928183042695, 'learning_rate': 3.1012952461324515e-09, 'flos': 11368519964400.0, 'epoch': 0.98, 'num_input_tokens_seen': 174142200}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 8173/8316 [26:37:27<26:51, 11.27s/it]language loss:  0.35927316546440125
mlp auxiliary loss:  0.11906518042087555
clip auxiliary loss:  0.11906518042087555
language loss:  1.0263795852661133
mlp auxiliary loss:  0.11907175928354263
clip auxiliary loss:  0.11907175928354263
{'loss': 0.971, 'grad_norm': 10.207546987544543, 'learning_rate': 3.0580831658204575e-09, 'flos': 14380491124320.0, 'epoch': 0.98, 'num_input_tokens_seen': 174159500}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 8174/8316 [26:37:38<26:34, 11.23s/it]language loss:  0.5744091868400574
mlp auxiliary loss:  0.11907033622264862
clip auxiliary loss:  0.11907033622264862
language loss:  0.698361873626709
mlp auxiliary loss:  0.11905844509601593
clip auxiliary loss:  0.11905844509601593
{'loss': 1.0123, 'grad_norm': 12.936029765305396, 'learning_rate': 3.015174020009281e-09, 'flos': 15353587702080.0, 'epoch': 0.98, 'num_input_tokens_seen': 174178545}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 8175/8316 [26:37:49<26:03, 11.09s/it]language loss:  0.8013700246810913
mlp auxiliary loss:  0.11903845518827438
clip auxiliary loss:  0.11903845518827438
language loss:  0.6636435389518738
mlp auxiliary loss:  0.11907022446393967
clip auxiliary loss:  0.11907022446393967
[2024-09-21 22:39:53,611] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.9772, 'grad_norm': 3.559224948952181, 'learning_rate': 2.9725678152086043e-09, 'flos': 16896743414880.0, 'epoch': 0.98, 'num_input_tokens_seen': 174196835}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 8176/8316 [26:38:01<26:25, 11.32s/it]language loss:  0.7973045110702515
mlp auxiliary loss:  0.11905713379383087
clip auxiliary loss:  0.11905713379383087
language loss:  0.9498046636581421
mlp auxiliary loss:  0.1190740317106247
clip auxiliary loss:  0.1190740317106247
{'loss': 1.045, 'grad_norm': 5.728935468228028, 'learning_rate': 2.930264557881257e-09, 'flos': 7953756654240.0, 'epoch': 0.98, 'num_input_tokens_seen': 174211740}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 8177/8316 [26:38:12<25:48, 11.14s/it]language loss:  0.620309591293335
mlp auxiliary loss:  0.11899715662002563
clip auxiliary loss:  0.11899715662002563
language loss:  0.6106967329978943
mlp auxiliary loss:  0.11899715662002563
clip auxiliary loss:  0.11899715662002563
{'loss': 0.8574, 'grad_norm': 0.8068151400676054, 'learning_rate': 2.8882642544452163e-09, 'flos': 42944643096240.0, 'epoch': 0.98, 'num_input_tokens_seen': 174276185}
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 8178/8316 [26:38:24<26:07, 11.36s/it]language loss:  0.7993688583374023
mlp auxiliary loss:  0.1190379410982132
clip auxiliary loss:  0.1190379410982132
language loss:  0.6887674927711487
mlp auxiliary loss:  0.1190684586763382
clip auxiliary loss:  0.1190684586763382
{'loss': 0.9717, 'grad_norm': 4.490770199312388, 'learning_rate': 2.8465669112716083e-09, 'flos': 9611854086240.0, 'epoch': 0.98, 'num_input_tokens_seen': 174293430}
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 8179/8316 [26:38:34<25:30, 11.17s/it]language loss:  0.8415859937667847
mlp auxiliary loss:  0.11906583607196808
clip auxiliary loss:  0.11906583607196808
language loss:  0.8964217305183411
mlp auxiliary loss:  0.11905565857887268
clip auxiliary loss:  0.11905565857887268
{'loss': 0.9722, 'grad_norm': 4.972704771034388, 'learning_rate': 2.8051725346858177e-09, 'flos': 16295189309640.0, 'epoch': 0.98, 'num_input_tokens_seen': 174313410}
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 8180/8316 [26:38:45<25:06, 11.07s/it]language loss:  0.35763323307037354
mlp auxiliary loss:  0.11906908452510834
clip auxiliary loss:  0.11906908452510834
language loss:  0.7528314590454102
mlp auxiliary loss:  0.11907556653022766
clip auxiliary loss:  0.11907556653022766
{'loss': 0.9271, 'grad_norm': 6.3618974861297035, 'learning_rate': 2.7640811309674883e-09, 'flos': 19706917125360.0, 'epoch': 0.98, 'num_input_tokens_seen': 174332630}
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 8181/8316 [26:38:56<24:41, 10.97s/it]language loss:  0.8215948939323425
mlp auxiliary loss:  0.11908221989870071
clip auxiliary loss:  0.11908221989870071
language loss:  0.571570873260498
mlp auxiliary loss:  0.11904332041740417
clip auxiliary loss:  0.11904332041740417
{'loss': 1.0353, 'grad_norm': 3.230404983500037, 'learning_rate': 2.7232927063498557e-09, 'flos': 20834151992160.0, 'epoch': 0.98, 'num_input_tokens_seen': 174352725}
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 8182/8316 [26:39:07<24:38, 11.03s/it]language loss:  0.2688795030117035
mlp auxiliary loss:  0.11907553672790527
clip auxiliary loss:  0.11907553672790527
language loss:  0.5798500180244446
mlp auxiliary loss:  0.11904560029506683
clip auxiliary loss:  0.11904560029506683
[2024-09-21 22:41:12,157] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.9089, 'grad_norm': 4.910113131896003, 'learning_rate': 2.682807267020859e-09, 'flos': 28644354143640.0, 'epoch': 0.98, 'num_input_tokens_seen': 174375205}
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 8183/8316 [26:39:20<25:24, 11.46s/it]language loss:  0.8909817337989807
mlp auxiliary loss:  0.11905192583799362
clip auxiliary loss:  0.11905192583799362
language loss:  0.6372973918914795
mlp auxiliary loss:  0.1190609484910965
clip auxiliary loss:  0.1190609484910965
{'loss': 0.8517, 'grad_norm': 5.51090840655641, 'learning_rate': 2.642624819121808e-09, 'flos': 17188457202480.0, 'epoch': 0.98, 'num_input_tokens_seen': 174395075}
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 8184/8316 [26:39:31<24:52, 11.31s/it]language loss:  0.8584622144699097
mlp auxiliary loss:  0.11906040459871292
clip auxiliary loss:  0.11906040459871292
language loss:  0.5808042883872986
mlp auxiliary loss:  0.11905890703201294
clip auxiliary loss:  0.11905890703201294
{'loss': 0.8456, 'grad_norm': 5.456621071344001, 'learning_rate': 2.6027453687487154e-09, 'flos': 10372023068280.0, 'epoch': 0.98, 'num_input_tokens_seen': 174411885}
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 8185/8316 [26:39:41<24:28, 11.21s/it]language loss:  0.18369613587856293
mlp auxiliary loss:  0.11904951184988022
clip auxiliary loss:  0.11904951184988022
language loss:  0.7471761107444763
mlp auxiliary loss:  0.11906546354293823
clip auxiliary loss:  0.11906546354293823
{'loss': 0.7335, 'grad_norm': 27.85404748731827, 'learning_rate': 2.5631689219509643e-09, 'flos': 15877091492880.0, 'epoch': 0.98, 'num_input_tokens_seen': 174430285}
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████  | 8186/8316 [26:39:53<24:09, 11.15s/it]language loss:  0.7475600242614746
mlp auxiliary loss:  0.11905325204133987
clip auxiliary loss:  0.11905325204133987
language loss:  0.7617040872573853
mlp auxiliary loss:  0.11904475837945938
clip auxiliary loss:  0.11904475837945938
{'loss': 1.0677, 'grad_norm': 5.247903314351108, 'learning_rate': 2.523895484732197e-09, 'flos': 15486012058560.0, 'epoch': 0.98, 'num_input_tokens_seen': 174449460}
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 8187/8316 [26:40:03<23:47, 11.07s/it]language loss:  0.720394492149353
mlp auxiliary loss:  0.11906155198812485
clip auxiliary loss:  0.11906155198812485
language loss:  0.6065441966056824
mlp auxiliary loss:  0.1190543845295906
clip auxiliary loss:  0.1190543845295906
{'loss': 0.9805, 'grad_norm': 3.861101019378818, 'learning_rate': 2.4849250630505357e-09, 'flos': 12779465951640.0, 'epoch': 0.98, 'num_input_tokens_seen': 174467425}
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 8188/8316 [26:40:15<24:14, 11.36s/it]language loss:  0.8347631096839905
mlp auxiliary loss:  0.11907343566417694
clip auxiliary loss:  0.11907343566417694
language loss:  0.4420117139816284
mlp auxiliary loss:  0.11904467642307281
clip auxiliary loss:  0.11904467642307281
{'loss': 0.9596, 'grad_norm': 3.59517051154463, 'learning_rate': 2.4462576628172528e-09, 'flos': 17949852646920.0, 'epoch': 0.98, 'num_input_tokens_seen': 174485775}
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 8189/8316 [26:40:26<23:42, 11.20s/it]language loss:  0.7037017345428467
mlp auxiliary loss:  0.11906470358371735
clip auxiliary loss:  0.11906470358371735
language loss:  0.643887460231781
mlp auxiliary loss:  0.11905833333730698
clip auxiliary loss:  0.11905833333730698
{'loss': 0.9688, 'grad_norm': 10.338992220319925, 'learning_rate': 2.407893289898766e-09, 'flos': 13151682978480.0, 'epoch': 0.98, 'num_input_tokens_seen': 174504525}
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 8190/8316 [26:40:37<23:22, 11.13s/it]language loss:  0.8918964862823486
mlp auxiliary loss:  0.11906206607818604
clip auxiliary loss:  0.11906206607818604
language loss:  0.6918344497680664
mlp auxiliary loss:  0.11907168477773666
clip auxiliary loss:  0.11907168477773666
{'loss': 1.0667, 'grad_norm': 7.283121416979323, 'learning_rate': 2.3698319501144202e-09, 'flos': 19471263765600.0, 'epoch': 0.98, 'num_input_tokens_seen': 174525230}
 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 8191/8316 [26:40:48<22:57, 11.02s/it]language loss:  0.9307900071144104
mlp auxiliary loss:  0.11907906830310822
clip auxiliary loss:  0.11907906830310822
language loss:  0.6805972456932068
mlp auxiliary loss:  0.11903539299964905
clip auxiliary loss:  0.11903539299964905
{'loss': 0.9556, 'grad_norm': 5.323404049691281, 'learning_rate': 2.3320736492382644e-09, 'flos': 13282328964480.0, 'epoch': 0.99, 'num_input_tokens_seen': 174543785}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 8192/8316 [26:40:59<22:42, 10.99s/it]language loss:  0.4911799430847168
mlp auxiliary loss:  0.11906038224697113
clip auxiliary loss:  0.11906038224697113
language loss:  0.31681495904922485
mlp auxiliary loss:  0.11905751377344131
clip auxiliary loss:  0.11905751377344131
{'loss': 0.9114, 'grad_norm': 4.14448016653839, 'learning_rate': 2.29461839299816e-09, 'flos': 15850839649440.0, 'epoch': 0.99, 'num_input_tokens_seen': 174563220}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 8193/8316 [26:41:11<23:00, 11.22s/it]language loss:  0.8177258968353271
mlp auxiliary loss:  0.11905550956726074
clip auxiliary loss:  0.11905550956726074
language loss:  0.932106614112854
mlp auxiliary loss:  0.1190604716539383
clip auxiliary loss:  0.1190604716539383
{'loss': 1.0296, 'grad_norm': 4.637487513276021, 'learning_rate': 2.257466187076229e-09, 'flos': 18757619466240.0, 'epoch': 0.99, 'num_input_tokens_seen': 174582145}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 8194/8316 [26:41:22<22:47, 11.21s/it]language loss:  0.8116418719291687
mlp auxiliary loss:  0.1190687045454979
clip auxiliary loss:  0.1190687045454979
language loss:  0.7402905225753784
mlp auxiliary loss:  0.11905582249164581
clip auxiliary loss:  0.11905582249164581
{'loss': 0.9247, 'grad_norm': 3.7475193719954865, 'learning_rate': 2.2206170371081854e-09, 'flos': 14826741801240.0, 'epoch': 0.99, 'num_input_tokens_seen': 174600450}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 8195/8316 [26:41:33<22:21, 11.09s/it]language loss:  0.9288140535354614
mlp auxiliary loss:  0.1190728098154068
clip auxiliary loss:  0.1190728098154068
language loss:  1.076741337776184
mlp auxiliary loss:  0.11906462907791138
clip auxiliary loss:  0.11906462907791138
{'loss': 1.0776, 'grad_norm': 3.66195260081093, 'learning_rate': 2.1840709486842247e-09, 'flos': 17974663397040.0, 'epoch': 0.99, 'num_input_tokens_seen': 174619790}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 8196/8316 [26:41:44<22:06, 11.05s/it]language loss:  0.8278601765632629
mlp auxiliary loss:  0.11906852573156357
clip auxiliary loss:  0.11906852573156357
language loss:  0.6743295192718506
mlp auxiliary loss:  0.11907647550106049
clip auxiliary loss:  0.11907647550106049
{'loss': 1.0093, 'grad_norm': 21.849742673148636, 'learning_rate': 2.1478279273481335e-09, 'flos': 13518043647360.0, 'epoch': 0.99, 'num_input_tokens_seen': 174637995}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 8197/8316 [26:41:54<21:47, 10.99s/it]language loss:  0.9028429388999939
mlp auxiliary loss:  0.11905840039253235
clip auxiliary loss:  0.11905840039253235
language loss:  0.6435608863830566
mlp auxiliary loss:  0.11906743049621582
clip auxiliary loss:  0.11906743049621582
{'loss': 1.0142, 'grad_norm': 4.232767285527592, 'learning_rate': 2.1118879785981815e-09, 'flos': 24347606394720.0, 'epoch': 0.99, 'num_input_tokens_seen': 174657855}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 8198/8316 [26:42:06<21:39, 11.02s/it]language loss:  0.9610541462898254
mlp auxiliary loss:  0.11904830485582352
clip auxiliary loss:  0.11904830485582352
language loss:  0.9490268230438232
mlp auxiliary loss:  0.11908361315727234
clip auxiliary loss:  0.11908361315727234
{'loss': 1.0227, 'grad_norm': 3.866113502024309, 'learning_rate': 2.0762511078862288e-09, 'flos': 18500405481720.0, 'epoch': 0.99, 'num_input_tokens_seen': 174677920}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 8199/8316 [26:42:17<21:59, 11.28s/it]language loss:  0.7691410183906555
mlp auxiliary loss:  0.11906689405441284
clip auxiliary loss:  0.11906689405441284
language loss:  0.7206202149391174
mlp auxiliary loss:  0.11906357854604721
clip auxiliary loss:  0.11906357854604721
{'loss': 0.8803, 'grad_norm': 7.96029585628756, 'learning_rate': 2.0409173206186183e-09, 'flos': 16848563007960.0, 'epoch': 0.99, 'num_input_tokens_seen': 174696880}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 8200/8316 [26:42:28<21:31, 11.13s/it]language loss:  0.8141866326332092
mlp auxiliary loss:  0.11907538026571274
clip auxiliary loss:  0.11907538026571274
language loss:  0.5365859866142273
mlp auxiliary loss:  0.1190413236618042
clip auxiliary loss:  0.1190413236618042
{'loss': 1.0944, 'grad_norm': 4.908016591411674, 'learning_rate': 2.0058866221550617e-09, 'flos': 14147811935880.0, 'epoch': 0.99, 'num_input_tokens_seen': 174714840}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 8201/8316 [26:42:39<21:08, 11.03s/it]language loss:  0.8698552846908569
mlp auxiliary loss:  0.11907069385051727
clip auxiliary loss:  0.11907069385051727
language loss:  0.597781240940094
mlp auxiliary loss:  0.11905471980571747
clip auxiliary loss:  0.11905471980571747
{'loss': 0.9825, 'grad_norm': 5.698008460421857, 'learning_rate': 1.971159017809976e-09, 'flos': 14069148390240.0, 'epoch': 0.99, 'num_input_tokens_seen': 174732850}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 8202/8316 [26:42:50<21:04, 11.09s/it]language loss:  0.7449691295623779
mlp auxiliary loss:  0.11905784904956818
clip auxiliary loss:  0.11905784904956818
language loss:  0.8177918791770935
mlp auxiliary loss:  0.11903947591781616
clip auxiliary loss:  0.11903947591781616
{'loss': 0.9981, 'grad_norm': 4.709087457719196, 'learning_rate': 1.93673451285159e-09, 'flos': 15379318299000.0, 'epoch': 0.99, 'num_input_tokens_seen': 174751620}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 8203/8316 [26:43:01<20:44, 11.01s/it]language loss:  0.614693284034729
mlp auxiliary loss:  0.1189969927072525
clip auxiliary loss:  0.1189969927072525
language loss:  0.6001675724983215
mlp auxiliary loss:  0.1189969927072525
clip auxiliary loss:  0.1189969927072525
{'loss': 0.8272, 'grad_norm': 0.7433326319475956, 'learning_rate': 1.9026131125019495e-09, 'flos': 37747513972560.0, 'epoch': 0.99, 'num_input_tokens_seen': 174808710}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 8204/8316 [26:43:12<20:43, 11.10s/it]language loss:  0.9698199033737183
mlp auxiliary loss:  0.11905796080827713
clip auxiliary loss:  0.11905796080827713
language loss:  0.9449107646942139
mlp auxiliary loss:  0.11905839294195175
clip auxiliary loss:  0.11905839294195175
[2024-09-21 22:45:16,799] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 1.09, 'grad_norm': 2.3369118179967283, 'learning_rate': 1.8687948219371363e-09, 'flos': 16609812830640.0, 'epoch': 0.99, 'num_input_tokens_seen': 174827655}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 8205/8316 [26:43:24<20:56, 11.32s/it]language loss:  0.984055757522583
mlp auxiliary loss:  0.119077667593956
clip auxiliary loss:  0.119077667593956
language loss:  0.7342140078544617
mlp auxiliary loss:  0.11906611919403076
clip auxiliary loss:  0.11906611919403076
{'loss': 1.1068, 'grad_norm': 7.654646226717547, 'learning_rate': 1.835279646287491e-09, 'flos': 15354047625480.0, 'epoch': 0.99, 'num_input_tokens_seen': 174845385}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 8206/8316 [26:43:35<20:42, 11.30s/it]language loss:  0.9602540731430054
mlp auxiliary loss:  0.11906333267688751
clip auxiliary loss:  0.11906333267688751
language loss:  0.93227219581604
mlp auxiliary loss:  0.11907608807086945
clip auxiliary loss:  0.11907608807086945
{'loss': 1.0015, 'grad_norm': 3.9765385175480157, 'learning_rate': 1.8020675906371685e-09, 'flos': 15824311851960.0, 'epoch': 0.99, 'num_input_tokens_seen': 174864500}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 8207/8316 [26:43:46<20:18, 11.18s/it]language loss:  0.6556980609893799
mlp auxiliary loss:  0.11907794326543808
clip auxiliary loss:  0.11907794326543808
language loss:  0.5943629741668701
mlp auxiliary loss:  0.11903905868530273
clip auxiliary loss:  0.11903905868530273
{'loss': 0.9727, 'grad_norm': 5.049845007604697, 'learning_rate': 1.7691586600243612e-09, 'flos': 18366999955320.0, 'epoch': 0.99, 'num_input_tokens_seen': 174883120}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 8208/8316 [26:43:57<19:59, 11.11s/it]language loss:  0.7024245262145996
mlp auxiliary loss:  0.11905373632907867
clip auxiliary loss:  0.11905373632907867
language loss:  0.7450385093688965
mlp auxiliary loss:  0.11907291412353516
clip auxiliary loss:  0.11907291412353516
{'loss': 1.0934, 'grad_norm': 6.4930691080297605, 'learning_rate': 1.7365528594415202e-09, 'flos': 11603744062320.0, 'epoch': 0.99, 'num_input_tokens_seen': 174896910}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 8209/8316 [26:44:10<20:24, 11.44s/it]language loss:  0.1075790748000145
mlp auxiliary loss:  0.11903896927833557
clip auxiliary loss:  0.11903896927833557
language loss:  0.7647716999053955
mlp auxiliary loss:  0.11905404925346375
clip auxiliary loss:  0.11905404925346375
[2024-09-21 22:46:13,398] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.9059, 'grad_norm': 4.259393968770812, 'learning_rate': 1.7042501938346888e-09, 'flos': 25318342032360.0, 'epoch': 0.99, 'num_input_tokens_seen': 174919360}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 8210/8316 [26:44:21<20:07, 11.40s/it]language loss:  0.7386757731437683
mlp auxiliary loss:  0.1190536618232727
clip auxiliary loss:  0.1190536618232727
language loss:  0.6449970006942749
mlp auxiliary loss:  0.11906524002552032
clip auxiliary loss:  0.11906524002552032
{'loss': 1.0003, 'grad_norm': 5.703831960677359, 'learning_rate': 1.6722506681043913e-09, 'flos': 15222911054520.0, 'epoch': 0.99, 'num_input_tokens_seen': 174938040}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 8211/8316 [26:44:32<19:41, 11.25s/it]language loss:  0.7832529544830322
mlp auxiliary loss:  0.11908064782619476
clip auxiliary loss:  0.11908064782619476
language loss:  0.928950309753418
mlp auxiliary loss:  0.1190742701292038
clip auxiliary loss:  0.1190742701292038
{'loss': 0.9141, 'grad_norm': 10.251062849811294, 'learning_rate': 1.640554287104745e-09, 'flos': 11552129560680.0, 'epoch': 0.99, 'num_input_tokens_seen': 174956035}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 8212/8316 [26:44:43<19:19, 11.14s/it]language loss:  0.9274923205375671
mlp auxiliary loss:  0.11905860900878906
clip auxiliary loss:  0.11905860900878906
language loss:  0.9215561747550964
mlp auxiliary loss:  0.11906637251377106
clip auxiliary loss:  0.11906637251377106
{'loss': 1.0031, 'grad_norm': 4.958822060655942, 'learning_rate': 1.609161055644348e-09, 'flos': 12647532180120.0, 'epoch': 0.99, 'num_input_tokens_seen': 174971680}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 8213/8316 [26:44:54<19:01, 11.08s/it]language loss:  0.26551252603530884
mlp auxiliary loss:  0.11906127631664276
clip auxiliary loss:  0.11906127631664276
language loss:  0.776902437210083
mlp auxiliary loss:  0.11905926465988159
clip auxiliary loss:  0.11905926465988159
{'loss': 0.9009, 'grad_norm': 6.020690212286646, 'learning_rate': 1.5780709784849467e-09, 'flos': 18598667312280.0, 'epoch': 0.99, 'num_input_tokens_seen': 174988420}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 8214/8316 [26:45:06<19:19, 11.37s/it]language loss:  0.8517760634422302
mlp auxiliary loss:  0.11907075345516205
clip auxiliary loss:  0.11907075345516205
language loss:  0.8941353559494019
mlp auxiliary loss:  0.11907246708869934
clip auxiliary loss:  0.11907246708869934
{'loss': 1.0438, 'grad_norm': 4.394808150998406, 'learning_rate': 1.5472840603436565e-09, 'flos': 11310343888920.0, 'epoch': 0.99, 'num_input_tokens_seen': 175005370}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 8215/8316 [26:45:17<19:00, 11.29s/it]language loss:  0.8021248579025269
mlp auxiliary loss:  0.11905909329652786
clip auxiliary loss:  0.11905909329652786
language loss:  0.8132159113883972
mlp auxiliary loss:  0.11904758960008621
clip auxiliary loss:  0.11904758960008621
{'loss': 1.0221, 'grad_norm': 4.396919273039696, 'learning_rate': 1.5168003058900757e-09, 'flos': 13334556697320.0, 'epoch': 0.99, 'num_input_tokens_seen': 175023090}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 8216/8316 [26:45:28<18:40, 11.20s/it]language loss:  0.8469452857971191
mlp auxiliary loss:  0.1190587729215622
clip auxiliary loss:  0.1190587729215622
language loss:  0.7379540205001831
mlp auxiliary loss:  0.11906176060438156
clip auxiliary loss:  0.11906176060438156
{'loss': 1.1446, 'grad_norm': 5.781847453820279, 'learning_rate': 1.4866197197491715e-09, 'flos': 15903987229080.0, 'epoch': 0.99, 'num_input_tokens_seen': 175042170}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 8217/8316 [26:45:39<18:19, 11.11s/it]language loss:  0.6861768364906311
mlp auxiliary loss:  0.11907730996608734
clip auxiliary loss:  0.11907730996608734
language loss:  0.9454566240310669
mlp auxiliary loss:  0.11907310783863068
clip auxiliary loss:  0.11907310783863068
{'loss': 0.9799, 'grad_norm': 5.149236729919963, 'learning_rate': 1.4567423064988371e-09, 'flos': 11079351086280.0, 'epoch': 0.99, 'num_input_tokens_seen': 175059240}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 8218/8316 [26:45:49<17:58, 11.01s/it]language loss:  0.6150149703025818
mlp auxiliary loss:  0.1190648302435875
clip auxiliary loss:  0.1190648302435875
language loss:  0.812474250793457
mlp auxiliary loss:  0.11905574798583984
clip auxiliary loss:  0.11905574798583984
{'loss': 1.0058, 'grad_norm': 10.303006327868086, 'learning_rate': 1.4271680706718913e-09, 'flos': 15269313090960.0, 'epoch': 0.99, 'num_input_tokens_seen': 175076635}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 8219/8316 [26:46:01<17:55, 11.09s/it]language loss:  0.6968900561332703
mlp auxiliary loss:  0.11907587945461273
clip auxiliary loss:  0.11907587945461273
language loss:  0.9469972252845764
mlp auxiliary loss:  0.11906597018241882
clip auxiliary loss:  0.11906597018241882
{'loss': 1.0492, 'grad_norm': 3.679783910958313, 'learning_rate': 1.3978970167543013e-09, 'flos': 19966001465040.0, 'epoch': 0.99, 'num_input_tokens_seen': 175096535}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 8220/8316 [26:46:12<17:39, 11.03s/it]language loss:  1.015725016593933
mlp auxiliary loss:  0.11905719339847565
clip auxiliary loss:  0.11905719339847565
language loss:  0.9173836708068848
mlp auxiliary loss:  0.11908123642206192
clip auxiliary loss:  0.11908123642206192
{'loss': 0.9883, 'grad_norm': 16.379300783019556, 'learning_rate': 1.3689291491867372e-09, 'flos': 9950675126160.0, 'epoch': 0.99, 'num_input_tokens_seen': 175114570}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 8221/8316 [26:46:24<17:57, 11.34s/it]language loss:  0.8755245208740234
mlp auxiliary loss:  0.11906294524669647
clip auxiliary loss:  0.11906294524669647
language loss:  0.6088359355926514
mlp auxiliary loss:  0.11906059831380844
clip auxiliary loss:  0.11906059831380844
{'loss': 0.9646, 'grad_norm': 4.53950012513585, 'learning_rate': 1.3402644723636836e-09, 'flos': 18814630402440.0, 'epoch': 0.99, 'num_input_tokens_seen': 175136320}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 8222/8316 [26:46:35<17:37, 11.25s/it]language loss:  0.6437578201293945
mlp auxiliary loss:  0.11904249340295792
clip auxiliary loss:  0.11904249340295792
language loss:  0.9434395432472229
mlp auxiliary loss:  0.11904445290565491
clip auxiliary loss:  0.11904445290565491
{'loss': 1.0497, 'grad_norm': 3.2733431053663193, 'learning_rate': 1.311902990633218e-09, 'flos': 17950281908760.0, 'epoch': 0.99, 'num_input_tokens_seen': 175155005}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 8223/8316 [26:46:46<17:30, 11.30s/it]language loss:  0.9142044186592102
mlp auxiliary loss:  0.11907825618982315
clip auxiliary loss:  0.11907825618982315
language loss:  0.7935935854911804
mlp auxiliary loss:  0.11907587200403214
clip auxiliary loss:  0.11907587200403214
{'loss': 0.931, 'grad_norm': 4.164775164741977, 'learning_rate': 1.2838447082978987e-09, 'flos': 18762188038680.0, 'epoch': 0.99, 'num_input_tokens_seen': 175175880}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 8224/8316 [26:46:57<17:12, 11.23s/it]language loss:  0.900559663772583
mlp auxiliary loss:  0.11904042214155197
clip auxiliary loss:  0.11904042214155197
language loss:  0.8323673605918884
mlp auxiliary loss:  0.11907118558883667
clip auxiliary loss:  0.11907118558883667
{'loss': 1.0442, 'grad_norm': 8.354745085392763, 'learning_rate': 1.2560896296143208e-09, 'flos': 17294016484320.0, 'epoch': 0.99, 'num_input_tokens_seen': 175194065}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 8225/8316 [26:47:08<16:55, 11.16s/it]language loss:  0.750640869140625
mlp auxiliary loss:  0.11905540525913239
clip auxiliary loss:  0.11905540525913239
language loss:  0.6418821215629578
mlp auxiliary loss:  0.11906305700540543
clip auxiliary loss:  0.11906305700540543
{'loss': 1.0323, 'grad_norm': 4.990234375, 'learning_rate': 1.2286377587926722e-09, 'flos': 13438030993080.0, 'epoch': 0.99, 'num_input_tokens_seen': 175210575}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 8226/8316 [26:47:19<16:33, 11.04s/it]language loss:  0.9645183682441711
mlp auxiliary loss:  0.11907100677490234
clip auxiliary loss:  0.11907100677490234
language loss:  0.6669422388076782
mlp auxiliary loss:  0.1190580427646637
clip auxiliary loss:  0.1190580427646637
{'loss': 0.9814, 'grad_norm': 6.308617332579761, 'learning_rate': 1.2014890999973992e-09, 'flos': 18631051467720.0, 'epoch': 0.99, 'num_input_tokens_seen': 175227215}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 8227/8316 [26:47:31<17:00, 11.47s/it]language loss:  0.7879413962364197
mlp auxiliary loss:  0.11905837804079056
clip auxiliary loss:  0.11905837804079056
language loss:  0.8627692461013794
mlp auxiliary loss:  0.11906076967716217
clip auxiliary loss:  0.11906076967716217
{'loss': 1.0009, 'grad_norm': 5.240677139850849, 'learning_rate': 1.1746436573472073e-09, 'flos': 18108958108680.0, 'epoch': 0.99, 'num_input_tokens_seen': 175248670}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 8228/8316 [26:47:42<16:34, 11.30s/it]language loss:  0.9462924003601074
mlp auxiliary loss:  0.11904735863208771
clip auxiliary loss:  0.11904735863208771
language loss:  0.7136488556861877
mlp auxiliary loss:  0.11907349526882172
clip auxiliary loss:  0.11907349526882172
{'loss': 0.9134, 'grad_norm': 6.015935716407324, 'learning_rate': 1.1481014349141726e-09, 'flos': 14327619498720.0, 'epoch': 0.99, 'num_input_tokens_seen': 175265610}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 8229/8316 [26:47:53<16:09, 11.15s/it]language loss:  0.6040148138999939
mlp auxiliary loss:  0.11905962228775024
clip auxiliary loss:  0.11905962228775024
language loss:  0.7289422750473022
mlp auxiliary loss:  0.11904513835906982
clip auxiliary loss:  0.11904513835906982
{'loss': 1.0611, 'grad_norm': 3.9312835546374534, 'learning_rate': 1.121862436724852e-09, 'flos': 17529853813440.0, 'epoch': 0.99, 'num_input_tokens_seen': 175284170}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 8230/8316 [26:48:04<15:46, 11.01s/it]language loss:  0.4009779691696167
mlp auxiliary loss:  0.11907105147838593
clip auxiliary loss:  0.11907105147838593
language loss:  0.7236156463623047
mlp auxiliary loss:  0.11906781792640686
clip auxiliary loss:  0.11906781792640686
[2024-09-21 22:50:09,112] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.9392, 'grad_norm': 6.061215608750585, 'learning_rate': 1.0959266667598388e-09, 'flos': 15485092211760.0, 'epoch': 0.99, 'num_input_tokens_seen': 175302705}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 8231/8316 [26:48:17<16:21, 11.54s/it]language loss:  0.6322821974754333
mlp auxiliary loss:  0.11906176060438156
clip auxiliary loss:  0.11906176060438156
language loss:  0.7187376618385315
mlp auxiliary loss:  0.11906120181083679
clip auxiliary loss:  0.11906120181083679
{'loss': 0.9654, 'grad_norm': 7.950312568709833, 'learning_rate': 1.0702941289533196e-09, 'flos': 15144646109160.0, 'epoch': 0.99, 'num_input_tokens_seen': 175321100}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 8232/8316 [26:48:27<15:52, 11.34s/it]language loss:  0.7078688740730286
mlp auxiliary loss:  0.11905656009912491
clip auxiliary loss:  0.11905656009912491
language loss:  0.8105511665344238
mlp auxiliary loss:  0.11906243115663528
clip auxiliary loss:  0.11906243115663528
{'loss': 1.1116, 'grad_norm': 4.955604486171617, 'learning_rate': 1.0449648271939615e-09, 'flos': 13145581328040.0, 'epoch': 0.99, 'num_input_tokens_seen': 175337165}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 8233/8316 [26:48:38<15:28, 11.19s/it]language loss:  0.8983908295631409
mlp auxiliary loss:  0.11903808265924454
clip auxiliary loss:  0.11903808265924454
language loss:  0.5352701544761658
mlp auxiliary loss:  0.11906540393829346
clip auxiliary loss:  0.11906540393829346
{'loss': 0.9627, 'grad_norm': 4.067840821489283, 'learning_rate': 1.0199387653240243e-09, 'flos': 16898889724080.0, 'epoch': 0.99, 'num_input_tokens_seen': 175356575}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 8234/8316 [26:48:50<15:34, 11.40s/it]language loss:  0.7134771943092346
mlp auxiliary loss:  0.11906705051660538
clip auxiliary loss:  0.11906705051660538
language loss:  0.7774615287780762
mlp auxiliary loss:  0.11906638741493225
clip auxiliary loss:  0.11906638741493225
{'loss': 0.9324, 'grad_norm': 5.773394882602348, 'learning_rate': 9.952159471400267e-10, 'flos': 11604326631960.0, 'epoch': 0.99, 'num_input_tokens_seen': 175373335}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 8235/8316 [26:49:01<15:15, 11.30s/it]language loss:  0.7315605282783508
mlp auxiliary loss:  0.11905346065759659
clip auxiliary loss:  0.11905346065759659
language loss:  0.857304573059082
mlp auxiliary loss:  0.11906668543815613
clip auxiliary loss:  0.11906668543815613
{'loss': 1.0643, 'grad_norm': 4.345396758863724, 'learning_rate': 9.707963763923022e-10, 'flos': 16030984489440.0, 'epoch': 0.99, 'num_input_tokens_seen': 175392105}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 8236/8316 [26:49:12<14:56, 11.21s/it]language loss:  0.9938693642616272
mlp auxiliary loss:  0.11904804408550262
clip auxiliary loss:  0.11904804408550262
language loss:  0.7842300534248352
mlp auxiliary loss:  0.11907142400741577
clip auxiliary loss:  0.11907142400741577
{'loss': 1.0143, 'grad_norm': 6.353010362152819, 'learning_rate': 9.466800567854427e-10, 'flos': 11420962328160.0, 'epoch': 0.99, 'num_input_tokens_seen': 175410425}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 8237/8316 [26:49:23<14:36, 11.09s/it]language loss:  0.7639249563217163
mlp auxiliary loss:  0.11904088407754898
clip auxiliary loss:  0.11904088407754898
language loss:  0.42575937509536743
mlp auxiliary loss:  0.11904977262020111
clip auxiliary loss:  0.11904977262020111
[2024-09-21 22:51:27,109] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.9107, 'grad_norm': 5.1125780617273575, 'learning_rate': 9.228669919778553e-10, 'flos': 18972448078680.0, 'epoch': 0.99, 'num_input_tokens_seen': 175429070}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 8238/8316 [26:49:35<14:35, 11.22s/it]language loss:  0.8310471177101135
mlp auxiliary loss:  0.11904972791671753
clip auxiliary loss:  0.11904972791671753
language loss:  0.8031123876571655
mlp auxiliary loss:  0.11906002461910248
clip auxiliary loss:  0.11906002461910248
{'loss': 1.0201, 'grad_norm': 5.2482356785330015, 'learning_rate': 8.993571855817617e-10, 'flos': 16428472189800.0, 'epoch': 0.99, 'num_input_tokens_seen': 175447620}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 8239/8316 [26:49:46<14:27, 11.27s/it]language loss:  0.9616904258728027
mlp auxiliary loss:  0.11906830966472626
clip auxiliary loss:  0.11906830966472626
language loss:  0.5349456071853638
mlp auxiliary loss:  0.11905775964260101
clip auxiliary loss:  0.11905775964260101
{'loss': 0.9654, 'grad_norm': 3.8859628220221283, 'learning_rate': 8.761506411638642e-10, 'flos': 15694585712760.0, 'epoch': 0.99, 'num_input_tokens_seen': 175466805}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 8240/8316 [26:49:57<14:08, 11.16s/it]language loss:  0.6756861805915833
mlp auxiliary loss:  0.11904340237379074
clip auxiliary loss:  0.11904340237379074
language loss:  0.918079137802124
mlp auxiliary loss:  0.11907195299863815
clip auxiliary loss:  0.11907195299863815
{'loss': 0.96, 'grad_norm': 3.66103460791203, 'learning_rate': 8.53247362244236e-10, 'flos': 13647800448120.0, 'epoch': 0.99, 'num_input_tokens_seen': 175485335}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 8241/8316 [26:50:08<13:53, 11.12s/it]language loss:  0.837739884853363
mlp auxiliary loss:  0.11905223876237869
clip auxiliary loss:  0.11905223876237869
language loss:  0.62791508436203
mlp auxiliary loss:  0.119046151638031
clip auxiliary loss:  0.119046151638031
{'loss': 0.9128, 'grad_norm': 4.020175833960738, 'learning_rate': 8.306473522976532e-10, 'flos': 16794372935280.0, 'epoch': 0.99, 'num_input_tokens_seen': 175504460}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 8242/8316 [26:50:20<13:57, 11.32s/it]language loss:  0.5109236836433411
mlp auxiliary loss:  0.11906902492046356
clip auxiliary loss:  0.11906902492046356
language loss:  0.6243836879730225
mlp auxiliary loss:  0.11905651539564133
clip auxiliary loss:  0.11905651539564133
{'loss': 0.9491, 'grad_norm': 7.498320073493832, 'learning_rate': 8.083506147522623e-10, 'flos': 16140805728120.0, 'epoch': 0.99, 'num_input_tokens_seen': 175523575}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 8243/8316 [26:50:31<13:40, 11.24s/it]language loss:  0.8861101269721985
mlp auxiliary loss:  0.11906253546476364
clip auxiliary loss:  0.11906253546476364
language loss:  0.7044458985328674
mlp auxiliary loss:  0.11906309425830841
clip auxiliary loss:  0.11906309425830841
{'loss': 1.0762, 'grad_norm': 5.41628696382377, 'learning_rate': 7.863571529906909e-10, 'flos': 9532945248120.0, 'epoch': 0.99, 'num_input_tokens_seen': 175538880}
 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 8244/8316 [26:50:42<13:22, 11.15s/it]language loss:  0.6300091743469238
mlp auxiliary loss:  0.11899696290493011
clip auxiliary loss:  0.11899696290493011
language loss:  0.5797588229179382
mlp auxiliary loss:  0.11899696290493011
clip auxiliary loss:  0.11899696290493011
[2024-09-21 22:52:45,958] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.8794, 'grad_norm': 0.8023366197902545, 'learning_rate': 7.646669703489372e-10, 'flos': 44260761347640.0, 'epoch': 0.99, 'num_input_tokens_seen': 175602910}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 8245/8316 [26:50:53<13:24, 11.33s/it]
language loss:  0.5202412605285645
mlp auxiliary loss:  0.1190599575638771
clip auxiliary loss:  0.1190599575638771
language loss:  0.5065881013870239
mlp auxiliary loss:  0.11907671391963959
clip auxiliary loss:  0.11907671391963959
{'loss': 0.8067, 'grad_norm': 5.432078497834933, 'learning_rate': 7.432800701177023e-10, 'flos': 13308212869200.0, 'epoch': 0.99, 'num_input_tokens_seen': 175620630}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 8246/8316 [26:51:05<13:14, 11.35s/it]
language loss:  0.6514224410057068
mlp auxiliary loss:  0.1189970076084137
clip auxiliary loss:  0.1189970076084137
language loss:  0.5539870262145996
mlp auxiliary loss:  0.1189970076084137
clip auxiliary loss:  0.1189970076084137
[2024-09-21 22:53:09,283] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.8298, 'grad_norm': 0.8447494239139088, 'learning_rate': 7.221964555415017e-10, 'flos': 47209527936240.0, 'epoch': 0.99, 'num_input_tokens_seen': 175680010}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 8247/8316 [26:51:17<13:15, 11.52s/it]
language loss:  0.7073527574539185
mlp auxiliary loss:  0.11907952278852463
clip auxiliary loss:  0.11907952278852463
language loss:  0.4069729149341583
mlp auxiliary loss:  0.11906279623508453
clip auxiliary loss:  0.11906279623508453
{'loss': 0.9726, 'grad_norm': 12.376646606637273, 'learning_rate': 7.01416129818222e-10, 'flos': 11735401879800.0, 'epoch': 0.99, 'num_input_tokens_seen': 175697350}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 8248/8316 [26:51:28<12:49, 11.31s/it]
language loss:  0.16690757870674133
mlp auxiliary loss:  0.11905023455619812
clip auxiliary loss:  0.11905023455619812
language loss:  0.6250206232070923
mlp auxiliary loss:  0.11907178908586502
clip auxiliary loss:  0.11907178908586502
[2024-09-21 22:53:32,502] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.8128, 'grad_norm': 15.900117482645028, 'learning_rate': 6.809390961006745e-10, 'flos': 18081019879440.0, 'epoch': 0.99, 'num_input_tokens_seen': 175717200}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 8249/8316 [26:51:40<12:59, 11.64s/it]
language loss:  0.7738886475563049
mlp auxiliary loss:  0.1190686896443367
clip auxiliary loss:  0.1190686896443367
language loss:  0.34900614619255066
mlp auxiliary loss:  0.11906308680772781
clip auxiliary loss:  0.11906308680772781
{'loss': 0.9157, 'grad_norm': 4.062012863296584, 'learning_rate': 6.607653574948191e-10, 'flos': 17819329307160.0, 'epoch': 0.99, 'num_input_tokens_seen': 175737700}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 8250/8316 [26:51:51<12:31, 11.38s/it]
language loss:  0.8935855031013489
mlp auxiliary loss:  0.11905979365110397
clip auxiliary loss:  0.11905979365110397
language loss:  0.779933512210846
mlp auxiliary loss:  0.1190587654709816
clip auxiliary loss:  0.1190587654709816
{'loss': 1.0464, 'grad_norm': 5.729539708794957, 'learning_rate': 6.408949170613187e-10, 'flos': 15506407544040.0, 'epoch': 0.99, 'num_input_tokens_seen': 175756685}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 8251/8316 [26:52:02<12:18, 11.36s/it]
language loss:  0.7604174613952637
mlp auxiliary loss:  0.11905455589294434
clip auxiliary loss:  0.11905455589294434
language loss:  0.7814413905143738
mlp auxiliary loss:  0.11906205117702484
clip auxiliary loss:  0.11906205117702484
[2024-09-21 22:54:06,767] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 1.0457, 'grad_norm': 4.29148098935918, 'learning_rate': 6.213277778144288e-10, 'flos': 17688039428400.0, 'epoch': 0.99, 'num_input_tokens_seen': 175778050}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 8252/8316 [26:52:14<12:22, 11.61s/it]
language loss:  0.7431114912033081
mlp auxiliary loss:  0.11906912922859192
clip auxiliary loss:  0.11906912922859192
language loss:  0.7945927977561951
mlp auxiliary loss:  0.11905601620674133
clip auxiliary loss:  0.11905601620674133
{'loss': 0.9021, 'grad_norm': 4.9605935765996945, 'learning_rate': 6.020639427224416e-10, 'flos': 15350981469480.0, 'epoch': 0.99, 'num_input_tokens_seen': 175795415}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 8253/8316 [26:52:25<11:58, 11.41s/it]
language loss:  0.231685608625412
mlp auxiliary loss:  0.11905506998300552
clip auxiliary loss:  0.11905506998300552
language loss:  0.8325868248939514
mlp auxiliary loss:  0.11905542016029358
clip auxiliary loss:  0.11905542016029358
{'loss': 0.9464, 'grad_norm': 8.449176310284075, 'learning_rate': 5.831034147076864e-10, 'flos': 17785780012440.0, 'epoch': 0.99, 'num_input_tokens_seen': 175812385}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 8254/8316 [26:52:36<11:35, 11.21s/it]
language loss:  0.5869423747062683
mlp auxiliary loss:  0.11899703741073608
clip auxiliary loss:  0.11899703741073608
language loss:  0.6094763278961182
mlp auxiliary loss:  0.11899703741073608
clip auxiliary loss:  0.11899703741073608
[2024-09-21 22:54:40,417] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.8037, 'grad_norm': 0.7220465072409997, 'learning_rate': 5.644461966463065e-10, 'flos': 49351012742400.0, 'epoch': 0.99, 'num_input_tokens_seen': 175879715}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 8255/8316 [26:52:48<11:37, 11.43s/it]
language loss:  0.7841521501541138
mlp auxiliary loss:  0.119051493704319
clip auxiliary loss:  0.119051493704319
language loss:  0.7568915486335754
mlp auxiliary loss:  0.11905692517757416
clip auxiliary loss:  0.11905692517757416
{'loss': 0.9786, 'grad_norm': 6.529611190273322, 'learning_rate': 5.460922913687049e-10, 'flos': 14856029139120.0, 'epoch': 0.99, 'num_input_tokens_seen': 175898525}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 8256/8316 [26:52:59<11:14, 11.24s/it]
language loss:  0.8055164813995361
mlp auxiliary loss:  0.11906986683607101
clip auxiliary loss:  0.11906986683607101
language loss:  0.7084669470787048
mlp auxiliary loss:  0.11906938254833221
clip auxiliary loss:  0.11906938254833221
{'loss': 0.9722, 'grad_norm': 3.5206614044165536, 'learning_rate': 5.280417016593208e-10, 'flos': 15850808987880.0, 'epoch': 0.99, 'num_input_tokens_seen': 175918035}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 8257/8316 [26:53:09<10:54, 11.10s/it]
language loss:  0.8586729168891907
mlp auxiliary loss:  0.11905799061059952
clip auxiliary loss:  0.11905799061059952
language loss:  0.6562264561653137
mlp auxiliary loss:  0.11904747784137726
clip auxiliary loss:  0.11904747784137726
{'loss': 0.9776, 'grad_norm': 4.031670836727345, 'learning_rate': 5.102944302559642e-10, 'flos': 12311961265560.0, 'epoch': 0.99, 'num_input_tokens_seen': 175935250}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 8258/8316 [26:53:21<10:59, 11.38s/it]
language loss:  0.5958698391914368
mlp auxiliary loss:  0.11906933039426804
clip auxiliary loss:  0.11906933039426804
language loss:  0.6207148432731628
mlp auxiliary loss:  0.11906592547893524
clip auxiliary loss:  0.11906592547893524
{'loss': 1.0065, 'grad_norm': 5.1848785027125395, 'learning_rate': 4.9285047985137e-10, 'flos': 16114400576880.0, 'epoch': 0.99, 'num_input_tokens_seen': 175954390}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 8259/8316 [26:53:32<10:38, 11.19s/it]
language loss:  0.42190808057785034
mlp auxiliary loss:  0.11907025426626205
clip auxiliary loss:  0.11907025426626205
language loss:  0.9207397699356079
mlp auxiliary loss:  0.11906999349594116
clip auxiliary loss:  0.11906999349594116
{'loss': 0.9765, 'grad_norm': 5.932238044083356, 'learning_rate': 4.757098530916436e-10, 'flos': 20047853812920.0, 'epoch': 0.99, 'num_input_tokens_seen': 175974555}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 8260/8316 [26:53:43<10:23, 11.13s/it]
language loss:  0.3830414116382599
mlp auxiliary loss:  0.11907869577407837
clip auxiliary loss:  0.11907869577407837
language loss:  0.7347227931022644
mlp auxiliary loss:  0.11905449628829956
clip auxiliary loss:  0.11905449628829956
{'loss': 1.0074, 'grad_norm': 8.03743093410499, 'learning_rate': 4.5887255257670563e-10, 'flos': 14304679103760.0, 'epoch': 0.99, 'num_input_tokens_seen': 175991315}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 8261/8316 [26:53:54<10:08, 11.06s/it]
language loss:  0.5919137001037598
mlp auxiliary loss:  0.1190667599439621
clip auxiliary loss:  0.1190667599439621
language loss:  0.8485701084136963
mlp auxiliary loss:  0.1190667599439621
clip auxiliary loss:  0.1190667599439621
{'loss': 0.9955, 'grad_norm': 15.795200459125086, 'learning_rate': 4.4233858086117906e-10, 'flos': 15171143245080.0, 'epoch': 0.99, 'num_input_tokens_seen': 176009560}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 8262/8316 [26:54:05<09:53, 10.99s/it]
language loss:  0.9924217462539673
mlp auxiliary loss:  0.11905881762504578
clip auxiliary loss:  0.11905881762504578
language loss:  0.8527011871337891
mlp auxiliary loss:  0.11903911083936691
clip auxiliary loss:  0.11903911083936691
{'loss': 0.8945, 'grad_norm': 6.200285677327194, 'learning_rate': 4.261079404528356e-10, 'flos': 14016859334280.0, 'epoch': 0.99, 'num_input_tokens_seen': 176028760}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 8263/8316 [26:54:16<09:49, 11.12s/it]
language loss:  0.8595508933067322
mlp auxiliary loss:  0.11907470971345901
clip auxiliary loss:  0.11907470971345901
language loss:  0.9569141864776611
mlp auxiliary loss:  0.11905267089605331
clip auxiliary loss:  0.11905267089605331
{'loss': 0.9122, 'grad_norm': 6.493346403501752, 'learning_rate': 4.1018063381437205e-10, 'flos': 15613959827280.0, 'epoch': 0.99, 'num_input_tokens_seen': 176048865}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 8264/8316 [26:54:27<09:34, 11.04s/it]
language loss:  0.7195755243301392
mlp auxiliary loss:  0.11899705231189728
clip auxiliary loss:  0.11899705231189728
language loss:  0.5435246229171753
mlp auxiliary loss:  0.11899705231189728
clip auxiliary loss:  0.11899705231189728
[2024-09-21 22:56:32,152] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.8889, 'grad_norm': 0.9372391019825054, 'learning_rate': 3.9455666336141167e-10, 'flos': 49994584281000.0, 'epoch': 0.99, 'num_input_tokens_seen': 176112365}
 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 8265/8316 [26:54:40<09:43, 11.44s/it]
language loss:  0.8559572696685791
mlp auxiliary loss:  0.11904430389404297
clip auxiliary loss:  0.11904430389404297
language loss:  0.8657811284065247
mlp auxiliary loss:  0.11904626339673996
clip auxiliary loss:  0.11904626339673996
[2024-09-21 22:56:44,150] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 1.0421, 'grad_norm': 9.363893160282613, 'learning_rate': 3.7923603146450267e-10, 'flos': 10659045637200.0, 'epoch': 0.99, 'num_input_tokens_seen': 176128145}
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 8266/8316 [26:54:52<09:40, 11.61s/it]
language loss:  0.7915540337562561
mlp auxiliary loss:  0.11907057464122772
clip auxiliary loss:  0.11907057464122772
language loss:  0.7932321429252625
mlp auxiliary loss:  0.11906447261571884
clip auxiliary loss:  0.11906447261571884
{'loss': 1.016, 'grad_norm': 11.459641334553258, 'learning_rate': 3.642187404473418e-10, 'flos': 12548043887160.0, 'epoch': 0.99, 'num_input_tokens_seen': 176146025}
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 8267/8316 [26:55:03<09:22, 11.48s/it]
language loss:  0.8940191268920898
mlp auxiliary loss:  0.11905871331691742
clip auxiliary loss:  0.11905871331691742
language loss:  0.9225553274154663
mlp auxiliary loss:  0.11906838417053223
clip auxiliary loss:  0.11906838417053223
{'loss': 1.0777, 'grad_norm': 6.5867078156415, 'learning_rate': 3.495047925885508e-10, 'flos': 13596707193000.0, 'epoch': 0.99, 'num_input_tokens_seen': 176164080}
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 8268/8316 [26:55:14<09:02, 11.29s/it]
language loss:  0.9741791486740112
mlp auxiliary loss:  0.11906201392412186
clip auxiliary loss:  0.11906201392412186
language loss:  1.1099990606307983
mlp auxiliary loss:  0.11905232071876526
clip auxiliary loss:  0.11905232071876526
{'loss': 1.0614, 'grad_norm': 7.261321600220992, 'learning_rate': 3.350941901199e-10, 'flos': 12647194902960.0, 'epoch': 0.99, 'num_input_tokens_seen': 176180720}
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 8269/8316 [26:55:24<08:43, 11.14s/it]
language loss:  0.9210659265518188
mlp auxiliary loss:  0.11906267702579498
clip auxiliary loss:  0.11906267702579498
language loss:  0.8641558885574341
mlp auxiliary loss:  0.11905689537525177
clip auxiliary loss:  0.11905689537525177
{'loss': 1.0622, 'grad_norm': 4.655336840905814, 'learning_rate': 3.2098693522764066e-10, 'flos': 13325848814280.0, 'epoch': 0.99, 'num_input_tokens_seen': 176193640}
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 8270/8316 [26:55:35<08:27, 11.02s/it]
language loss:  1.042396903038025
mlp auxiliary loss:  0.11906824260950089
clip auxiliary loss:  0.11906824260950089
language loss:  0.7293930649757385
mlp auxiliary loss:  0.11906905472278595
clip auxiliary loss:  0.11906905472278595
[2024-09-21 22:57:40,526] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 1.0456, 'grad_norm': 8.616623943918984, 'learning_rate': 3.071830300516165e-10, 'flos': 14908747456920.0, 'epoch': 0.99, 'num_input_tokens_seen': 176211190}
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 8271/8316 [26:55:48<08:40, 11.56s/it]
language loss:  0.7884408831596375
mlp auxiliary loss:  0.11907237768173218
clip auxiliary loss:  0.11907237768173218
language loss:  0.7849518060684204
mlp auxiliary loss:  0.11908015608787537
clip auxiliary loss:  0.11908015608787537
{'loss': 0.9184, 'grad_norm': 13.721370229556612, 'learning_rate': 2.9368247668615234e-10, 'flos': 10424036170200.0, 'epoch': 0.99, 'num_input_tokens_seen': 176229500}
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 8272/8316 [26:55:59<08:18, 11.32s/it]
language loss:  0.9516737461090088
mlp auxiliary loss:  0.11905765533447266
clip auxiliary loss:  0.11905765533447266
language loss:  0.20802322030067444
mlp auxiliary loss:  0.11906686425209045
clip auxiliary loss:  0.11906686425209045
[2024-09-21 22:58:03,406] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.8292, 'grad_norm': 5.897070477454416, 'learning_rate': 2.804852771789434e-10, 'flos': 8923143183240.0, 'epoch': 0.99, 'num_input_tokens_seen': 176242520}
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 8273/8316 [26:56:11<08:16, 11.56s/it]
language loss:  0.5158246159553528
mlp auxiliary loss:  0.11907300353050232
clip auxiliary loss:  0.11907300353050232
language loss:  0.41331881284713745
mlp auxiliary loss:  0.11906828731298447
clip auxiliary loss:  0.11906828731298447
{'loss': 0.7853, 'grad_norm': 12.000871626669651, 'learning_rate': 2.675914335321661e-10, 'flos': 13360931187000.0, 'epoch': 0.99, 'num_input_tokens_seen': 176260995}
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 8274/8316 [26:56:22<07:59, 11.42s/it]
language loss:  0.9125993251800537
mlp auxiliary loss:  0.11908378452062607
clip auxiliary loss:  0.11908378452062607
language loss:  0.7074512839317322
mlp auxiliary loss:  0.11906348168849945
clip auxiliary loss:  0.11906348168849945
[2024-09-21 22:58:25,973] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.9999, 'grad_norm': 7.104528632316316, 'learning_rate': 2.550009477018111e-10, 'flos': 17713831348440.0, 'epoch': 1.0, 'num_input_tokens_seen': 176279485}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 8275/8316 [26:56:33<07:48, 11.43s/it]
language loss:  0.38654056191444397
mlp auxiliary loss:  0.11908021569252014
clip auxiliary loss:  0.11908021569252014
language loss:  0.42643141746520996
mlp auxiliary loss:  0.11907585710287094
clip auxiliary loss:  0.11907585710287094
{'loss': 0.8575, 'grad_norm': 3.2371048026998, 'learning_rate': 2.4271382159790634e-10, 'flos': 16875550728840.0, 'epoch': 1.0, 'num_input_tokens_seen': 176296635}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 8276/8316 [26:56:44<07:29, 11.23s/it]
language loss:  0.9363990426063538
mlp auxiliary loss:  0.11904722452163696
clip auxiliary loss:  0.11904722452163696
language loss:  0.6760830283164978
mlp auxiliary loss:  0.11905902624130249
clip auxiliary loss:  0.11905902624130249
{'loss': 1.0869, 'grad_norm': 9.568852328021803, 'learning_rate': 2.3073005708429406e-10, 'flos': 15799777055880.0, 'epoch': 1.0, 'num_input_tokens_seen': 176316000}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 8277/8316 [26:56:55<07:15, 11.17s/it]
language loss:  0.7271086573600769
mlp auxiliary loss:  0.1190524697303772
clip auxiliary loss:  0.1190524697303772
language loss:  0.9034560918807983
mlp auxiliary loss:  0.11904479563236237
clip auxiliary loss:  0.11904479563236237
{'loss': 0.948, 'grad_norm': 4.399881291955589, 'learning_rate': 2.190496559788535e-10, 'flos': 15061935237600.0, 'epoch': 1.0, 'num_input_tokens_seen': 176334005}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 8278/8316 [26:57:06<07:00, 11.06s/it]
language loss:  0.6683356761932373
mlp auxiliary loss:  0.11905951797962189
clip auxiliary loss:  0.11905951797962189
language loss:  0.6566596627235413
mlp auxiliary loss:  0.11906599998474121
clip auxiliary loss:  0.11906599998474121
{'loss': 0.9865, 'grad_norm': 14.937725751759055, 'learning_rate': 2.0767262005372265e-10, 'flos': 10502975669880.0, 'epoch': 1.0, 'num_input_tokens_seen': 176351240}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 8279/8316 [26:57:17<06:50, 11.08s/it]
language loss:  0.737637460231781
mlp auxiliary loss:  0.11906178295612335
clip auxiliary loss:  0.11906178295612335
language loss:  0.5490667223930359
mlp auxiliary loss:  0.11906920373439789
clip auxiliary loss:  0.11906920373439789
{'loss': 0.98, 'grad_norm': 4.2260362787598655, 'learning_rate': 1.965989510346322e-10, 'flos': 13623112344240.0, 'epoch': 1.0, 'num_input_tokens_seen': 176370080}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 8280/8316 [26:57:28<06:36, 11.00s/it]
language loss:  0.8617202639579773
mlp auxiliary loss:  0.11905831843614578
clip auxiliary loss:  0.11905831843614578
language loss:  0.48507174849510193
mlp auxiliary loss:  0.11906622350215912
clip auxiliary loss:  0.11906622350215912
{'loss': 0.9342, 'grad_norm': 4.96889759540269, 'learning_rate': 1.8582865060134955e-10, 'flos': 14225892911880.0, 'epoch': 1.0, 'num_input_tokens_seen': 176387990}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 8281/8316 [26:57:40<06:37, 11.35s/it]
language loss:  0.6812337040901184
mlp auxiliary loss:  0.11899702250957489
clip auxiliary loss:  0.11899702250957489
language loss:  0.53847736120224
mlp auxiliary loss:  0.11899702250957489
clip auxiliary loss:  0.11899702250957489
{'loss': 0.8177, 'grad_norm': 0.7996023665113098, 'learning_rate': 1.7536172038790098e-10, 'flos': 41135749485240.0, 'epoch': 1.0, 'num_input_tokens_seen': 176448020}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 8282/8316 [26:57:51<06:25, 11.33s/it]
language loss:  0.10988211631774902
mlp auxiliary loss:  0.11904563009738922
clip auxiliary loss:  0.11904563009738922
language loss:  0.7744982838630676
mlp auxiliary loss:  0.11905825138092041
clip auxiliary loss:  0.11905825138092041
{'loss': 0.9119, 'grad_norm': 7.912601377491604, 'learning_rate': 1.651981619819054e-10, 'flos': 19785611332560.0, 'epoch': 1.0, 'num_input_tokens_seen': 176464890}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 8283/8316 [26:58:02<06:10, 11.23s/it]
language loss:  0.21018078923225403
mlp auxiliary loss:  0.11906247586011887
clip auxiliary loss:  0.11906247586011887
language loss:  0.6049093008041382
mlp auxiliary loss:  0.11907488107681274
clip auxiliary loss:  0.11907488107681274
{'loss': 0.9166, 'grad_norm': 5.630534522258758, 'learning_rate': 1.5533797692546257e-10, 'flos': 17084124383040.0, 'epoch': 1.0, 'num_input_tokens_seen': 176483345}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 8284/8316 [26:58:13<05:56, 11.15s/it]
language loss:  0.7622263431549072
mlp auxiliary loss:  0.11903468519449234
clip auxiliary loss:  0.11903468519449234
language loss:  0.8162622451782227
mlp auxiliary loss:  0.1190769225358963
clip auxiliary loss:  0.1190769225358963
{'loss': 1.0572, 'grad_norm': 6.303929922730989, 'learning_rate': 1.4578116671404296e-10, 'flos': 13255954474800.0, 'epoch': 1.0, 'num_input_tokens_seen': 176501345}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 8285/8316 [26:58:24<05:43, 11.08s/it]
language loss:  0.6557791233062744
mlp auxiliary loss:  0.11905685067176819
clip auxiliary loss:  0.11905685067176819
language loss:  0.6999959945678711
mlp auxiliary loss:  0.11904646456241608
clip auxiliary loss:  0.11904646456241608
{'loss': 0.9432, 'grad_norm': 4.84166825382559, 'learning_rate': 1.3652773279759777e-10, 'flos': 14199825037800.0, 'epoch': 1.0, 'num_input_tokens_seen': 176517715}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 8286/8316 [26:58:35<05:29, 10.97s/it]
language loss:  0.21207240223884583
mlp auxiliary loss:  0.11907186359167099
clip auxiliary loss:  0.11907186359167099
language loss:  0.7137969732284546
mlp auxiliary loss:  0.11906345188617706
clip auxiliary loss:  0.11906345188617706
{'loss': 0.8529, 'grad_norm': 8.839684943889491, 'learning_rate': 1.2757767657989305e-10, 'flos': 23612830732440.0, 'epoch': 1.0, 'num_input_tokens_seen': 176541225}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 8287/8316 [26:58:46<05:19, 11.02s/it]
language loss:  0.9413776993751526
mlp auxiliary loss:  0.11906658858060837
clip auxiliary loss:  0.11906658858060837
language loss:  0.6438208818435669
mlp auxiliary loss:  0.11906649172306061
clip auxiliary loss:  0.11906649172306061
{'loss': 1.0917, 'grad_norm': 15.685761252776404, 'learning_rate': 1.1893099941850948e-10, 'flos': 16426356542160.0, 'epoch': 1.0, 'num_input_tokens_seen': 176559840}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 8288/8316 [26:58:57<05:07, 10.96s/it]
language loss:  0.5129674077033997
mlp auxiliary loss:  0.11907920986413956
clip auxiliary loss:  0.11907920986413956
language loss:  0.6696808934211731
mlp auxiliary loss:  0.11906417459249496
clip auxiliary loss:  0.11906417459249496
[2024-09-21 23:01:02,259] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.9913, 'grad_norm': 18.660531171003264, 'learning_rate': 1.105877026252866e-10, 'flos': 16323556800720.0, 'epoch': 1.0, 'num_input_tokens_seen': 176577890}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 8289/8316 [26:59:10<05:10, 11.50s/it]
language loss:  0.8431910276412964
mlp auxiliary loss:  0.11905015259981155
clip auxiliary loss:  0.11905015259981155
language loss:  0.5431981682777405
mlp auxiliary loss:  0.11907066404819489
clip auxiliary loss:  0.11907066404819489
{'loss': 0.9502, 'grad_norm': 3.6287857544832978, 'learning_rate': 1.0254778746565663e-10, 'flos': 9321182791680.0, 'epoch': 1.0, 'num_input_tokens_seen': 176592885}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 8290/8316 [26:59:21<04:54, 11.34s/it]
language loss:  0.8715710043907166
mlp auxiliary loss:  0.11905097961425781
clip auxiliary loss:  0.11905097961425781
language loss:  0.6263975501060486
mlp auxiliary loss:  0.1190667599439621
clip auxiliary loss:  0.1190667599439621
{'loss': 0.9608, 'grad_norm': 5.761784957246729, 'learning_rate': 9.481125515953259e-11, 'flos': 10345679240160.0, 'epoch': 1.0, 'num_input_tokens_seen': 176610665}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 8291/8316 [26:59:32<04:42, 11.29s/it]
language loss:  0.9111561179161072
mlp auxiliary loss:  0.1190735325217247
clip auxiliary loss:  0.1190735325217247
language loss:  0.5991153120994568
mlp auxiliary loss:  0.11906550824642181
clip auxiliary loss:  0.11906550824642181
{'loss': 1.028, 'grad_norm': 3.9974546678750853, 'learning_rate': 8.737810688064228e-11, 'flos': 18313821714120.0, 'epoch': 1.0, 'num_input_tokens_seen': 176630220}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 8292/8316 [26:59:43<04:29, 11.22s/it]
language loss:  0.7507357001304626
mlp auxiliary loss:  0.11907324194908142
clip auxiliary loss:  0.11907324194908142
language loss:  0.803996741771698
mlp auxiliary loss:  0.11906617879867554
clip auxiliary loss:  0.11906617879867554
[2024-09-21 23:01:47,734] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 1.0154, 'grad_norm': 3.7986757380264637, 'learning_rate': 8.024834375608414e-11, 'flos': 15248733636120.0, 'epoch': 1.0, 'num_input_tokens_seen': 176648530}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 8293/8316 [26:59:55<04:25, 11.54s/it]
language loss:  0.8577700853347778
mlp auxiliary loss:  0.11899702250957489
clip auxiliary loss:  0.11899702250957489
language loss:  0.539711058139801
mlp auxiliary loss:  0.11899702250957489
clip auxiliary loss:  0.11899702250957489
[2024-09-21 23:01:59,599] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.8904, 'grad_norm': 0.8251555845188953, 'learning_rate': 7.342196686788149e-11, 'flos': 51718804680600.0, 'epoch': 1.0, 'num_input_tokens_seen': 176701415}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 8294/8316 [27:00:07<04:15, 11.64s/it]
language loss:  0.8163565397262573
mlp auxiliary loss:  0.11906514316797256
clip auxiliary loss:  0.11906514316797256
language loss:  0.7757387161254883
mlp auxiliary loss:  0.11906490474939346
clip auxiliary loss:  0.11906490474939346
{'loss': 0.9044, 'grad_norm': 9.309843157994921, 'learning_rate': 6.689897725142834e-11, 'flos': 13960338983040.0, 'epoch': 1.0, 'num_input_tokens_seen': 176720610}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 8295/8316 [27:00:18<04:01, 11.48s/it]
language loss:  1.076292634010315
mlp auxiliary loss:  0.11903108656406403
clip auxiliary loss:  0.11903108656406403
language loss:  0.7817807197570801
mlp auxiliary loss:  0.11906492710113525
clip auxiliary loss:  0.11906492710113525
[2024-09-21 23:02:22,851] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 1.1003, 'grad_norm': 8.291886318754287, 'learning_rate': 6.067937589615545e-11, 'flos': 11289212526000.0, 'epoch': 1.0, 'num_input_tokens_seen': 176738405}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 8296/8316 [27:00:30<03:53, 11.68s/it]
language loss:  0.5464093685150146
mlp auxiliary loss:  0.1189970076084137
clip auxiliary loss:  0.1189970076084137
language loss:  0.6230358481407166
mlp auxiliary loss:  0.1189970076084137
clip auxiliary loss:  0.1189970076084137
{'loss': 0.8111, 'grad_norm': 0.7568596030196181, 'learning_rate': 5.476316374575241e-11, 'flos': 42916766190120.0, 'epoch': 1.0, 'num_input_tokens_seen': 176801610}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 8297/8316 [27:00:42<03:41, 11.65s/it]
language loss:  0.6795260310173035
mlp auxiliary loss:  0.11905622482299805
clip auxiliary loss:  0.11905622482299805
language loss:  0.8930067420005798
mlp auxiliary loss:  0.11903552711009979
clip auxiliary loss:  0.11903552711009979
{'loss': 0.957, 'grad_norm': 8.200503208258159, 'learning_rate': 4.9150341697723476e-11, 'flos': 15979553957160.0, 'epoch': 1.0, 'num_input_tokens_seen': 176821220}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 8298/8316 [27:00:53<03:25, 11.41s/it]
language loss:  0.14559082686901093
mlp auxiliary loss:  0.11908604204654694
clip auxiliary loss:  0.11908604204654694
language loss:  0.622796356678009
mlp auxiliary loss:  0.11904220283031464
clip auxiliary loss:  0.11904220283031464
{'loss': 0.8853, 'grad_norm': 3.642721333897319, 'learning_rate': 4.384091060338768e-11, 'flos': 18525860124600.0, 'epoch': 1.0, 'num_input_tokens_seen': 176841410}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 8299/8316 [27:01:04<03:11, 11.27s/it]
language loss:  0.7836769819259644
mlp auxiliary loss:  0.1190691888332367
clip auxiliary loss:  0.1190691888332367
language loss:  0.6579342484474182
mlp auxiliary loss:  0.11904940009117126
clip auxiliary loss:  0.11904940009117126
{'loss': 0.9566, 'grad_norm': 7.6437380325086455, 'learning_rate': 3.883487126810081e-11, 'flos': 16087872779400.0, 'epoch': 1.0, 'num_input_tokens_seen': 176860390}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 8300/8316 [27:01:15<03:00, 11.28s/it]
language loss:  0.8206018805503845
mlp auxiliary loss:  0.11904406547546387
clip auxiliary loss:  0.11904406547546387
language loss:  0.9739602208137512
mlp auxiliary loss:  0.11905968934297562
clip auxiliary loss:  0.11905968934297562
{'loss': 1.0289, 'grad_norm': 4.6262244588357655, 'learning_rate': 3.41322244516995e-11, 'flos': 12915385725960.0, 'epoch': 1.0, 'num_input_tokens_seen': 176878055}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 8301/8316 [27:01:26<02:47, 11.16s/it]
language loss:  0.3154739737510681
mlp auxiliary loss:  0.1190769150853157
clip auxiliary loss:  0.1190769150853157
language loss:  0.5927735567092896
mlp auxiliary loss:  0.11905190348625183
clip auxiliary loss:  0.11905190348625183
{'loss': 0.8535, 'grad_norm': 6.019342715351363, 'learning_rate': 2.9732970866946925e-11, 'flos': 23875686444000.0, 'epoch': 1.0, 'num_input_tokens_seen': 176897655}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 8302/8316 [27:01:37<02:35, 11.11s/it]
language loss:  0.8944792747497559
mlp auxiliary loss:  0.11905084550380707
clip auxiliary loss:  0.11905084550380707
language loss:  0.681995153427124
mlp auxiliary loss:  0.11905848979949951
clip auxiliary loss:  0.11905848979949951
{'loss': 1.0138, 'grad_norm': 8.79448574000506, 'learning_rate': 2.563711118175327e-11, 'flos': 10974956943720.0, 'epoch': 1.0, 'num_input_tokens_seen': 176914260}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 8303/8316 [27:01:49<02:27, 11.32s/it]
language loss:  1.0381832122802734
mlp auxiliary loss:  0.11905638128519058
clip auxiliary loss:  0.11905638128519058
language loss:  0.8346471190452576
mlp auxiliary loss:  0.11907170712947845
clip auxiliary loss:  0.11907170712947845
{'loss': 1.058, 'grad_norm': 4.067391839538302, 'learning_rate': 2.184464601717728e-11, 'flos': 14173695840600.0, 'epoch': 1.0, 'num_input_tokens_seen': 176932295}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 8304/8316 [27:02:00<02:15, 11.29s/it]
language loss:  0.5571566224098206
mlp auxiliary loss:  0.11905308067798615
clip auxiliary loss:  0.11905308067798615
language loss:  0.7877466678619385
mlp auxiliary loss:  0.11906461417675018
clip auxiliary loss:  0.11906461417675018
{'loss': 0.9978, 'grad_norm': 4.317283520509159, 'learning_rate': 1.8355575948758585e-11, 'flos': 14462159502840.0, 'epoch': 1.0, 'num_input_tokens_seen': 176950000}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 8305/8316 [27:02:11<02:02, 11.15s/it]
language loss:  0.7882661819458008
mlp auxiliary loss:  0.11906247586011887
clip auxiliary loss:  0.11906247586011887
language loss:  0.2886173725128174
mlp auxiliary loss:  0.11906644701957703
clip auxiliary loss:  0.11906644701957703
{'loss': 0.9578, 'grad_norm': 5.951056173493754, 'learning_rate': 1.5169901505407424e-11, 'flos': 16870675540800.0, 'epoch': 1.0, 'num_input_tokens_seen': 176966785}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 8306/8316 [27:02:21<01:50, 11.02s/it]
language loss:  0.5478531718254089
mlp auxiliary loss:  0.11907413601875305
clip auxiliary loss:  0.11907413601875305
language loss:  0.9885261058807373
mlp auxiliary loss:  0.11907333880662918
clip auxiliary loss:  0.11907333880662918
{'loss': 0.968, 'grad_norm': 4.2783106159656725, 'learning_rate': 1.228762317073695e-11, 'flos': 17816907043920.0, 'epoch': 1.0, 'num_input_tokens_seen': 176985335}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 8307/8316 [27:02:33<01:39, 11.06s/it]
language loss:  0.7286106944084167
mlp auxiliary loss:  0.11906556785106659
clip auxiliary loss:  0.11906556785106659
language loss:  0.914776086807251
mlp auxiliary loss:  0.11905545741319656
clip auxiliary loss:  0.11905545741319656
{'loss': 1.0119, 'grad_norm': 5.836966917868644, 'learning_rate': 9.70874138195299e-12, 'flos': 22302967439280.0, 'epoch': 1.0, 'num_input_tokens_seen': 177006965}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 8308/8316 [27:02:44<01:28, 11.08s/it]
language loss:  0.8225339651107788
mlp auxiliary loss:  0.11907321214675903
clip auxiliary loss:  0.11907321214675903
language loss:  0.7194108366966248
mlp auxiliary loss:  0.11904317885637283
clip auxiliary loss:  0.11904317885637283
{'loss': 0.9707, 'grad_norm': 7.490361186996921, 'learning_rate': 7.433256530076093e-12, 'flos': 13885569455520.0, 'epoch': 1.0, 'num_input_tokens_seen': 177026640}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 8309/8316 [27:02:55<01:17, 11.07s/it]
language loss:  0.7983697056770325
mlp auxiliary loss:  0.1190555989742279
clip auxiliary loss:  0.1190555989742279
language loss:  0.5808034539222717
mlp auxiliary loss:  0.11907528340816498
clip auxiliary loss:  0.11907528340816498
{'loss': 0.9808, 'grad_norm': 8.624396261563174, 'learning_rate': 5.46116896038562e-12, 'flos': 12179322278160.0, 'epoch': 1.0, 'num_input_tokens_seen': 177040770}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 8310/8316 [27:03:06<01:05, 10.99s/it]
language loss:  0.2576330602169037
mlp auxiliary loss:  0.11907507479190826
clip auxiliary loss:  0.11907507479190826
language loss:  0.6231279373168945
mlp auxiliary loss:  0.11906914412975311
clip auxiliary loss:  0.11906914412975311
[2024-09-21 23:05:09,550] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.8562, 'grad_norm': 3.853155522407368, 'learning_rate': 3.792478972197699e-12, 'flos': 33235391251200.0, 'epoch': 1.0, 'num_input_tokens_seen': 177061075}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 8311/8316 [27:03:17<00:55, 11.12s/it]
language loss:  0.8833701610565186
mlp auxiliary loss:  0.11906631290912628
clip auxiliary loss:  0.11906631290912628
language loss:  0.5872548818588257
mlp auxiliary loss:  0.11906416714191437
clip auxiliary loss:  0.11906416714191437
{'loss': 0.9098, 'grad_norm': 4.41433990417036, 'learning_rate': 2.4271868181990895e-12, 'flos': 10712438509320.0, 'epoch': 1.0, 'num_input_tokens_seen': 177077960}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 8312/8316 [27:03:29<00:45, 11.42s/it]
language loss:  0.3823350667953491
mlp auxiliary loss:  0.11907607316970825
clip auxiliary loss:  0.11907607316970825
language loss:  0.7844411730766296
mlp auxiliary loss:  0.11906837671995163
clip auxiliary loss:  0.11906837671995163
{'loss': 1.032, 'grad_norm': 4.410106080870532, 'learning_rate': 1.3652927060014973e-12, 'flos': 8824636060200.0, 'epoch': 1.0, 'num_input_tokens_seen': 177093275}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 8313/8316 [27:03:40<00:33, 11.29s/it]
language loss:  0.5506418347358704
mlp auxiliary loss:  0.11907031387090683
clip auxiliary loss:  0.11907031387090683
language loss:  0.6324272155761719
mlp auxiliary loss:  0.11906832456588745
clip auxiliary loss:  0.11906832456588745
{'loss': 0.8687, 'grad_norm': 4.071298783607068, 'learning_rate': 6.067967965872612e-13, 'flos': 13649701464840.0, 'epoch': 1.0, 'num_input_tokens_seen': 177112605}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 8314/8316 [27:03:51<00:22, 11.22s/it]
language loss:  0.11190015822649002
mlp auxiliary loss:  0.1190374568104744
clip auxiliary loss:  0.1190374568104744
language loss:  0.9112127423286438
mlp auxiliary loss:  0.11902806162834167
clip auxiliary loss:  0.11902806162834167
[2024-09-21 23:05:56,350] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happeni
ng frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure
that all ranks flush their caches at the same time
{'loss': 0.9998, 'grad_norm': 19.08065861554902, 'learning_rate': 1.5169920497548615e-13, 'flos': 45061236263760.0, 'epoch': 1.0, 'num_input_tokens_seen': 177136945}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 8315/8316 [27:04:04<00:11, 11.64s/it]
language loss:  0.6867167949676514
mlp auxiliary loss:  0.11903844773769379
clip auxiliary loss:  0.11903844773769379
language loss:  0.5694534778594971
mlp auxiliary loss:  0.11900030821561813
clip auxiliary loss:  0.11900030821561813
{'loss': 0.7958, 'grad_norm': 2.9171146230477514, 'learning_rate': 0.0, 'flos': 36421455827640.0, 'epoch': 1.0, 'num_input_tokens_seen': 177185545}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8316/8316 [27:04:15<00:00, 11.62s/it]
/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/mast
er/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f92537f000bb722'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
Traceback (most recent call last):
Traceback (most recent call last):
OSError: [Errno 16] Device or resource busy: '.nfs000000014fba1c27000bb726'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fba1c23000bb724'
OSError: [Errno 16] Device or resource busy: '.nfs000000014f925382000bb725'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fba1c1f000bb71f'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fba1c21000bb720'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
Traceback (most recent call last):
OSError: [Errno 16] Device or resource busy: '.nfs000000014f925381000bb71e'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fa9130c000bb71d'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
OSError: [Errno 16] Device or resource busy: '.nfs000000014faf15ed000bb71b'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fba1c1c000bb721'
Traceback (most recent call last):
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fba1c2b000bb71c'
OSError: [Errno 16] Device or resource busy: '.nfs000000014fba1c24000bb723'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f925383000bb727'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fba1c2d000bb728'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fba1c30000bb72a'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f925385000bb729'
{'train_runtime': 97926.6258, 'train_samples_per_second': 3.397, 'train_steps_per_second': 0.085, 'train_loss': 0.9941432189775121, 'epoch': 1.0, 'num_input_tokens_seen': 177185545}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8316/8316 [27:12:06<00:00, 11.78s/it]
[2024-09-21 23:14:14,374] [INFO] [launch.py:348:main] Process 481225 exits successfully.
[2024-09-21 23:14:18,378] [INFO] [launch.py:348:main] Process 481224 exits successfully.
[2024-09-21 23:14:18,379] [INFO] [launch.py:348:main] Process 481226 exits successfully.
[2024-09-21 23:15:07,429] [INFO] [launch.py:348:main] Process 481223 exits successfully.

