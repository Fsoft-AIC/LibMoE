Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Loading weight MLP SMoE
Loading weight MLP SMoE
Loading weight MLP SMoE
Loading weight MLP SMoE
Loading weight clip SMoE
Loading weight clip SMoE
Loading weight clip SMoE
Loading weight clip SMoE
Formatting inputs...Skip in lazy mode
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using /home/anonymous/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/anonymous/.cache/torch_extensions/py39_cu121/cpu_adam/build.ninja...
/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compi
lation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.728629112243652 seconds
Using /home/anonymous/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Using /home/anonymous/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Using /home/anonymous/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/anonymous/.cache/torch_extensions/py39_cu121/cpu_adam/build.ninja...
/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compi
lation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.74653697013855 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.40601921081543 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.579676389694214 seconds
^C[2024-09-21 23:25:24,863] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3188345
[rank0]: Traceback (most recent call last):
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train_mem.py", line 11, in <module>
[rank0]:     train(attn_implementation="flash_attention_2")
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train.py", line 1401, in train
[rank0]:     trainer.train()
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 2098, in _inner_training_loop
[rank0]:     model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/accelerate/accelerator.py", line 1291, in prepare
[rank0]:     result = self._prepare_deepspeed(*args)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/accelerate/accelerator.py", line 1758, in _prepare_deepspeed
[rank0]:     engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/__init__.py", line 176, in initialize
[rank0]:     engine = DeepSpeedEngine(args=args,
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 307, in __init__
[rank0]:     self._configure_optimizer(optimizer, model_parameters)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1256, in _configure_optimizer
[rank0]:     self.optimizer = self._configure_zero_optimizer(basic_optimizer)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1579, in _configure_zero_optimizer
[rank0]:     optimizer = DeepSpeedZeroOptimizer_Stage3(
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py", line 125, in __init__
[rank0]:     see_memory_usage("Stage 3 initialize beginning", force=True)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/runtime/utils.py", line 797, in see_memory_usage
[rank0]:     gc.collect()
[rank0]: KeyboardInterrupt
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/bin/deepspeed", line 6, in <module>
    main()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/launcher/runner.py", line 584, in main
    result.wait()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/subprocess.py", line 1189, in wait
    return self._wait(timeout=timeout)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/subprocess.py", line 1933, in _wait
    (pid, sts) = self._try_wait(0)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/subprocess.py", line 1891, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt

(moe) anonymous@ithndgx004:/cm/archive/anonymous/toolkitmoe$ [2024-09-21 23:25:26,524] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3188346
[2024-09-21 23:25:28,024] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3188347
[2024-09-21 23:25:29,569] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3188348
[2024-09-21 23:25:30,994] [INFO] [launch.py:325:sigkill_handler] Main process received SIGINT, exiting
^C
(moe) anonymous@ithndgx004:/cm/archive/anonymous/toolkitmoe$ ^C
(moe) anonymous@ithndgx004:/cm/archive/anonymous/toolkitmoe$ bash /cm/archive/anonymous/toolkitmoe/scripts/train/run_train_all.sh
Staring stage sft
[2024-09-21 23:25:58,223] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-21 23:26:01,212] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-09-21 23:26:01,212] [INFO] [runner.py:568:main] cmd = /cm/archive/anonymous/miniconda3/envs/moe/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgNSwgNiwgN119 --mast
er_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None moe_model/train/train_mem.py --deepspeed ./scripts/zero3_offload.json --model_name_or_path /cm/archive/anonymous/checkpoints/phi3mini-c
lip/pft --version phi3 --data_path /cm/archive/anonymous/data/jsons/llava_v1_5_mix665k_half.json --image_folder /cm/archive/anonymous/data --vision_tower openai/clip-vit-large-patch14-336 --vision_tower_
dir /cm/archive/anonymous/checkpoints/phi3mini-clip/pft/clip.bin --scales 1,3 --pretrain_mm_mlp_adapter /cm/archive/anonymous/checkpoints/phi3mini-clip/pft/mm_projector.bin --mm_projector_type moe --mlp_
smoe true --clip_smoe true --moe_name smoe_sigmoidgating --num_experts 4 --num_selected 2 --balance_loss_coef 0.1 --router_z_loss_coef 0.01 --mm_vision_select_layer -2 --mm_use_im_start_end False --m
m_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /cm/archive/anonymous/checkpoints/phi3mini-clip/sft/smoe_sigmoidgating --num_train_epochs 1
--per_device_train_batch_size 5 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 832 --save_total_limit 13 --learning_rate 4e
-6 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess Tr
ue --report_to none --max_steps -1
[2024-09-21 23:26:03,658] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-21 23:26:04,949] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 5, 6, 7]}
[2024-09-21 23:26:04,949] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-09-21 23:26:04,949] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-09-21 23:26:04,949] [INFO] [launch.py:163:main] dist_world_size=4
[2024-09-21 23:26:04,949] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,5,6,7
[2024-09-21 23:26:04,965] [INFO] [launch.py:253:main] process 3192400 spawned with command: ['/cm/archive/anonymous/miniconda3/envs/moe/bin/python', '-u', 'moe_model/train/train_mem.py', '--local_rank=
0', '--deepspeed', './scripts/zero3_offload.json', '--model_name_or_path', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft', '--version', 'phi3', '--data_path', '/cm/archive/anonymous/data/jsons/lla
va_v1_5_mix665k_half.json', '--image_folder', '/cm/archive/anonymous/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--vision_tower_dir', '/cm/archive/anonymous/checkpoints/phi3mini-clip/p
ft/clip.bin', '--scales', '1,3', '--pretrain_mm_mlp_adapter', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft/mm_projector.bin', '--mm_projector_type', 'moe', '--mlp_smoe', 'true', '--clip_smoe',
'true', '--moe_name', 'smoe_sigmoidgating', '--num_experts', '4', '--num_selected', '2', '--balance_loss_coef', '0.1', '--router_z_loss_coef', '0.01', '--mm_vision_select_layer', '-2', '--mm_use_im_s
tart_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/cm/archive/anonymous/checkpoints/phi3min
i-clip/sft/smoe_sigmoidgating', '--num_train_epochs', '1', '--per_device_train_batch_size', '5', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'n
o', '--save_strategy', 'steps', '--save_steps', '832', '--save_total_limit', '13', '--learning_rate', '4e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--log
ging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none', '--max_steps
', '-1']
[2024-09-21 23:26:04,976] [INFO] [launch.py:253:main] process 3192401 spawned with command: ['/cm/archive/anonymous/miniconda3/envs/moe/bin/python', '-u', 'moe_model/train/train_mem.py', '--local_rank=
1', '--deepspeed', './scripts/zero3_offload.json', '--model_name_or_path', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft', '--version', 'phi3', '--data_path', '/cm/archive/anonymous/data/jsons/lla
va_v1_5_mix665k_half.json', '--image_folder', '/cm/archive/anonymous/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--vision_tower_dir', '/cm/archive/anonymous/checkpoints/phi3mini-clip/p
ft/clip.bin', '--scales', '1,3', '--pretrain_mm_mlp_adapter', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft/mm_projector.bin', '--mm_projector_type', 'moe', '--mlp_smoe', 'true', '--clip_smoe',
'true', '--moe_name', 'smoe_sigmoidgating', '--num_experts', '4', '--num_selected', '2', '--balance_loss_coef', '0.1', '--router_z_loss_coef', '0.01', '--mm_vision_select_layer', '-2', '--mm_use_im_s
tart_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/cm/archive/anonymous/checkpoints/phi3min
i-clip/sft/smoe_sigmoidgating', '--num_train_epochs', '1', '--per_device_train_batch_size', '5', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'n
o', '--save_strategy', 'steps', '--save_steps', '832', '--save_total_limit', '13', '--learning_rate', '4e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--log
ging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none', '--max_steps
', '-1']
[2024-09-21 23:26:04,988] [INFO] [launch.py:253:main] process 3192402 spawned with command: ['/cm/archive/anonymous/miniconda3/envs/moe/bin/python', '-u', 'moe_model/train/train_mem.py', '--local_rank=
2', '--deepspeed', './scripts/zero3_offload.json', '--model_name_or_path', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft', '--version', 'phi3', '--data_path', '/cm/archive/anonymous/data/jsons/lla
va_v1_5_mix665k_half.json', '--image_folder', '/cm/archive/anonymous/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--vision_tower_dir', '/cm/archive/anonymous/checkpoints/phi3mini-clip/p
ft/clip.bin', '--scales', '1,3', '--pretrain_mm_mlp_adapter', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft/mm_projector.bin', '--mm_projector_type', 'moe', '--mlp_smoe', 'true', '--clip_smoe',
'true', '--moe_name', 'smoe_sigmoidgating', '--num_experts', '4', '--num_selected', '2', '--balance_loss_coef', '0.1', '--router_z_loss_coef', '0.01', '--mm_vision_select_layer', '-2', '--mm_use_im_s
tart_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/cm/archive/anonymous/checkpoints/phi3min
i-clip/sft/smoe_sigmoidgating', '--num_train_epochs', '1', '--per_device_train_batch_size', '5', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'n
o', '--save_strategy', 'steps', '--save_steps', '832', '--save_total_limit', '13', '--learning_rate', '4e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--log
ging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none', '--max_steps
', '-1']
[2024-09-21 23:26:05,000] [INFO] [launch.py:253:main] process 3192403 spawned with command: ['/cm/archive/anonymous/miniconda3/envs/moe/bin/python', '-u', 'moe_model/train/train_mem.py', '--local_rank=
3', '--deepspeed', './scripts/zero3_offload.json', '--model_name_or_path', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft', '--version', 'phi3', '--data_path', '/cm/archive/anonymous/data/jsons/lla
va_v1_5_mix665k_half.json', '--image_folder', '/cm/archive/anonymous/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--vision_tower_dir', '/cm/archive/anonymous/checkpoints/phi3mini-clip/p
ft/clip.bin', '--scales', '1,3', '--pretrain_mm_mlp_adapter', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft/mm_projector.bin', '--mm_projector_type', 'moe', '--mlp_smoe', 'true', '--clip_smoe',
'true', '--moe_name', 'smoe_sigmoidgating', '--num_experts', '4', '--num_selected', '2', '--balance_loss_coef', '0.1', '--router_z_loss_coef', '0.01', '--mm_vision_select_layer', '-2', '--mm_use_im_s
tart_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/cm/archive/anonymous/checkpoints/phi3min
i-clip/sft/smoe_sigmoidgating', '--num_train_epochs', '1', '--per_device_train_batch_size', '5', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'n
o', '--save_strategy', 'steps', '--save_steps', '832', '--save_total_limit', '13', '--learning_rate', '4e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--log
ging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none', '--max_steps
', '-1']
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set
the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set
the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set
the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set
the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /home/anonymous/.cache/huggingface/token
Login successful
/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Tr
ansformers. Use `eval_strategy` instead
  warnings.warn(
[2024-09-21 23:26:12,080] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Token is valid (permission: fineGrained).
Your token has been saved to /home/anonymous/.cache/huggingface/token
Login successful
/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Tr
ansformers. Use `eval_strategy` instead
  warnings.warn(
Token is valid (permission: fineGrained).
Your token has been saved to /home/anonymous/.cache/huggingface/token
Login successful
/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Tr
ansformers. Use `eval_strategy` instead
  warnings.warn(
[2024-09-21 23:26:12,413] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-21 23:26:12,416] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-21 23:26:12,416] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-09-21 23:26:12,433] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Token is valid (permission: fineGrained).
Your token has been saved to /home/anonymous/.cache/huggingface/token
Login successful
/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Tr
ansformers. Use `eval_strategy` instead
  warnings.warn(
[2024-09-21 23:26:12,715] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-21 23:26:12,718] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-21 23:26:12,734] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-21 23:26:13,015] [INFO] [comm.py:637:init_distributed] cdb=None
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
=========================================
=============CLIP-vision_tower=========
=========================================
==================================================================================
=============CLIP-vision_tower=========
=========================================

=============CLIP-vision_tower=========
=========================================
=========================================
=============CLIP-vision_tower=========
=========================================
[2024-09-21 23:26:25,874] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 588, num_elems = 4.14B
Loading checkpoint shards:   0%|                                                                                                                                       Loading checkpoint shards:   0%|
                                                                                                                                       Loading checkpoint shards:   0%|
                                                                                                       Loading checkpoint shards:   0%|
                                                                       Loading checkpoint shards:  50%|█████████████████████████████████████████████████████████████████████████▌
                                       Loading checkpoint shards:  50%|█████████████████████████████████████████████████████████████████████████▌
       Loading checkpoint shards:  50%|█████████████████████████████████████████████████████████████████████████▌                                                             Loading checkpoint shards
:  50%|█████████████████████████████████████████████████████████████████████████▌                                                             Loading checkpoint shards: 100%|█████████████████████████
██████████████████████████████████████████████████████████████████████████████████████████████████████████████Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████
██████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.14s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████Loading checkpoint shards: 100%|
███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.14s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████Loading checkpoint shards: 100%|
███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.13s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████Loading checkpoint shards: 100%|
███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.20s/it]
Model: phi3
=========================================
=============CLIP-vision_tower=========
=========================================
Model: phi3
=========================================
=============CLIP-vision_tower=========
=========================================
Model: phi3
=========================================
=============CLIP-vision_tower=========
=========================================
Model: phi3
=========================================
=============CLIP-vision_tower=========
=========================================
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Loading weight MLP SMoE
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Loading weight MLP SMoE
Using CLIP Encoder MoE Layer
Loading weight MLP SMoE
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Loading weight MLP SMoE
Loading weight clip SMoE
Loading weight clip SMoE
Loading weight clip SMoE
Loading weight clip SMoE
Formatting inputs...Skip in lazy mode
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using /home/anonymous/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Using /home/anonymous/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/anonymous/.cache/torch_extensions/py39_cu121/cpu_adam/build.ninja...
/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compi
lation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/anonymous/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.828690767288208 seconds
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.733537197113037 seconds
Time to load cpu_adam op: 3.776923418045044 seconds
Using /home/anonymous/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/anonymous/.cache/torch_extensions/py39_cu121/cpu_adam/build.ninja...
/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compi
lation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 5.019192695617676 seconds
Parameter Offload: Total persistent parameters: 1021952 in 485 params
  0%|                                                                                                                                                                                   | 0/8316 [00:00
<?, ?it/s]Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf2615000bb73c'
Traceback (most recent call last):
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf2612000bb73b'
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf260f000bb73e'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f618001000bb73d'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf261d000bb73f'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f2af8b7000bb743'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf261e000bb740'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf2611000bb741'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf261b000bb742'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf2616000bb745'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
Traceback (most recent call last):
OSError: [Errno 16] Device or resource busy: '.nfs000000014f618000000bb744'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf2610000bb746'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f2af8b6000bb749'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
Traceback (most recent call last):
OSError: [Errno 16] Device or resource busy: '.nfs000000014f618002000bb747'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f2af8b4000bb74a'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf2618000bb748'
[rank2]: Traceback (most recent call last):
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train_mem.py", line 11, in <module>
[rank2]:     train(attn_implementation="flash_attention_2")
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train.py", line 1401, in train
[rank2]:     trainer.train()
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 2279, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3318, in training_step
[rank2]:     loss = self.compute_loss(model, inputs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3363, in compute_loss
[rank2]:     outputs = model(**inputs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1852, in forward
[rank2]:     loss = self.module(*inputs, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/language_model/llava_phi.py", line 288, in forward
[rank2]:     ) = self.prepare_inputs_labels_for_multimodal(
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/llava_arch.py", line 224, in prepare_inputs_labels_for_multimodal
[rank2]:     image_features, auxiliary_loss_clip, vision_id_experts = self.get_model().get_vision_tower()(images, return_id_experts = kwargs['return_id_experts'])
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_encoder.py", line 112, in forward
[rank2]:     out_x, auxilarity_loss, _ = self.vision_model( pixel_values = x,   return_id_experts = return_id_experts)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 231, in forward
[rank2]:     encoder_outputs, auxiliary_losses, stored_history_experts = self.encoder(hidden_states, return_id_experts)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 173, in forward
[rank2]:     layer_outputs = encoder_layer(hidden_states = hidden_states)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 138, in forward
[rank2]:     results, auxiliary_loss, id_experts = self.moelayer(hidden_states, return_id_experts)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 40, in forward
[rank2]:     output = self.compute_moe(gate_logits, output, x)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 32, in compute_moe
[rank2]:     results += weights_slice*self.experts[i](x)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 104, in forward
[rank2]:     hidden_states = self.activation_fn(hidden_states)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/activations.py", line 96, in forward
[rank2]:     return input * torch.sigmoid(1.702 * input)
[rank2]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 204.00 MiB. GPU  has a total capacity of 79.15 GiB of which 172.19 MiB is free. Including non-PyTorch memory, this process
has 78.94 GiB memory in use. Of the allocated memory 76.21 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PY
TORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train_mem.py", line 11, in <module>
[rank3]:     train(attn_implementation="flash_attention_2")
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train.py", line 1401, in train
[rank3]:     trainer.train()
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 2279, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3318, in training_step
[rank3]:     loss = self.compute_loss(model, inputs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3363, in compute_loss
[rank3]:     outputs = model(**inputs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1852, in forward
[rank3]:     loss = self.module(*inputs, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/language_model/llava_phi.py", line 288, in forward
[rank3]:     ) = self.prepare_inputs_labels_for_multimodal(
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/llava_arch.py", line 224, in prepare_inputs_labels_for_multimodal
[rank3]:     image_features, auxiliary_loss_clip, vision_id_experts = self.get_model().get_vision_tower()(images, return_id_experts = kwargs['return_id_experts'])
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_encoder.py", line 112, in forward
[rank3]:     out_x, auxilarity_loss, _ = self.vision_model( pixel_values = x,   return_id_experts = return_id_experts)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 231, in forward
[rank3]:     encoder_outputs, auxiliary_losses, stored_history_experts = self.encoder(hidden_states, return_id_experts)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 173, in forward
[rank3]:     layer_outputs = encoder_layer(hidden_states = hidden_states)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 138, in forward
[rank3]:     results, auxiliary_loss, id_experts = self.moelayer(hidden_states, return_id_experts)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 40, in forward
[rank3]:     output = self.compute_moe(gate_logits, output, x)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 32, in compute_moe
[rank3]:     results += weights_slice*self.experts[i](x)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 104, in forward
[rank3]:     hidden_states = self.activation_fn(hidden_states)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/activations.py", line 96, in forward
[rank3]:     return input * torch.sigmoid(1.702 * input)
[rank3]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 204.00 MiB. GPU  has a total capacity of 79.15 GiB of which 112.19 MiB is free. Including non-PyTorch memory, this process
has 78.99 GiB memory in use. Of the allocated memory 76.41 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PY
TORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train_mem.py", line 11, in <module>
[rank0]:     train(attn_implementation="flash_attention_2")
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train.py", line 1401, in train
[rank0]:     trainer.train()
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 2279, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3318, in training_step
[rank0]:     loss = self.compute_loss(model, inputs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3363, in compute_loss
[rank0]:     outputs = model(**inputs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1852, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/language_model/llava_phi.py", line 288, in forward
[rank0]:     ) = self.prepare_inputs_labels_for_multimodal(
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/llava_arch.py", line 224, in prepare_inputs_labels_for_multimodal
[rank0]:     image_features, auxiliary_loss_clip, vision_id_experts = self.get_model().get_vision_tower()(images, return_id_experts = kwargs['return_id_experts'])
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_encoder.py", line 112, in forward
[rank0]:     out_x, auxilarity_loss, _ = self.vision_model( pixel_values = x,   return_id_experts = return_id_experts)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 231, in forward
[rank0]:     encoder_outputs, auxiliary_losses, stored_history_experts = self.encoder(hidden_states, return_id_experts)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 173, in forward
[rank0]:     layer_outputs = encoder_layer(hidden_states = hidden_states)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 138, in forward
[rank0]:     results, auxiliary_loss, id_experts = self.moelayer(hidden_states, return_id_experts)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 40, in forward
[rank0]:     output = self.compute_moe(gate_logits, output, x)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 32, in compute_moe
[rank0]:     results += weights_slice*self.experts[i](x)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 104, in forward
[rank0]:     hidden_states = self.activation_fn(hidden_states)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/activations.py", line 96, in forward
[rank0]:     return input * torch.sigmoid(1.702 * input)
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 204.00 MiB. GPU
[rank1]: Traceback (most recent call last):
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train_mem.py", line 11, in <module>
[rank1]:     train(attn_implementation="flash_attention_2")
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train.py", line 1401, in train
[rank1]:     trainer.train()
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 2279, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3318, in training_step
[rank1]:     loss = self.compute_loss(model, inputs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3363, in compute_loss
[rank1]:     outputs = model(**inputs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1852, in forward
[rank1]:     loss = self.module(*inputs, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/language_model/llava_phi.py", line 288, in forward
[rank1]:     ) = self.prepare_inputs_labels_for_multimodal(
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/llava_arch.py", line 224, in prepare_inputs_labels_for_multimodal
[rank1]:     image_features, auxiliary_loss_clip, vision_id_experts = self.get_model().get_vision_tower()(images, return_id_experts = kwargs['return_id_experts'])
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_encoder.py", line 112, in forward
[rank1]:     out_x, auxilarity_loss, _ = self.vision_model( pixel_values = x,   return_id_experts = return_id_experts)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 231, in forward
[rank1]:     encoder_outputs, auxiliary_losses, stored_history_experts = self.encoder(hidden_states, return_id_experts)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 173, in forward
[rank1]:     layer_outputs = encoder_layer(hidden_states = hidden_states)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 138, in forward
[rank1]:     results, auxiliary_loss, id_experts = self.moelayer(hidden_states, return_id_experts)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 40, in forward
[rank1]:     output = self.compute_moe(gate_logits, output, x)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 32, in compute_moe
[rank1]:     results += weights_slice*self.experts[i](x)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 103, in forward
[rank1]:     hidden_states = self.fc1(hidden_states)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 116, in forward
[rank1]:     return F.linear(input, self.weight, self.bias)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py", line 111, in zero3_linear_wrap
[rank1]:     return LinearFunctionForZeroStage3.apply(input, weight, bias)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/autograd/function.py", line 598, in apply
[rank1]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py", line 115, in decorate_fwd
[rank1]:     return fwd(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py", line 57, in forward
[rank1]:     output = input.matmul(weight.t())
[rank1]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 204.00 MiB. GPU  has a total capacity of 79.15 GiB of which 66.19 MiB is free. Including non-PyTorch memory, this process h
as 78.76 GiB memory in use. Of the allocated memory 76.02 GiB is allocated by PyTorch, and 1.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYT
ORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|                                                                                                                                                                                   | 0/8316 [00:07
<?, ?it/s]
[2024-09-21 23:27:52,114] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3192400
[2024-09-21 23:27:53,422] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3192401
[2024-09-21 23:27:55,090] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3192402
[2024-09-21 23:27:55,090] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3192403
[2024-09-21 23:27:58,255] [ERROR] [launch.py:322:sigkill_handler] ['/cm/archive/anonymous/miniconda3/envs/moe/bin/python', '-u', 'moe_model/train/train_mem.py', '--local_rank=3', '--deepspeed', './scri
pts/zero3_offload.json', '--model_name_or_path', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft', '--version', 'phi3', '--data_path', '/cm/archive/anonymous/data/jsons/llava_v1_5_mix665k_half.json'
, '--image_folder', '/cm/archive/anonymous/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--vision_tower_dir', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft/clip.bin', '--scales',
'1,3', '--pretrain_mm_mlp_adapter', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft/mm_projector.bin', '--mm_projector_type', 'moe', '--mlp_smoe', 'true', '--clip_smoe', 'true', '--moe_name', 'smo
e_sigmoidgating', '--num_experts', '4', '--num_selected', '2', '--balance_loss_coef', '0.1', '--router_z_loss_coef', '0.01', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_
use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/cm/archive/anonymous/checkpoints/phi3mini-clip/sft/smoe_sigmoidgat
ing', '--num_train_epochs', '1', '--per_device_train_batch_size', '5', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'st
eps', '--save_steps', '832', '--save_total_limit', '13', '--learning_rate', '4e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32'
, 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none', '--max_steps', '-1'] exits with return
 code = 1
(moe) anonymous@ithndgx004:/cm/archive/anonymous/toolkitmoe$ bash /cm/archive/anonymous/toolkitmoe/scripts/train/run_train_all.sh
Staring stage sft
[2024-09-21 23:28:49,837] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-21 23:28:52,794] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-09-21 23:28:52,794] [INFO] [runner.py:568:main] cmd = /cm/archive/anonymous/miniconda3/envs/moe/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgNiwgN119 --mast
er_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None moe_model/train/train_mem.py --deepspeed ./scripts/zero3_offload.json --model_name_or_path /cm/archive/anonymous/checkpoints/phi3mini-c
lip/pft --version phi3 --data_path /cm/archive/anonymous/data/jsons/llava_v1_5_mix665k_half.json --image_folder /cm/archive/anonymous/data --vision_tower openai/clip-vit-large-patch14-336 --vision_tower_
dir /cm/archive/anonymous/checkpoints/phi3mini-clip/pft/clip.bin --scales 1,3 --pretrain_mm_mlp_adapter /cm/archive/anonymous/checkpoints/phi3mini-clip/pft/mm_projector.bin --mm_projector_type moe --mlp_
smoe true --clip_smoe true --moe_name smoe_sigmoidgating --num_experts 4 --num_selected 2 --balance_loss_coef 0.1 --router_z_loss_coef 0.01 --mm_vision_select_layer -2 --mm_use_im_start_end False --m
m_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /cm/archive/anonymous/checkpoints/phi3mini-clip/sft/smoe_sigmoidgating --num_train_epochs 1
--per_device_train_batch_size 5 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 832 --save_total_limit 13 --learning_rate 4e
-6 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess Tr
ue --report_to none --max_steps -1
[2024-09-21 23:28:55,213] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-21 23:28:57,492] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 6, 7]}
[2024-09-21 23:28:57,492] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-09-21 23:28:57,492] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-09-21 23:28:57,492] [INFO] [launch.py:163:main] dist_world_size=4
[2024-09-21 23:28:57,492] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,6,7
[2024-09-21 23:28:57,509] [INFO] [launch.py:253:main] process 3199536 spawned with command: ['/cm/archive/anonymous/miniconda3/envs/moe/bin/python', '-u', 'moe_model/train/train_mem.py', '--local_rank=
0', '--deepspeed', './scripts/zero3_offload.json', '--model_name_or_path', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft', '--version', 'phi3', '--data_path', '/cm/archive/anonymous/data/jsons/lla
va_v1_5_mix665k_half.json', '--image_folder', '/cm/archive/anonymous/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--vision_tower_dir', '/cm/archive/anonymous/checkpoints/phi3mini-clip/p
ft/clip.bin', '--scales', '1,3', '--pretrain_mm_mlp_adapter', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft/mm_projector.bin', '--mm_projector_type', 'moe', '--mlp_smoe', 'true', '--clip_smoe',
'true', '--moe_name', 'smoe_sigmoidgating', '--num_experts', '4', '--num_selected', '2', '--balance_loss_coef', '0.1', '--router_z_loss_coef', '0.01', '--mm_vision_select_layer', '-2', '--mm_use_im_s
tart_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/cm/archive/anonymous/checkpoints/phi3min
i-clip/sft/smoe_sigmoidgating', '--num_train_epochs', '1', '--per_device_train_batch_size', '5', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'n
o', '--save_strategy', 'steps', '--save_steps', '832', '--save_total_limit', '13', '--learning_rate', '4e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--log
ging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none', '--max_steps
', '-1']
[2024-09-21 23:28:57,524] [INFO] [launch.py:253:main] process 3199537 spawned with command: ['/cm/archive/anonymous/miniconda3/envs/moe/bin/python', '-u', 'moe_model/train/train_mem.py', '--local_rank=
1', '--deepspeed', './scripts/zero3_offload.json', '--model_name_or_path', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft', '--version', 'phi3', '--data_path', '/cm/archive/anonymous/data/jsons/lla
va_v1_5_mix665k_half.json', '--image_folder', '/cm/archive/anonymous/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--vision_tower_dir', '/cm/archive/anonymous/checkpoints/phi3mini-clip/p
ft/clip.bin', '--scales', '1,3', '--pretrain_mm_mlp_adapter', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft/mm_projector.bin', '--mm_projector_type', 'moe', '--mlp_smoe', 'true', '--clip_smoe',
'true', '--moe_name', 'smoe_sigmoidgating', '--num_experts', '4', '--num_selected', '2', '--balance_loss_coef', '0.1', '--router_z_loss_coef', '0.01', '--mm_vision_select_layer', '-2', '--mm_use_im_s
tart_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/cm/archive/anonymous/checkpoints/phi3min
i-clip/sft/smoe_sigmoidgating', '--num_train_epochs', '1', '--per_device_train_batch_size', '5', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'n
o', '--save_strategy', 'steps', '--save_steps', '832', '--save_total_limit', '13', '--learning_rate', '4e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--log
ging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none', '--max_steps
', '-1']
[2024-09-21 23:28:57,538] [INFO] [launch.py:253:main] process 3199538 spawned with command: ['/cm/archive/anonymous/miniconda3/envs/moe/bin/python', '-u', 'moe_model/train/train_mem.py', '--local_rank=
2', '--deepspeed', './scripts/zero3_offload.json', '--model_name_or_path', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft', '--version', 'phi3', '--data_path', '/cm/archive/anonymous/data/jsons/lla
va_v1_5_mix665k_half.json', '--image_folder', '/cm/archive/anonymous/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--vision_tower_dir', '/cm/archive/anonymous/checkpoints/phi3mini-clip/p
ft/clip.bin', '--scales', '1,3', '--pretrain_mm_mlp_adapter', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft/mm_projector.bin', '--mm_projector_type', 'moe', '--mlp_smoe', 'true', '--clip_smoe',
'true', '--moe_name', 'smoe_sigmoidgating', '--num_experts', '4', '--num_selected', '2', '--balance_loss_coef', '0.1', '--router_z_loss_coef', '0.01', '--mm_vision_select_layer', '-2', '--mm_use_im_s
tart_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/cm/archive/anonymous/checkpoints/phi3min
i-clip/sft/smoe_sigmoidgating', '--num_train_epochs', '1', '--per_device_train_batch_size', '5', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'n
o', '--save_strategy', 'steps', '--save_steps', '832', '--save_total_limit', '13', '--learning_rate', '4e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--log
ging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none', '--max_steps
', '-1']
[2024-09-21 23:28:57,553] [INFO] [launch.py:253:main] process 3199539 spawned with command: ['/cm/archive/anonymous/miniconda3/envs/moe/bin/python', '-u', 'moe_model/train/train_mem.py', '--local_rank=
3', '--deepspeed', './scripts/zero3_offload.json', '--model_name_or_path', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft', '--version', 'phi3', '--data_path', '/cm/archive/anonymous/data/jsons/lla
va_v1_5_mix665k_half.json', '--image_folder', '/cm/archive/anonymous/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--vision_tower_dir', '/cm/archive/anonymous/checkpoints/phi3mini-clip/p
ft/clip.bin', '--scales', '1,3', '--pretrain_mm_mlp_adapter', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft/mm_projector.bin', '--mm_projector_type', 'moe', '--mlp_smoe', 'true', '--clip_smoe',
'true', '--moe_name', 'smoe_sigmoidgating', '--num_experts', '4', '--num_selected', '2', '--balance_loss_coef', '0.1', '--router_z_loss_coef', '0.01', '--mm_vision_select_layer', '-2', '--mm_use_im_s
tart_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/cm/archive/anonymous/checkpoints/phi3min
i-clip/sft/smoe_sigmoidgating', '--num_train_epochs', '1', '--per_device_train_batch_size', '5', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'n
o', '--save_strategy', 'steps', '--save_steps', '832', '--save_total_limit', '13', '--learning_rate', '4e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--log
ging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none', '--max_steps
', '-1']
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set
the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set
the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set
the git credential as well.
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set
the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /home/anonymous/.cache/huggingface/token
Login successful
Token is valid (permission: fineGrained).
Your token has been saved to /home/anonymous/.cache/huggingface/token
Login successful
/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Tr
ansformers. Use `eval_strategy` instead
  warnings.warn(
/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Tr
ansformers. Use `eval_strategy` instead
  warnings.warn(
Token is valid (permission: fineGrained).
Your token has been saved to /home/anonymous/.cache/huggingface/token
Login successful
/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Tr
ansformers. Use `eval_strategy` instead
  warnings.warn(
Token is valid (permission: fineGrained).
Your token has been saved to /home/anonymous/.cache/huggingface/token
Login successful
[2024-09-21 23:29:04,326] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Tr
ansformers. Use `eval_strategy` instead
  warnings.warn(
[2024-09-21 23:29:04,329] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-21 23:29:04,356] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-21 23:29:04,412] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-21 23:29:04,641] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-21 23:29:04,642] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-21 23:29:04,672] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-21 23:29:04,723] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-21 23:29:04,723] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
==================================================================================

=============CLIP-vision_tower======================CLIP-vision_tower=========

==================================================================================

=========================================
=============CLIP-vision_tower=========
=========================================
=========================================
=============CLIP-vision_tower=========
=========================================
[2024-09-21 23:29:16,756] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 588, num_elems = 4.14B
Loading checkpoint shards:   0%|                                                                                                                                       Loading checkpoint shards:   0%|
                                                                                                                                       Loading checkpoint shards:   0%|
                                                                                                       Loading checkpoint shards:   0%|
                                                                       Loading checkpoint shards:  50%|█████████████████████████████████████████████████████████████████████████▌
                                       Loading checkpoint shards:  50%|█████████████████████████████████████████████████████████████████████████▌
       Loading checkpoint shards:  50%|█████████████████████████████████████████████████████████████████████████▌                                                             Loading checkpoint shards
:  50%|█████████████████████████████████████████████████████████████████████████▌                                                             Loading checkpoint shards: 100%|█████████████████████████
██████████████████████████████████████████████████████████████████████████████████████████████████████████████Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████
██████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.32s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████Loading checkpoint shards: 100%|
███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.34s/it]
Model: phi3
=========================================
=============CLIP-vision_tower=========
=========================================
Model: phi3
=========================================
=============CLIP-vision_tower=========
=========================================
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████Loading checkpoint shards: 100%|
███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.37s/it]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████Loading checkpoint shards: 100%|
███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.43s/it]
Model: phi3
=========================================
=============CLIP-vision_tower=========
=========================================
Model: phi3
=========================================
=============CLIP-vision_tower=========
=========================================
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Using CLIP Encoder MoE Layer
Loading weight MLP SMoE
Loading weight MLP SMoE
Loading weight MLP SMoE
Loading weight MLP SMoE
Loading weight clip SMoE
Loading weight clip SMoE
Loading weight clip SMoE
Loading weight clip SMoE
Formatting inputs...Skip in lazy mode
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using /home/anonymous/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Using /home/anonymous/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Using /home/anonymous/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Using /home/anonymous/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/anonymous/.cache/torch_extensions/py39_cu121/cpu_adam/build.ninja...
/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compi
lation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.844177007675171 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.100571870803833 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.580711364746094 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 4.719303607940674 seconds
Parameter Offload: Total persistent parameters: 1021952 in 485 params
  0%|                                                                                                                                                                                   | 0/8316 [00:00
<?, ?it/s]Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f61800c000bb754'
Traceback (most recent call last):
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf2632000bb752'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf2633000bb74f'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf262b000bb750'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf2631000bb74f'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf262c000bb755'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f618008000bb753'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
Traceback (most recent call last):
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf262f000bb756'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f618009000bb757'
Traceback (most recent call last):
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf262e000bb758'
OSError: [Errno 16] Device or resource busy: '.nfs000000014f61800b000bb759'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f2af8c6000bb751'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fbf2634000bb75a'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f61800e000bb75b'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f2af8ce000bb75d'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 690, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 688, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f2af8cf000bb75c'
[rank1]: Traceback (most recent call last):
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train_mem.py", line 11, in <module>
[rank1]:     train(attn_implementation="flash_attention_2")
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train.py", line 1401, in train
[rank1]:     trainer.train()
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 2279, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3318, in training_step
[rank1]:     loss = self.compute_loss(model, inputs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3363, in compute_loss
[rank1]:     outputs = model(**inputs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1852, in forward
[rank1]:     loss = self.module(*inputs, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/language_model/llava_phi.py", line 288, in forward
[rank1]:     ) = self.prepare_inputs_labels_for_multimodal(
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/llava_arch.py", line 224, in prepare_inputs_labels_for_multimodal
[rank1]:     image_features, auxiliary_loss_clip, vision_id_experts = self.get_model().get_vision_tower()(images, return_id_experts = kwargs['return_id_experts'])
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_encoder.py", line 112, in forward
[rank1]:     out_x, auxilarity_loss, _ = self.vision_model( pixel_values = x,   return_id_experts = return_id_experts)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 231, in forward
[rank1]:     encoder_outputs, auxiliary_losses, stored_history_experts = self.encoder(hidden_states, return_id_experts)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 173, in forward
[rank1]:     layer_outputs = encoder_layer(hidden_states = hidden_states)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 138, in forward
[rank1]:     results, auxiliary_loss, id_experts = self.moelayer(hidden_states, return_id_experts)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 40, in forward
[rank1]:     output = self.compute_moe(gate_logits, output, x)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 32, in compute_moe
[rank1]:     results += weights_slice*self.experts[i](x)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 104, in forward
[rank1]:     hidden_states = self.activation_fn(hidden_states)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/activations.py", line 96, in forward
[rank1]:     return input * torch.sigmoid(1.702 * input)
[rank1]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 204.00 MiB. GPU  has a total capacity of 79.15 GiB of which 172.19 MiB is free. Including non-PyTorch memory, this process
has 78.94 GiB memory in use. Of the allocated memory 76.21 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PY
TORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train_mem.py", line 11, in <module>
[rank2]:     train(attn_implementation="flash_attention_2")
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train.py", line 1401, in train
[rank2]:     trainer.train()
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 2279, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3318, in training_step
[rank2]:     loss = self.compute_loss(model, inputs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3363, in compute_loss
[rank2]:     outputs = model(**inputs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1852, in forward
[rank2]:     loss = self.module(*inputs, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/language_model/llava_phi.py", line 288, in forward
[rank2]:     ) = self.prepare_inputs_labels_for_multimodal(
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/llava_arch.py", line 224, in prepare_inputs_labels_for_multimodal
[rank2]:     image_features, auxiliary_loss_clip, vision_id_experts = self.get_model().get_vision_tower()(images, return_id_experts = kwargs['return_id_experts'])
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_encoder.py", line 112, in forward
[rank2]:     out_x, auxilarity_loss, _ = self.vision_model( pixel_values = x,   return_id_experts = return_id_experts)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 231, in forward
[rank2]:     encoder_outputs, auxiliary_losses, stored_history_experts = self.encoder(hidden_states, return_id_experts)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 173, in forward
[rank2]:     layer_outputs = encoder_layer(hidden_states = hidden_states)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 138, in forward
[rank2]:     results, auxiliary_loss, id_experts = self.moelayer(hidden_states, return_id_experts)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 40, in forward
[rank2]:     output = self.compute_moe(gate_logits, output, x)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 32, in compute_moe
[rank2]:     results += weights_slice*self.experts[i](x)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 104, in forward
[rank2]:     hidden_states = self.activation_fn(hidden_states)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/activations.py", line 96, in forward
[rank2]:     return input * torch.sigmoid(1.702 * input)
[rank2]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 204.00 MiB. GPU  has a total capacity of 79.15 GiB of which 172.19 MiB is free. Including non-PyTorch memory, this process
has 78.94 GiB memory in use. Of the allocated memory 76.21 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PY
TORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train_mem.py", line 11, in <module>
[rank3]:     train(attn_implementation="flash_attention_2")
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train.py", line 1401, in train
[rank3]:     trainer.train()
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 2279, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3318, in training_step
[rank3]:     loss = self.compute_loss(model, inputs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3363, in compute_loss
[rank3]:     outputs = model(**inputs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1852, in forward
[rank3]:     loss = self.module(*inputs, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/language_model/llava_phi.py", line 288, in forward
[rank3]:     ) = self.prepare_inputs_labels_for_multimodal(
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/llava_arch.py", line 224, in prepare_inputs_labels_for_multimodal
[rank3]:     image_features, auxiliary_loss_clip, vision_id_experts = self.get_model().get_vision_tower()(images, return_id_experts = kwargs['return_id_experts'])
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_encoder.py", line 112, in forward
[rank3]:     out_x, auxilarity_loss, _ = self.vision_model( pixel_values = x,   return_id_experts = return_id_experts)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 231, in forward
[rank3]:     encoder_outputs, auxiliary_losses, stored_history_experts = self.encoder(hidden_states, return_id_experts)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 173, in forward
[rank3]:     layer_outputs = encoder_layer(hidden_states = hidden_states)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 138, in forward
[rank3]:     results, auxiliary_loss, id_experts = self.moelayer(hidden_states, return_id_experts)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 40, in forward
[rank3]:     output = self.compute_moe(gate_logits, output, x)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 32, in compute_moe
[rank3]:     results += weights_slice*self.experts[i](x)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 104, in forward
[rank3]:     hidden_states = self.activation_fn(hidden_states)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/activations.py", line 96, in forward
[rank3]:     return input * torch.sigmoid(1.702 * input)
[rank3]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 204.00 MiB. GPU  has a total capacity of 79.15 GiB of which 112.19 MiB is free. Including non-PyTorch memory, this process
has 78.99 GiB memory in use. Of the allocated memory 76.41 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PY
TORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train_mem.py", line 11, in <module>
[rank0]:     train(attn_implementation="flash_attention_2")
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/train/train.py", line 1401, in train
[rank0]:     trainer.train()
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 2279, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3318, in training_step
[rank0]:     loss = self.compute_loss(model, inputs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/trainer.py", line 3363, in compute_loss
[rank0]:     outputs = model(**inputs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1852, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/language_model/llava_phi.py", line 288, in forward
[rank0]:     ) = self.prepare_inputs_labels_for_multimodal(
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/llava_arch.py", line 224, in prepare_inputs_labels_for_multimodal
[rank0]:     image_features, auxiliary_loss_clip, vision_id_experts = self.get_model().get_vision_tower()(images, return_id_experts = kwargs['return_id_experts'])
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_encoder.py", line 112, in forward
[rank0]:     out_x, auxilarity_loss, _ = self.vision_model( pixel_values = x,   return_id_experts = return_id_experts)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 231, in forward
[rank0]:     encoder_outputs, auxiliary_losses, stored_history_experts = self.encoder(hidden_states, return_id_experts)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 173, in forward
[rank0]:     layer_outputs = encoder_layer(hidden_states = hidden_states)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 138, in forward
[rank0]:     results, auxiliary_loss, id_experts = self.moelayer(hidden_states, return_id_experts)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 40, in forward
[rank0]:     output = self.compute_moe(gate_logits, output, x)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/moe/moe_sigmoidgating.py", line 32, in compute_moe
[rank0]:     results += weights_slice*self.experts[i](x)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/toolkitmoe/moe_model/model/multimodal_encoder/clip_smoe.py", line 104, in forward
[rank0]:     hidden_states = self.activation_fn(hidden_states)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1582, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/transformers/activations.py", line 96, in forward
[rank0]:     return input * torch.sigmoid(1.702 * input)
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 204.00 MiB. GPU
  0%|                                                                                                                                                                                   | 0/8316 [00:07
<?, ?it/s]
[2024-09-21 23:30:45,667] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3199536
[2024-09-21 23:30:46,177] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3199537
[2024-09-21 23:30:46,177] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3199538
[2024-09-21 23:30:50,298] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3199539
[2024-09-21 23:30:50,441] [ERROR] [launch.py:322:sigkill_handler] ['/cm/archive/anonymous/miniconda3/envs/moe/bin/python', '-u', 'moe_model/train/train_mem.py', '--local_rank=3', '--deepspeed', './scri
pts/zero3_offload.json', '--model_name_or_path', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft', '--version', 'phi3', '--data_path', '/cm/archive/anonymous/data/jsons/llava_v1_5_mix665k_half.json'
, '--image_folder', '/cm/archive/anonymous/data', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--vision_tower_dir', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft/clip.bin', '--scales',
'1,3', '--pretrain_mm_mlp_adapter', '/cm/archive/anonymous/checkpoints/phi3mini-clip/pft/mm_projector.bin', '--mm_projector_type', 'moe', '--mlp_smoe', 'true', '--clip_smoe', 'true', '--moe_name', 'smo
e_sigmoidgating', '--num_experts', '4', '--num_selected', '2', '--balance_loss_coef', '0.1', '--router_z_loss_coef', '0.01', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_
use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/cm/archive/anonymous/checkpoints/phi3mini-clip/sft/smoe_sigmoidgat
ing', '--num_train_epochs', '1', '--per_device_train_batch_size', '5', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'st
eps', '--save_steps', '832', '--save_total_limit', '13', '--learning_rate', '4e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32'
, 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none', '--max_steps', '-1'] exits with return
 code = 1
(moe) anonymous@ithndgx004:/cm/archive/anonymous/toolkitmoe$ bash /cm/archive/anonymous/toolkitmoe/scripts/run_full.sh
