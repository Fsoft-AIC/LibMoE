clip auxiliary loss:  0.11903157830238342
language loss:  0.8446186184883118
mlp auxiliary loss:  0.1190478578209877
clip auxiliary loss:  0.1190478578209877
{'loss': 0.9426, 'grad_norm': 3.822736661382182, 'learning_rate': 3.643466668853
845e-09, 'flos': 16062172844040.0, 'epoch': 0.98, 'num_input_tokens_seen': 17387
1915}
 98%|███████████████████████████████████▎| 8161/8316 [10:37:29<31:23, 12.15s/it]
language loss:  0.6448997259140015
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.31681162118911743
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
[2024-09-20 11:37:44,736] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9679, 'grad_norm': 2.824194015035831, 'learning_rate': 3.596619898980
08e-09, 'flos': 18082920896160.0, 'epoch': 0.98, 'num_input_tokens_seen': 173892
690}
 98%|███████████████████████████████████▎| 8162/8316 [10:37:42<31:36, 12.32s/it]
language loss:  0.6271241307258606
mlp auxiliary loss:  0.119036465883255
clip auxiliary loss:  0.119036465883255
language loss:  0.9201329350471497
mlp auxiliary loss:  0.11903809010982513
clip auxiliary loss:  0.11903809010982513
{'loss': 0.9944, 'grad_norm': 13.385798293238459, 'learning_rate': 3.55007598191
15934e-09, 'flos': 17790379246440.0, 'epoch': 0.98, 'num_input_tokens_seen': 173
912775}
 98%|███████████████████████████████████▎| 8163/8316 [10:37:54<31:06, 12.20s/it]
language loss:  0.9879185557365417
mlp auxiliary loss:  0.1190478578209877
clip auxiliary loss:  0.1190478578209877
language loss:  0.9211463928222656
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
{'loss': 1.044, 'grad_norm': 3.8461135172563576, 'learning_rate': 3.503834924709
4034e-09, 'flos': 14667636339480.0, 'epoch': 0.98, 'num_input_tokens_seen': 1739
29755}
 98%|███████████████████████████████████▎| 8164/8316 [10:38:05<30:15, 11.94s/it]
language loss:  0.7060694694519043
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
language loss:  0.5394203066825867
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
{'loss': 0.9985, 'grad_norm': 3.6164593550746202, 'learning_rate': 3.45789673438
78994e-09, 'flos': 12547921240920.0, 'epoch': 0.98, 'num_input_tokens_seen': 173
945680}
 98%|███████████████████████████████████▎| 8165/8316 [10:38:17<29:45, 11.82s/it]
language loss:  0.9267622828483582
mlp auxiliary loss:  0.11905762553215027
clip auxiliary loss:  0.11905762553215027
language loss:  0.6804138422012329
mlp auxiliary loss:  0.11902506649494171
clip auxiliary loss:  0.11902506649494171
{'loss': 1.0337, 'grad_norm': 3.9867960437037406, 'learning_rate': 3.41226141791
61733e-09, 'flos': 16010619665520.0, 'epoch': 0.98, 'num_input_tokens_seen': 173
965360}
 98%|███████████████████████████████████▎| 8166/8316 [10:38:29<29:34, 11.83s/it]
language loss:  0.7290285229682922
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
language loss:  1.0075576305389404
mlp auxiliary loss:  0.11903320252895355
clip auxiliary loss:  0.11903320252895355
{'loss': 0.9922, 'grad_norm': 3.1875964879410446, 'learning_rate': 3.36692898221
691e-09, 'flos': 14200223638080.0, 'epoch': 0.98, 'num_input_tokens_seen': 17398
3445}
 98%|███████████████████████████████████▎| 8167/8316 [10:38:41<29:40, 11.95s/it]
language loss:  1.0328177213668823
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.755152702331543
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
{'loss': 0.955, 'grad_norm': 5.662063898467769, 'learning_rate': 3.3218994341668
305e-09, 'flos': 13335016620720.0, 'epoch': 0.98, 'num_input_tokens_seen': 17400
2095}
 98%|███████████████████████████████████▎| 8168/8316 [10:38:53<29:17, 11.87s/it]
language loss:  0.8546444773674011
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
language loss:  0.69452303647995
mlp auxiliary loss:  0.11903902888298035
clip auxiliary loss:  0.11903902888298035
[2024-09-20 11:39:07,428] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9779, 'grad_norm': 3.881591482082685, 'learning_rate': 3.277172780597
1373e-09, 'flos': 18919576453080.0, 'epoch': 0.98, 'num_input_tokens_seen': 1740
23200}
 98%|███████████████████████████████████▎| 8169/8316 [10:39:05<29:15, 11.94s/it]
language loss:  0.8991314768791199
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.8610085248947144
mlp auxiliary loss:  0.11905436217784882
clip auxiliary loss:  0.11905436217784882
{'loss': 0.9945, 'grad_norm': 4.240225828382681, 'learning_rate': 3.232749028292
847e-09, 'flos': 15639445131720.0, 'epoch': 0.98, 'num_input_tokens_seen': 17403
9885}
 98%|███████████████████████████████████▎| 8170/8316 [10:39:16<28:52, 11.86s/it]
language loss:  0.8576316237449646
mlp auxiliary loss:  0.11905598640441895
clip auxiliary loss:  0.11905598640441895
language loss:  0.8576332926750183
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
{'loss': 1.1056, 'grad_norm': 3.8451226737465305, 'learning_rate': 3.18862818399
2792e-09, 'flos': 15563479803360.0, 'epoch': 0.98, 'num_input_tokens_seen': 1740
59870}
 98%|███████████████████████████████████▎| 8171/8316 [10:39:28<28:44, 11.89s/it]
language loss:  0.8949152827262878
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
language loss:  0.5937737226486206
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
[2024-09-20 11:39:44,294] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 0.886, 'grad_norm': 0.8675882942603335, 'learning_rate': 3.144810254390
2844e-09, 'flos': 42581900491440.0, 'epoch': 0.98, 'num_input_tokens_seen': 1741
23505}
 98%|███████████████████████████████████▍| 8172/8316 [10:39:42<29:30, 12.29s/it]
language loss:  0.8322771191596985
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
language loss:  0.8606942296028137
mlp auxiliary loss:  0.11903809010982513
clip auxiliary loss:  0.11903809010982513
{'loss': 0.8914, 'grad_norm': 3.8860354642403223, 'learning_rate': 3.10129524613
24515e-09, 'flos': 11368519964400.0, 'epoch': 0.98, 'num_input_tokens_seen': 174
142200}
 98%|███████████████████████████████████▍| 8173/8316 [10:39:53<28:47, 12.08s/it]
language loss:  0.3513767719268799
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
language loss:  1.027363657951355
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
{'loss': 0.9682, 'grad_norm': 3.506486875858983, 'learning_rate': 3.058083165820
4575e-09, 'flos': 14380491124320.0, 'epoch': 0.98, 'num_input_tokens_seen': 1741
59500}
 98%|███████████████████████████████████▍| 8174/8316 [10:40:05<28:22, 11.99s/it]
language loss:  0.5679838061332703
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.6892775297164917
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
{'loss': 1.0109, 'grad_norm': 3.7289715223915465, 'learning_rate': 3.01517402000
9281e-09, 'flos': 15353587702080.0, 'epoch': 0.98, 'num_input_tokens_seen': 1741
78545}
 98%|███████████████████████████████████▍| 8175/8316 [10:40:16<27:48, 11.84s/it]
language loss:  0.808076798915863
mlp auxiliary loss:  0.11903483420610428
clip auxiliary loss:  0.11903483420610428
language loss:  0.6655359268188477
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
{'loss': 0.9769, 'grad_norm': 3.5014457441454576, 'learning_rate': 2.97256781520
86043e-09, 'flos': 16896743414880.0, 'epoch': 0.98, 'num_input_tokens_seen': 174
196835}
 98%|███████████████████████████████████▍| 8176/8316 [10:40:28<27:33, 11.81s/it]
language loss:  0.7923022508621216
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
language loss:  0.9463745355606079
mlp auxiliary loss:  0.11905111372470856
clip auxiliary loss:  0.11905111372470856
{'loss': 1.0396, 'grad_norm': 22.073215378784376, 'learning_rate': 2.93026455788
1257e-09, 'flos': 7953756654240.0, 'epoch': 0.98, 'num_input_tokens_seen': 17421
1740}
 98%|███████████████████████████████████▍| 8177/8316 [10:40:40<27:20, 11.80s/it]
language loss:  0.6208639740943909
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
language loss:  0.6102138757705688
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
{'loss': 0.8575, 'grad_norm': 0.813699936623235, 'learning_rate': 2.888264254445
2163e-09, 'flos': 42944643096240.0, 'epoch': 0.98, 'num_input_tokens_seen': 1742
76185}
 98%|███████████████████████████████████▍| 8178/8316 [10:40:53<28:02, 12.19s/it]
language loss:  0.7980846762657166
mlp auxiliary loss:  0.11902344226837158
clip auxiliary loss:  0.11902344226837158
language loss:  0.6842921376228333
mlp auxiliary loss:  0.1190478503704071
clip auxiliary loss:  0.1190478503704071
{'loss': 0.9711, 'grad_norm': 22.386228333378103, 'learning_rate': 2.84656691127
16083e-09, 'flos': 9611854086240.0, 'epoch': 0.98, 'num_input_tokens_seen': 1742
93430}
 98%|███████████████████████████████████▍| 8179/8316 [10:41:05<27:55, 12.23s/it]
language loss:  0.8488014340400696
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.8938182592391968
mlp auxiliary loss:  0.119036465883255
clip auxiliary loss:  0.119036465883255
{'loss': 0.9694, 'grad_norm': 4.286786585584437, 'learning_rate': 2.805172534685
8177e-09, 'flos': 16295189309640.0, 'epoch': 0.98, 'num_input_tokens_seen': 1743
13410}
 98%|███████████████████████████████████▍| 8180/8316 [10:41:17<27:15, 12.02s/it]
language loss:  0.3626531660556793
mlp auxiliary loss:  0.1190478503704071
clip auxiliary loss:  0.1190478503704071
language loss:  0.7541711330413818
mlp auxiliary loss:  0.1190478503704071
clip auxiliary loss:  0.1190478503704071
{'loss': 0.9215, 'grad_norm': 4.503478930698805, 'learning_rate': 2.764081130967
4883e-09, 'flos': 19706917125360.0, 'epoch': 0.98, 'num_input_tokens_seen': 1743
32630}
 98%|███████████████████████████████████▍| 8181/8316 [10:41:29<26:59, 12.00s/it]
language loss:  0.8205474019050598
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
language loss:  0.5710508823394775
mlp auxiliary loss:  0.11902464181184769
clip auxiliary loss:  0.11902464181184769
{'loss': 1.0361, 'grad_norm': 3.1813525589934635, 'learning_rate': 2.72329270634
98557e-09, 'flos': 20834151992160.0, 'epoch': 0.98, 'num_input_tokens_seen': 174
352725}
 98%|███████████████████████████████████▍| 8182/8316 [10:41:42<27:17, 12.22s/it]
language loss:  0.25804629921913147
mlp auxiliary loss:  0.1190478503704071
clip auxiliary loss:  0.1190478503704071
language loss:  0.581735372543335
mlp auxiliary loss:  0.11903320252895355
clip auxiliary loss:  0.11903320252895355
{'loss': 0.9075, 'grad_norm': 5.184567093466462, 'learning_rate': 2.682807267020
859e-09, 'flos': 28644354143640.0, 'epoch': 0.98, 'num_input_tokens_seen': 17437
5205}
 98%|███████████████████████████████████▍| 8183/8316 [10:41:53<26:47, 12.09s/it]
language loss:  0.8919745683670044
mlp auxiliary loss:  0.11903645843267441
clip auxiliary loss:  0.11903645843267441
language loss:  0.638344407081604
mlp auxiliary loss:  0.11903483420610428
clip auxiliary loss:  0.11903483420610428
{'loss': 0.8524, 'grad_norm': 2.6590621423536143, 'learning_rate': 2.64262481912
1808e-09, 'flos': 17188457202480.0, 'epoch': 0.98, 'num_input_tokens_seen': 1743
95075}
 98%|███████████████████████████████████▍| 8184/8316 [10:42:05<26:20, 11.98s/it]
language loss:  0.8523082137107849
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.568894624710083
mlp auxiliary loss:  0.119036465883255
clip auxiliary loss:  0.119036465883255
{'loss': 0.839, 'grad_norm': 3.240735861961285, 'learning_rate': 2.6027453687487
154e-09, 'flos': 10372023068280.0, 'epoch': 0.98, 'num_input_tokens_seen': 17441
1885}
 98%|███████████████████████████████████▍| 8185/8316 [10:42:17<25:54, 11.86s/it]
language loss:  0.17325162887573242
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
language loss:  0.739503800868988
mlp auxiliary loss:  0.11904947459697723
clip auxiliary loss:  0.11904947459697723
{'loss': 0.7305, 'grad_norm': 11.645788494062456, 'learning_rate': 2.56316892195
09643e-09, 'flos': 15877091492880.0, 'epoch': 0.98, 'num_input_tokens_seen': 174
430285}
 98%|███████████████████████████████████▍| 8186/8316 [10:42:28<25:36, 11.82s/it]
language loss:  0.7446552515029907
mlp auxiliary loss:  0.119036465883255
clip auxiliary loss:  0.119036465883255
language loss:  0.7523214817047119
mlp auxiliary loss:  0.11903808265924454
clip auxiliary loss:  0.11903808265924454
{'loss': 1.0644, 'grad_norm': 5.8059819910057255, 'learning_rate': 2.52389548473
2197e-09, 'flos': 15486012058560.0, 'epoch': 0.98, 'num_input_tokens_seen': 1744
49460}
 98%|███████████████████████████████████▍| 8187/8316 [10:42:40<25:13, 11.73s/it]
language loss:  0.7198276519775391
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
language loss:  0.6153101325035095
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
{'loss': 0.9773, 'grad_norm': 8.365738487685109, 'learning_rate': 2.484925063050
5357e-09, 'flos': 12779465951640.0, 'epoch': 0.98, 'num_input_tokens_seen': 1744
67425}
 98%|███████████████████████████████████▍| 8188/8316 [10:42:51<24:50, 11.64s/it]
language loss:  0.8313878774642944
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
language loss:  0.42659705877304077
mlp auxiliary loss:  0.11903320252895355
clip auxiliary loss:  0.11903320252895355
{'loss': 0.9546, 'grad_norm': 3.0744307758741436, 'learning_rate': 2.44625766281
72528e-09, 'flos': 17949852646920.0, 'epoch': 0.98, 'num_input_tokens_seen': 174
485775}
 98%|███████████████████████████████████▍| 8189/8316 [10:43:04<25:10, 11.89s/it]
language loss:  0.6980782747268677
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
language loss:  0.6455081105232239
mlp auxiliary loss:  0.11903483420610428
clip auxiliary loss:  0.11903483420610428
{'loss': 0.9691, 'grad_norm': 4.032448050914629, 'learning_rate': 2.407893289898
766e-09, 'flos': 13151682978480.0, 'epoch': 0.98, 'num_input_tokens_seen': 17450
4525}
 98%|███████████████████████████████████▍| 8190/8316 [10:43:16<24:52, 11.85s/it]
language loss:  0.892939031124115
mlp auxiliary loss:  0.1190478503704071
clip auxiliary loss:  0.1190478503704071
language loss:  0.6997619867324829
mlp auxiliary loss:  0.1190478503704071
clip auxiliary loss:  0.1190478503704071
{'loss': 1.0647, 'grad_norm': 3.564950735813792, 'learning_rate': 2.369831950114
4202e-09, 'flos': 19471263765600.0, 'epoch': 0.98, 'num_input_tokens_seen': 1745
25230}
 98%|███████████████████████████████████▍| 8191/8316 [10:43:27<24:29, 11.75s/it]
language loss:  0.9258739948272705
mlp auxiliary loss:  0.11905436217784882
clip auxiliary loss:  0.11905436217784882
language loss:  0.6851860284805298
mlp auxiliary loss:  0.11902506649494171
clip auxiliary loss:  0.11902506649494171
{'loss': 0.9519, 'grad_norm': 4.559493445988139, 'learning_rate': 2.332073649238
2644e-09, 'flos': 13282328964480.0, 'epoch': 0.99, 'num_input_tokens_seen': 1745
43785}
 99%|███████████████████████████████████▍| 8192/8316 [10:43:39<24:02, 11.63s/it]
language loss:  0.4918152987957001
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
language loss:  0.31109151244163513
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
{'loss': 0.9121, 'grad_norm': 12.01956108320152, 'learning_rate': 2.294618392998
16e-09, 'flos': 15850839649440.0, 'epoch': 0.99, 'num_input_tokens_seen': 174563
220}
 99%|███████████████████████████████████▍| 8193/8316 [10:43:50<23:54, 11.66s/it]
language loss:  0.818586528301239
mlp auxiliary loss:  0.11903483420610428
clip auxiliary loss:  0.11903483420610428
language loss:  0.9273276925086975
mlp auxiliary loss:  0.11904134601354599
clip auxiliary loss:  0.11904134601354599
{'loss': 1.0272, 'grad_norm': 5.43963633696165, 'learning_rate': 2.2574661870762
29e-09, 'flos': 18757619466240.0, 'epoch': 0.99, 'num_input_tokens_seen': 174582
145}
 99%|███████████████████████████████████▍| 8194/8316 [10:44:02<23:34, 11.60s/it]
language loss:  0.8080964088439941
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.7370419502258301
mlp auxiliary loss:  0.11904134601354599
clip auxiliary loss:  0.11904134601354599
{'loss': 0.9222, 'grad_norm': 3.3340817723855563, 'learning_rate': 2.22061703710
81854e-09, 'flos': 14826741801240.0, 'epoch': 0.99, 'num_input_tokens_seen': 174
600450}
 99%|███████████████████████████████████▍| 8195/8316 [10:44:14<23:33, 11.68s/it]
language loss:  0.9315513372421265
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
language loss:  1.0710874795913696
mlp auxiliary loss:  0.11904134601354599
clip auxiliary loss:  0.11904134601354599
{'loss': 1.0745, 'grad_norm': 5.702127615055534, 'learning_rate': 2.184070948684
2247e-09, 'flos': 17974663397040.0, 'epoch': 0.99, 'num_input_tokens_seen': 1746
19790}
 99%|███████████████████████████████████▍| 8196/8316 [10:44:25<23:25, 11.71s/it]
language loss:  0.8230106830596924
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
language loss:  0.6759036183357239
mlp auxiliary loss:  0.11905273795127869
clip auxiliary loss:  0.11905273795127869
{'loss': 1.0074, 'grad_norm': 4.498183837607581, 'learning_rate': 2.147827927348
1335e-09, 'flos': 13518043647360.0, 'epoch': 0.99, 'num_input_tokens_seen': 1746
37995}
 99%|███████████████████████████████████▍| 8197/8316 [10:44:37<23:00, 11.60s/it]
language loss:  0.9078379273414612
mlp auxiliary loss:  0.11903808265924454
clip auxiliary loss:  0.11903808265924454
language loss:  0.637087881565094
mlp auxiliary loss:  0.11905110627412796
clip auxiliary loss:  0.11905110627412796
{'loss': 1.0138, 'grad_norm': 3.617898628084162, 'learning_rate': 2.111887978598
1815e-09, 'flos': 24347606394720.0, 'epoch': 0.99, 'num_input_tokens_seen': 1746
57855}
 99%|███████████████████████████████████▍| 8198/8316 [10:44:48<22:44, 11.57s/it]
language loss:  0.9609564542770386
mlp auxiliary loss:  0.11902669072151184
clip auxiliary loss:  0.11902669072151184
language loss:  0.9386328458786011
mlp auxiliary loss:  0.11905436217784882
clip auxiliary loss:  0.11905436217784882
{'loss': 1.0216, 'grad_norm': 3.4698268834005312, 'learning_rate': 2.07625110788
62288e-09, 'flos': 18500405481720.0, 'epoch': 0.99, 'num_input_tokens_seen': 174
677920}
 99%|███████████████████████████████████▍| 8199/8316 [10:45:00<22:58, 11.78s/it]
language loss:  0.7741603255271912
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
language loss:  0.7210118174552917
mlp auxiliary loss:  0.11903809010982513
clip auxiliary loss:  0.11903809010982513
{'loss': 0.8833, 'grad_norm': 5.536219546556637, 'learning_rate': 2.040917320618
6183e-09, 'flos': 16848563007960.0, 'epoch': 0.99, 'num_input_tokens_seen': 1746
96880}
 99%|███████████████████████████████████▍| 8200/8316 [10:45:12<22:38, 11.72s/it]
language loss:  0.8181015253067017
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
language loss:  0.524824857711792
mlp auxiliary loss:  0.11902756989002228
clip auxiliary loss:  0.11902756989002228
{'loss': 1.0921, 'grad_norm': 3.3409917275064602, 'learning_rate': 2.00588662215
50617e-09, 'flos': 14147811935880.0, 'epoch': 0.99, 'num_input_tokens_seen': 174
714840}
 99%|███████████████████████████████████▌| 8201/8316 [10:45:25<22:56, 11.97s/it]
language loss:  0.8761535286903381
mlp auxiliary loss:  0.1190478578209877
clip auxiliary loss:  0.1190478578209877
language loss:  0.5862738490104675
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
{'loss': 0.9763, 'grad_norm': 3.6389264904794096, 'learning_rate': 1.97115901780
9976e-09, 'flos': 14069148390240.0, 'epoch': 0.99, 'num_input_tokens_seen': 1747
32850}
 99%|███████████████████████████████████▌| 8202/8316 [10:45:36<22:28, 11.83s/it]
language loss:  0.7496552467346191
mlp auxiliary loss:  0.11903809010982513
clip auxiliary loss:  0.11903809010982513
language loss:  0.8149734139442444
mlp auxiliary loss:  0.11902344226837158
clip auxiliary loss:  0.11902344226837158
{'loss': 0.996, 'grad_norm': 4.08147845015888, 'learning_rate': 1.93673451285159
e-09, 'flos': 15379318299000.0, 'epoch': 0.99, 'num_input_tokens_seen': 17475162
0}
 99%|███████████████████████████████████▌| 8203/8316 [10:45:48<22:21, 11.87s/it]
language loss:  0.6152973175048828
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
language loss:  0.6012740731239319
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
{'loss': 0.8269, 'grad_norm': 0.7450608298410416, 'learning_rate': 1.90261311250
19495e-09, 'flos': 37747513972560.0, 'epoch': 0.99, 'num_input_tokens_seen': 174
808710}
 99%|███████████████████████████████████▌| 8204/8316 [10:46:01<22:39, 12.14s/it]
language loss:  0.9673165082931519
mlp auxiliary loss:  0.11903809010982513
clip auxiliary loss:  0.11903809010982513
language loss:  0.9372782707214355
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
{'loss': 1.0876, 'grad_norm': 2.7358456389791157, 'learning_rate': 1.86879482193
71363e-09, 'flos': 16609812830640.0, 'epoch': 0.99, 'num_input_tokens_seen': 174
827655}
 99%|███████████████████████████████████▌| 8205/8316 [10:46:13<22:14, 12.02s/it]
language loss:  0.9817813038825989
mlp auxiliary loss:  0.11905436217784882
clip auxiliary loss:  0.11905436217784882
language loss:  0.7337594032287598
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
{'loss': 1.1093, 'grad_norm': 3.576132940291877, 'learning_rate': 1.835279646287
491e-09, 'flos': 15354047625480.0, 'epoch': 0.99, 'num_input_tokens_seen': 17484
5385}
 99%|███████████████████████████████████▌| 8206/8316 [10:46:24<21:43, 11.85s/it]
language loss:  0.9651907086372375
mlp auxiliary loss:  0.1190478503704071
clip auxiliary loss:  0.1190478503704071
language loss:  0.9345863461494446
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
{'loss': 0.9999, 'grad_norm': 2.717674919752527, 'learning_rate': 1.802067590637
1685e-09, 'flos': 15824311851960.0, 'epoch': 0.99, 'num_input_tokens_seen': 1748
64500}
 99%|███████████████████████████████████▌| 8207/8316 [10:46:35<21:15, 11.70s/it]
language loss:  0.6593656539916992
mlp auxiliary loss:  0.11904134601354599
clip auxiliary loss:  0.11904134601354599
language loss:  0.6012406349182129
mlp auxiliary loss:  0.1190299540758133
clip auxiliary loss:  0.1190299540758133
{'loss': 0.9717, 'grad_norm': 3.3910015969392195, 'learning_rate': 1.76915866002
43612e-09, 'flos': 18366999955320.0, 'epoch': 0.99, 'num_input_tokens_seen': 174
883120}
 99%|███████████████████████████████████▌| 8208/8316 [10:46:47<21:00, 11.67s/it]
language loss:  0.6953689455986023
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
language loss:  0.7469034194946289
mlp auxiliary loss:  0.11904947459697723
clip auxiliary loss:  0.11904947459697723
{'loss': 1.0891, 'grad_norm': 6.98985945662634, 'learning_rate': 1.7365528594415
202e-09, 'flos': 11603744062320.0, 'epoch': 0.99, 'num_input_tokens_seen': 17489
6910}
 99%|███████████████████████████████████▌| 8209/8316 [10:46:58<20:42, 11.61s/it]
language loss:  0.10269424319267273
mlp auxiliary loss:  0.11902344226837158
clip auxiliary loss:  0.11902344226837158
language loss:  0.7604203224182129
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
{'loss': 0.9029, 'grad_norm': 7.6053215369195035, 'learning_rate': 1.70425019383
46888e-09, 'flos': 25318342032360.0, 'epoch': 0.99, 'num_input_tokens_seen': 174
919360}
 99%|███████████████████████████████████▌| 8210/8316 [10:47:10<20:34, 11.65s/it]
language loss:  0.7342503666877747
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.6503793001174927
mlp auxiliary loss:  0.1190478503704071
clip auxiliary loss:  0.1190478503704071
{'loss': 0.997, 'grad_norm': 16.711763836803982, 'learning_rate': 1.672250668104
3913e-09, 'flos': 15222911054520.0, 'epoch': 0.99, 'num_input_tokens_seen': 1749
38040}
 99%|███████████████████████████████████▌| 8211/8316 [10:47:23<20:48, 11.89s/it]
language loss:  0.7895784378051758
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
language loss:  0.9111029505729675
mlp auxiliary loss:  0.11905110627412796
clip auxiliary loss:  0.11905110627412796
{'loss': 0.9078, 'grad_norm': 4.817136636441353, 'learning_rate': 1.640554287104
745e-09, 'flos': 11552129560680.0, 'epoch': 0.99, 'num_input_tokens_seen': 17495
6035}
 99%|███████████████████████████████████▌| 8212/8316 [10:47:34<20:33, 11.86s/it]
language loss:  0.9224448204040527
mlp auxiliary loss:  0.119036465883255
clip auxiliary loss:  0.119036465883255
language loss:  0.9173669815063477
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
{'loss': 1.0038, 'grad_norm': 3.174283170504076, 'learning_rate': 1.609161055644
348e-09, 'flos': 12647532180120.0, 'epoch': 0.99, 'num_input_tokens_seen': 17497
1680}
 99%|███████████████████████████████████▌| 8213/8316 [10:47:46<20:05, 11.71s/it]
language loss:  0.27969563007354736
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
language loss:  0.7798651456832886
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
{'loss': 0.8998, 'grad_norm': 4.2433037457229945, 'learning_rate': 1.57807097848
49467e-09, 'flos': 18598667312280.0, 'epoch': 0.99, 'num_input_tokens_seen': 174
988420}
 99%|███████████████████████████████████▌| 8214/8316 [10:47:57<19:51, 11.68s/it]
language loss:  0.8537067174911499
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
language loss:  0.8929764628410339
mlp auxiliary loss:  0.11905110627412796
clip auxiliary loss:  0.11905110627412796
{'loss': 1.044, 'grad_norm': 5.222532991311487, 'learning_rate': 1.5472840603436
565e-09, 'flos': 11310343888920.0, 'epoch': 0.99, 'num_input_tokens_seen': 17500
5370}
 99%|███████████████████████████████████▌| 8215/8316 [10:48:09<19:37, 11.66s/it]
language loss:  0.7998365759849548
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
language loss:  0.8151510953903198
mlp auxiliary loss:  0.11903482675552368
clip auxiliary loss:  0.11903482675552368
{'loss': 1.0204, 'grad_norm': 18.806435250624652, 'learning_rate': 1.51680030589
00757e-09, 'flos': 13334556697320.0, 'epoch': 0.99, 'num_input_tokens_seen': 175
023090}
 99%|███████████████████████████████████▌| 8216/8316 [10:48:21<19:28, 11.69s/it]
language loss:  0.844925045967102
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
language loss:  0.7430936694145203
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
{'loss': 1.1455, 'grad_norm': 5.162099952992253, 'learning_rate': 1.486619719749
1715e-09, 'flos': 15903987229080.0, 'epoch': 0.99, 'num_input_tokens_seen': 1750
42170}
 99%|███████████████████████████████████▌| 8217/8316 [10:48:32<19:11, 11.63s/it]
language loss:  0.6806941628456116
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
language loss:  0.9409279227256775
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
{'loss': 0.982, 'grad_norm': 10.221393021754706, 'learning_rate': 1.456742306498
8371e-09, 'flos': 11079351086280.0, 'epoch': 0.99, 'num_input_tokens_seen': 1750
59240}
 99%|███████████████████████████████████▌| 8218/8316 [10:48:43<18:46, 11.50s/it]
language loss:  0.615487813949585
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
language loss:  0.8055910468101501
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
{'loss': 1.0022, 'grad_norm': 5.136675120121269, 'learning_rate': 1.427168070671
8913e-09, 'flos': 15269313090960.0, 'epoch': 0.99, 'num_input_tokens_seen': 1750
76635}
 99%|███████████████████████████████████▌| 8219/8316 [10:48:55<18:36, 11.51s/it]
language loss:  0.7013412117958069
mlp auxiliary loss:  0.1190478578209877
clip auxiliary loss:  0.1190478578209877
language loss:  0.9363244771957397
mlp auxiliary loss:  0.11905110627412796
clip auxiliary loss:  0.11905110627412796
{'loss': 1.0464, 'grad_norm': 3.8288228547191663, 'learning_rate': 1.39789701675
43013e-09, 'flos': 19966001465040.0, 'epoch': 0.99, 'num_input_tokens_seen': 175
096535}
 99%|███████████████████████████████████▌| 8220/8316 [10:49:07<18:36, 11.63s/it]
language loss:  1.0054960250854492
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.9293525815010071
mlp auxiliary loss:  0.11905273795127869
clip auxiliary loss:  0.11905273795127869
{'loss': 0.987, 'grad_norm': 7.22407236447003, 'learning_rate': 1.36892914918673
72e-09, 'flos': 9950675126160.0, 'epoch': 0.99, 'num_input_tokens_seen': 1751145
70}
 99%|███████████████████████████████████▌| 8221/8316 [10:49:19<18:41, 11.80s/it]
language loss:  0.8769651055335999
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.5977811217308044
mlp auxiliary loss:  0.11903645843267441
clip auxiliary loss:  0.11903645843267441
{'loss': 0.9611, 'grad_norm': 5.959963097682101, 'learning_rate': 1.340264472363
6836e-09, 'flos': 18814630402440.0, 'epoch': 0.99, 'num_input_tokens_seen': 1751
36320}
 99%|███████████████████████████████████▌| 8222/8316 [10:49:32<18:47, 12.00s/it]
language loss:  0.6421443819999695
mlp auxiliary loss:  0.11902669072151184
clip auxiliary loss:  0.11902669072151184
language loss:  0.942234218120575
mlp auxiliary loss:  0.11902441084384918
clip auxiliary loss:  0.11902441084384918
{'loss': 1.0475, 'grad_norm': 3.304476539605882, 'learning_rate': 1.311902990633
218e-09, 'flos': 17950281908760.0, 'epoch': 0.99, 'num_input_tokens_seen': 17515
5005}
 99%|███████████████████████████████████▌| 8223/8316 [10:49:44<18:47, 12.12s/it]
language loss:  0.9051674008369446
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
language loss:  0.8005048036575317
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
{'loss': 0.9359, 'grad_norm': 3.036165950737858, 'learning_rate': 1.283844708297
8987e-09, 'flos': 18762188038680.0, 'epoch': 0.99, 'num_input_tokens_seen': 1751
75880}
 99%|███████████████████████████████████▌| 8224/8316 [10:49:56<18:19, 11.96s/it]
language loss:  0.8974992036819458
mlp auxiliary loss:  0.11902669072151184
clip auxiliary loss:  0.11902669072151184
language loss:  0.8390542268753052
mlp auxiliary loss:  0.11905273795127869
clip auxiliary loss:  0.11905273795127869
{'loss': 1.0409, 'grad_norm': 5.138859686324625, 'learning_rate': 1.256089629614
3208e-09, 'flos': 17294016484320.0, 'epoch': 0.99, 'num_input_tokens_seen': 1751
94065}
 99%|███████████████████████████████████▌| 8225/8316 [10:50:09<18:36, 12.27s/it]
language loss:  0.7514737844467163
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
language loss:  0.6472428441047668
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
{'loss': 1.0297, 'grad_norm': 3.8256476658032965, 'learning_rate': 1.22863775879
26722e-09, 'flos': 13438030993080.0, 'epoch': 0.99, 'num_input_tokens_seen': 175
210575}
 99%|███████████████████████████████████▌| 8226/8316 [10:50:20<18:07, 12.09s/it]
language loss:  0.9658952355384827
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
language loss:  0.6820991039276123
mlp auxiliary loss:  0.11903320252895355
clip auxiliary loss:  0.11903320252895355
{'loss': 0.9823, 'grad_norm': 6.647510504282112, 'learning_rate': 1.201489099997
3992e-09, 'flos': 18631051467720.0, 'epoch': 0.99, 'num_input_tokens_seen': 1752
27215}
 99%|███████████████████████████████████▌| 8227/8316 [10:50:32<17:41, 11.92s/it]
language loss:  0.7890053987503052
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
language loss:  0.8606365919113159
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
{'loss': 1.001, 'grad_norm': 3.87230896569076, 'learning_rate': 1.17464365734720
73e-09, 'flos': 18108958108680.0, 'epoch': 0.99, 'num_input_tokens_seen': 175248
670}
 99%|███████████████████████████████████▌| 8228/8316 [10:50:43<17:16, 11.78s/it]
language loss:  0.9449014067649841
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.7161575555801392
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
{'loss': 0.9122, 'grad_norm': 4.565617789407058, 'learning_rate': 1.148101434914
1726e-09, 'flos': 14327619498720.0, 'epoch': 0.99, 'num_input_tokens_seen': 1752
65610}
 99%|███████████████████████████████████▌| 8229/8316 [10:50:54<16:50, 11.61s/it]
language loss:  0.606391191482544
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
language loss:  0.7329404354095459
mlp auxiliary loss:  0.11903809010982513
clip auxiliary loss:  0.11903809010982513
{'loss': 1.062, 'grad_norm': 3.5650606823821924, 'learning_rate': 1.121862436724
852e-09, 'flos': 17529853813440.0, 'epoch': 0.99, 'num_input_tokens_seen': 17528
4170}
 99%|███████████████████████████████████▋| 8230/8316 [10:51:06<16:43, 11.67s/it]
language loss:  0.40576761960983276
mlp auxiliary loss:  0.11905111372470856
clip auxiliary loss:  0.11905111372470856
language loss:  0.7182514667510986
mlp auxiliary loss:  0.11905110627412796
clip auxiliary loss:  0.11905110627412796
{'loss': 0.9391, 'grad_norm': 3.8720414957944094, 'learning_rate': 1.09592666675
98388e-09, 'flos': 15485092211760.0, 'epoch': 0.99, 'num_input_tokens_seen': 175
302705}
 99%|███████████████████████████████████▋| 8231/8316 [10:51:18<16:35, 11.71s/it]
language loss:  0.6364755630493164
mlp auxiliary loss:  0.11903483420610428
clip auxiliary loss:  0.11903483420610428
language loss:  0.7220487594604492
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
{'loss': 0.9633, 'grad_norm': 3.8332947991687236, 'learning_rate': 1.07029412895
33196e-09, 'flos': 15144646109160.0, 'epoch': 0.99, 'num_input_tokens_seen': 175
321100}
 99%|███████████████████████████████████▋| 8232/8316 [10:51:29<16:18, 11.65s/it]
language loss:  0.7130281925201416
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
language loss:  0.7984653115272522
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
[2024-09-20 11:51:44,794] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 1.1037, 'grad_norm': 4.8529367266683865, 'learning_rate': 1.04496482719
39615e-09, 'flos': 13145581328040.0, 'epoch': 0.99, 'num_input_tokens_seen': 175
337165}
 99%|███████████████████████████████████▋| 8233/8316 [10:51:42<16:31, 11.94s/it]
language loss:  0.9042214751243591
mlp auxiliary loss:  0.11902832239866257
clip auxiliary loss:  0.11902832239866257
language loss:  0.5335214138031006
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
{'loss': 0.9628, 'grad_norm': 2.506569432476232, 'learning_rate': 1.019938765324
0243e-09, 'flos': 16898889724080.0, 'epoch': 0.99, 'num_input_tokens_seen': 1753
56575}
 99%|███████████████████████████████████▋| 8234/8316 [10:51:54<16:16, 11.91s/it]
language loss:  0.704190731048584
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
language loss:  0.7781065702438354
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
{'loss': 0.9275, 'grad_norm': 3.8543147840477183, 'learning_rate': 9.95215947140
0267e-10, 'flos': 11604326631960.0, 'epoch': 0.99, 'num_input_tokens_seen': 1753
73335}
 99%|███████████████████████████████████▋| 8235/8316 [10:52:06<15:59, 11.84s/it]
language loss:  0.7291337847709656
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
language loss:  0.8520241975784302
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
{'loss': 1.061, 'grad_norm': 6.467662185387168, 'learning_rate': 9.7079637639230
22e-10, 'flos': 16030984489440.0, 'epoch': 0.99, 'num_input_tokens_seen': 175392
105}
 99%|███████████████████████████████████▋| 8236/8316 [10:52:17<15:40, 11.75s/it]
language loss:  0.9894084930419922
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.7787566781044006
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
{'loss': 1.015, 'grad_norm': 3.946265741896263, 'learning_rate': 9.4668005678544
27e-10, 'flos': 11420962328160.0, 'epoch': 0.99, 'num_input_tokens_seen': 175410
425}
 99%|███████████████████████████████████▋| 8237/8316 [10:52:29<15:25, 11.71s/it]
language loss:  0.761940062046051
mlp auxiliary loss:  0.11902344226837158
clip auxiliary loss:  0.11902344226837158
language loss:  0.4429420828819275
mlp auxiliary loss:  0.11904134601354599
clip auxiliary loss:  0.11904134601354599
{'loss': 0.9147, 'grad_norm': 5.30576930300673, 'learning_rate': 9.2286699197785
53e-10, 'flos': 18972448078680.0, 'epoch': 0.99, 'num_input_tokens_seen': 175429
070}
 99%|███████████████████████████████████▋| 8238/8316 [10:52:41<15:33, 11.96s/it]
language loss:  0.8317258954048157
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
language loss:  0.797040581703186
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
{'loss': 1.0212, 'grad_norm': 3.972554703486458, 'learning_rate': 8.993571855817
617e-10, 'flos': 16428472189800.0, 'epoch': 0.99, 'num_input_tokens_seen': 17544
7620}
 99%|███████████████████████████████████▋| 8239/8316 [10:52:53<15:14, 11.88s/it]
language loss:  0.9738520979881287
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.5315518975257874
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
{'loss': 0.9624, 'grad_norm': 3.1487535644607996, 'learning_rate': 8.76150641163
8642e-10, 'flos': 15694585712760.0, 'epoch': 0.99, 'num_input_tokens_seen': 1754
66805}
 99%|███████████████████████████████████▋| 8240/8316 [10:53:06<15:16, 12.06s/it]
language loss:  0.6832899451255798
mlp auxiliary loss:  0.11901650577783585
clip auxiliary loss:  0.11901650577783585
language loss:  0.933886706829071
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
{'loss': 0.9609, 'grad_norm': 4.557506389100769, 'learning_rate': 8.532473622442
36e-10, 'flos': 13647800448120.0, 'epoch': 0.99, 'num_input_tokens_seen': 175485
335}
 99%|███████████████████████████████████▋| 8241/8316 [10:53:17<14:58, 11.98s/it]
language loss:  0.8404053449630737
mlp auxiliary loss:  0.11903809010982513
clip auxiliary loss:  0.11903809010982513
language loss:  0.619819164276123
mlp auxiliary loss:  0.11903320252895355
clip auxiliary loss:  0.11903320252895355
{'loss': 0.9148, 'grad_norm': 4.192782130885209, 'learning_rate': 8.306473522976
532e-10, 'flos': 16794372935280.0, 'epoch': 0.99, 'num_input_tokens_seen': 17550
4460}
 99%|███████████████████████████████████▋| 8242/8316 [10:53:29<14:35, 11.83s/it]
language loss:  0.5026689171791077
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
language loss:  0.6219905018806458
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
{'loss': 0.9451, 'grad_norm': 6.792765460227245, 'learning_rate': 8.083506147522
623e-10, 'flos': 16140805728120.0, 'epoch': 0.99, 'num_input_tokens_seen': 17552
3575}
 99%|███████████████████████████████████▋| 8243/8316 [10:53:41<14:23, 11.84s/it]
language loss:  0.8869767785072327
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.7219277024269104
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
[2024-09-20 11:53:56,009] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0795, 'grad_norm': 3.5860873587882436, 'learning_rate': 7.86357152990
6909e-10, 'flos': 9532945248120.0, 'epoch': 0.99, 'num_input_tokens_seen': 17553
8880}
 99%|███████████████████████████████████▋| 8244/8316 [10:53:53<14:30, 12.10s/it]
language loss:  0.6305193901062012
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
language loss:  0.5807567834854126
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
[2024-09-20 11:54:09,253] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8801, 'grad_norm': 0.7891050648303506, 'learning_rate': 7.64666970348
9372e-10, 'flos': 44260761347640.0, 'epoch': 0.99, 'num_input_tokens_seen': 1756
02910}
 99%|███████████████████████████████████▋| 8245/8316 [10:54:07<14:43, 12.44s/it]
language loss:  0.5286502838134766
mlp auxiliary loss:  0.11903809010982513
clip auxiliary loss:  0.11903809010982513
language loss:  0.49480414390563965
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
{'loss': 0.8064, 'grad_norm': 5.594531468195778, 'learning_rate': 7.432800701177
023e-10, 'flos': 13308212869200.0, 'epoch': 0.99, 'num_input_tokens_seen': 17562
0630}
 99%|███████████████████████████████████▋| 8246/8316 [10:54:19<14:31, 12.46s/it]
language loss:  0.6522117853164673
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
language loss:  0.5547760725021362
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
[2024-09-20 11:54:35,724] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8309, 'grad_norm': 0.8039691273944197, 'learning_rate': 7.22196455541
5017e-10, 'flos': 47209527936240.0, 'epoch': 0.99, 'num_input_tokens_seen': 1756
80010}
 99%|███████████████████████████████████▋| 8247/8316 [10:54:33<14:50, 12.91s/it]
language loss:  0.7127741575241089
mlp auxiliary loss:  0.11905110627412796
clip auxiliary loss:  0.11905110627412796
language loss:  0.40395936369895935
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
{'loss': 0.9718, 'grad_norm': 4.133137161973047, 'learning_rate': 7.014161298182
22e-10, 'flos': 11735401879800.0, 'epoch': 0.99, 'num_input_tokens_seen': 175697
350}
 99%|███████████████████████████████████▋| 8248/8316 [10:54:45<14:10, 12.51s/it]
language loss:  0.15657292306423187
mlp auxiliary loss:  0.11903483420610428
clip auxiliary loss:  0.11903483420610428
language loss:  0.6165392994880676
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
{'loss': 0.8076, 'grad_norm': 4.057668541905768, 'learning_rate': 6.809390961006
745e-10, 'flos': 18081019879440.0, 'epoch': 0.99, 'num_input_tokens_seen': 17571
7200}
 99%|███████████████████████████████████▋| 8249/8316 [10:54:56<13:39, 12.23s/it]
language loss:  0.7746107578277588
mlp auxiliary loss:  0.11905436217784882
clip auxiliary loss:  0.11905436217784882
language loss:  0.3516082465648651
mlp auxiliary loss:  0.1190478503704071
clip auxiliary loss:  0.1190478503704071
{'loss': 0.9156, 'grad_norm': 2.6924442675816342, 'learning_rate': 6.60765357494
8191e-10, 'flos': 17819329307160.0, 'epoch': 0.99, 'num_input_tokens_seen': 1757
37700}
 99%|███████████████████████████████████▋| 8250/8316 [10:55:08<13:16, 12.07s/it]
language loss:  0.8965644836425781
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
language loss:  0.7678134441375732
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
{'loss': 1.0447, 'grad_norm': 9.469351469268377, 'learning_rate': 6.408949170613
187e-10, 'flos': 15506407544040.0, 'epoch': 0.99, 'num_input_tokens_seen': 17575
6685}
 99%|███████████████████████████████████▋| 8251/8316 [10:55:20<12:56, 11.94s/it]
language loss:  0.7574024200439453
mlp auxiliary loss:  0.11903157830238342
clip auxiliary loss:  0.11903157830238342
language loss:  0.7815127372741699
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
{'loss': 1.0444, 'grad_norm': 3.672422847054233, 'learning_rate': 6.213277778144
288e-10, 'flos': 17688039428400.0, 'epoch': 0.99, 'num_input_tokens_seen': 17577
8050}
 99%|███████████████████████████████████▋| 8252/8316 [10:55:31<12:35, 11.80s/it]
language loss:  0.7490755319595337
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.7833438515663147
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
{'loss': 0.8999, 'grad_norm': 4.397638007726087, 'learning_rate': 6.020639427224
416e-10, 'flos': 15350981469480.0, 'epoch': 0.99, 'num_input_tokens_seen': 17579
5415}
 99%|███████████████████████████████████▋| 8253/8316 [10:55:43<12:17, 11.71s/it]
language loss:  0.22510367631912231
mlp auxiliary loss:  0.11902627348899841
clip auxiliary loss:  0.11902627348899841
language loss:  0.8286324739456177
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
{'loss': 0.9413, 'grad_norm': 5.043906457830744, 'learning_rate': 5.831034147076
864e-10, 'flos': 17785780012440.0, 'epoch': 0.99, 'num_input_tokens_seen': 17581
2385}
 99%|███████████████████████████████████▋| 8254/8316 [10:55:54<12:05, 11.70s/it]
language loss:  0.5890383124351501
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
language loss:  0.6093381643295288
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
[2024-09-20 11:56:10,196] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 0.804, 'grad_norm': 0.7042260767536733, 'learning_rate': 5.644461966463
065e-10, 'flos': 49351012742400.0, 'epoch': 0.99, 'num_input_tokens_seen': 17587
9715}
 99%|███████████████████████████████████▋| 8255/8316 [10:56:08<12:24, 12.21s/it]
language loss:  0.7876834273338318
mlp auxiliary loss:  0.11903809010982513
clip auxiliary loss:  0.11903809010982513
language loss:  0.7522746920585632
mlp auxiliary loss:  0.11904134601354599
clip auxiliary loss:  0.11904134601354599
{'loss': 0.9779, 'grad_norm': 5.663376170702323, 'learning_rate': 5.460922913687
049e-10, 'flos': 14856029139120.0, 'epoch': 0.99, 'num_input_tokens_seen': 17589
8525}
 99%|███████████████████████████████████▋| 8256/8316 [10:56:19<12:03, 12.05s/it]
language loss:  0.8119332194328308
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.7059550285339355
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
{'loss': 0.9689, 'grad_norm': 4.733393098337457, 'learning_rate': 5.280417016593
208e-10, 'flos': 15850808987880.0, 'epoch': 0.99, 'num_input_tokens_seen': 17591
8035}
 99%|███████████████████████████████████▋| 8257/8316 [10:56:31<11:43, 11.93s/it]
language loss:  0.8503929376602173
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
language loss:  0.6535283923149109
mlp auxiliary loss:  0.11903320252895355
clip auxiliary loss:  0.11903320252895355
{'loss': 0.9753, 'grad_norm': 5.375677199175137, 'learning_rate': 5.102944302559
642e-10, 'flos': 12311961265560.0, 'epoch': 0.99, 'num_input_tokens_seen': 17593
5250}
 99%|███████████████████████████████████▋| 8258/8316 [10:56:42<11:23, 11.79s/it]
language loss:  0.5936471223831177
mlp auxiliary loss:  0.11903809010982513
clip auxiliary loss:  0.11903809010982513
language loss:  0.6205564141273499
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
{'loss': 1.0066, 'grad_norm': 3.711910337121585, 'learning_rate': 4.928504798513
7e-10, 'flos': 16114400576880.0, 'epoch': 0.99, 'num_input_tokens_seen': 1759543
90}
 99%|███████████████████████████████████▊| 8259/8316 [10:56:54<11:10, 11.76s/it]
language loss:  0.42780280113220215
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
language loss:  0.920948326587677
mlp auxiliary loss:  0.11905273795127869
clip auxiliary loss:  0.11905273795127869
{'loss': 0.9782, 'grad_norm': 2.9790919012189274, 'learning_rate': 4.75709853091
6436e-10, 'flos': 20047853812920.0, 'epoch': 0.99, 'num_input_tokens_seen': 1759
74555}
 99%|███████████████████████████████████▊| 8260/8316 [10:57:06<10:57, 11.75s/it]
language loss:  0.38934919238090515
mlp auxiliary loss:  0.11905761808156967
clip auxiliary loss:  0.11905761808156967
language loss:  0.7257288098335266
mlp auxiliary loss:  0.11903645843267441
clip auxiliary loss:  0.11903645843267441
{'loss': 1.0095, 'grad_norm': 8.042164314433743, 'learning_rate': 4.588725525767
0563e-10, 'flos': 14304679103760.0, 'epoch': 0.99, 'num_input_tokens_seen': 1759
91315}
 99%|███████████████████████████████████▊| 8261/8316 [10:57:17<10:43, 11.69s/it]
language loss:  0.591518223285675
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
language loss:  0.8476337790489197
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
{'loss': 0.9922, 'grad_norm': 5.45985657304455, 'learning_rate': 4.4233858086117
906e-10, 'flos': 15171143245080.0, 'epoch': 0.99, 'num_input_tokens_seen': 17600
9560}
 99%|███████████████████████████████████▊| 8262/8316 [10:57:29<10:29, 11.66s/it]
language loss:  0.9988075494766235
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
language loss:  0.8394585251808167
mlp auxiliary loss:  0.11902181059122086
clip auxiliary loss:  0.11902181059122086
{'loss': 0.89, 'grad_norm': 5.767112808756717, 'learning_rate': 4.26107940452835
6e-10, 'flos': 14016859334280.0, 'epoch': 0.99, 'num_input_tokens_seen': 1760287
60}
 99%|███████████████████████████████████▊| 8263/8316 [10:57:40<10:16, 11.63s/it]
language loss:  0.8527273535728455
mlp auxiliary loss:  0.11904947459697723
clip auxiliary loss:  0.11904947459697723
language loss:  0.9512225389480591
mlp auxiliary loss:  0.119036465883255
clip auxiliary loss:  0.119036465883255
{'loss': 0.9082, 'grad_norm': 3.3825814606527453, 'learning_rate': 4.10180633814
37205e-10, 'flos': 15613959827280.0, 'epoch': 0.99, 'num_input_tokens_seen': 176
048865}
 99%|███████████████████████████████████▊| 8264/8316 [10:57:52<10:09, 11.71s/it]
language loss:  0.7176552414894104
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
language loss:  0.5425522923469543
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
[2024-09-20 11:58:09,268] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 0.889, 'grad_norm': 0.9197175594787576, 'learning_rate': 3.945566633614
1167e-10, 'flos': 49994584281000.0, 'epoch': 0.99, 'num_input_tokens_seen': 1761
12365}
 99%|███████████████████████████████████▊| 8265/8316 [10:58:07<10:35, 12.46s/it]
language loss:  0.853705108165741
mlp auxiliary loss:  0.119036465883255
clip auxiliary loss:  0.119036465883255
language loss:  0.8758804798126221
mlp auxiliary loss:  0.11903157830238342
clip auxiliary loss:  0.11903157830238342
[2024-09-20 11:58:21,645] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0365, 'grad_norm': 7.054350886124148, 'learning_rate': 3.792360314645
0267e-10, 'flos': 10659045637200.0, 'epoch': 0.99, 'num_input_tokens_seen': 1761
28145}
 99%|███████████████████████████████████▊| 8266/8316 [10:58:19<10:21, 12.43s/it]
language loss:  0.7960039377212524
mlp auxiliary loss:  0.11903809010982513
clip auxiliary loss:  0.11903809010982513
language loss:  0.7938763499259949
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
{'loss': 1.0161, 'grad_norm': 5.654709748527142, 'learning_rate': 3.642187404473
418e-10, 'flos': 12548043887160.0, 'epoch': 0.99, 'num_input_tokens_seen': 17614
6025}
 99%|███████████████████████████████████▊| 8267/8316 [10:58:30<09:55, 12.15s/it]
language loss:  0.8989711403846741
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
language loss:  0.9222778081893921
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
{'loss': 1.0741, 'grad_norm': 8.262858225202988, 'learning_rate': 3.495047925885
508e-10, 'flos': 13596707193000.0, 'epoch': 0.99, 'num_input_tokens_seen': 17616
4080}
 99%|███████████████████████████████████▊| 8268/8316 [10:58:43<09:50, 12.30s/it]
language loss:  0.9679695963859558
mlp auxiliary loss:  0.11904134601354599
clip auxiliary loss:  0.11904134601354599
language loss:  1.1090853214263916
mlp auxiliary loss:  0.119036465883255
clip auxiliary loss:  0.119036465883255
{'loss': 1.0612, 'grad_norm': 5.4850355657991, 'learning_rate': 3.350941901199e-
10, 'flos': 12647194902960.0, 'epoch': 0.99, 'num_input_tokens_seen': 176180720}
 99%|███████████████████████████████████▊| 8269/8316 [10:58:55<09:25, 12.04s/it]
language loss:  0.9225630760192871
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
language loss:  0.869304895401001
mlp auxiliary loss:  0.11903809010982513
clip auxiliary loss:  0.11903809010982513
{'loss': 1.059, 'grad_norm': 6.784156563157833, 'learning_rate': 3.2098693522764
066e-10, 'flos': 13325848814280.0, 'epoch': 0.99, 'num_input_tokens_seen': 17619
3640}
 99%|███████████████████████████████████▊| 8270/8316 [10:59:06<09:02, 11.79s/it]
language loss:  1.0390793085098267
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
language loss:  0.7195910215377808
mlp auxiliary loss:  0.11905273795127869
clip auxiliary loss:  0.11905273795127869
{'loss': 1.0449, 'grad_norm': 3.897364405633984, 'learning_rate': 3.071830300516
165e-10, 'flos': 14908747456920.0, 'epoch': 0.99, 'num_input_tokens_seen': 17621
1190}
 99%|███████████████████████████████████▊| 8271/8316 [10:59:17<08:45, 11.68s/it]
language loss:  0.7891815900802612
mlp auxiliary loss:  0.11905110627412796
clip auxiliary loss:  0.11905110627412796
language loss:  0.7809002995491028
mlp auxiliary loss:  0.11905110627412796
clip auxiliary loss:  0.11905110627412796
{'loss': 0.9218, 'grad_norm': 3.8902646552817925, 'learning_rate': 2.93682476686
15234e-10, 'flos': 10424036170200.0, 'epoch': 0.99, 'num_input_tokens_seen': 176
229500}
 99%|███████████████████████████████████▊| 8272/8316 [10:59:29<08:30, 11.59s/it]
language loss:  0.9500536322593689
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
language loss:  0.20219556987285614
mlp auxiliary loss:  0.1190478578209877
clip auxiliary loss:  0.1190478578209877
{'loss': 0.8307, 'grad_norm': 7.405507754703959, 'learning_rate': 2.804852771789
434e-10, 'flos': 8923143183240.0, 'epoch': 0.99, 'num_input_tokens_seen': 176242
520}
 99%|███████████████████████████████████▊| 8273/8316 [10:59:40<08:20, 11.65s/it]
language loss:  0.5116689205169678
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
language loss:  0.4071764349937439
mlp auxiliary loss:  0.11904134601354599
clip auxiliary loss:  0.11904134601354599
{'loss': 0.7851, 'grad_norm': 3.908585727461535, 'learning_rate': 2.675914335321
661e-10, 'flos': 13360931187000.0, 'epoch': 0.99, 'num_input_tokens_seen': 17626
0995}
 99%|███████████████████████████████████▊| 8274/8316 [10:59:52<08:08, 11.63s/it]
language loss:  0.9055372476577759
mlp auxiliary loss:  0.11905598640441895
clip auxiliary loss:  0.11905598640441895
language loss:  0.7065872550010681
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
{'loss': 1.0045, 'grad_norm': 5.994323906655395, 'learning_rate': 2.550009477018
111e-10, 'flos': 17713831348440.0, 'epoch': 1.0, 'num_input_tokens_seen': 176279
485}
100%|███████████████████████████████████▊| 8275/8316 [11:00:03<07:55, 11.61s/it]
language loss:  0.38572314381599426
mlp auxiliary loss:  0.11905436217784882
clip auxiliary loss:  0.11905436217784882
language loss:  0.4305206537246704
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
{'loss': 0.8576, 'grad_norm': 4.298331384933245, 'learning_rate': 2.427138215979
0634e-10, 'flos': 16875550728840.0, 'epoch': 1.0, 'num_input_tokens_seen': 17629
6635}
100%|███████████████████████████████████▊| 8276/8316 [11:00:15<07:41, 11.53s/it]
language loss:  0.9324343800544739
mlp auxiliary loss:  0.11903809010982513
clip auxiliary loss:  0.11903809010982513
language loss:  0.6683378219604492
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
[2024-09-20 12:00:30,958] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 1.0812, 'grad_norm': 2.7030890137145787, 'learning_rate': 2.30730057084
29406e-10, 'flos': 15799777055880.0, 'epoch': 1.0, 'num_input_tokens_seen': 1763
16000}
100%|███████████████████████████████████▊| 8277/8316 [11:00:28<07:52, 12.11s/it]
language loss:  0.7296556830406189
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.9099682569503784
mlp auxiliary loss:  0.119036465883255
clip auxiliary loss:  0.119036465883255
{'loss': 0.9487, 'grad_norm': 4.144907913565569, 'learning_rate': 2.190496559788
535e-10, 'flos': 15061935237600.0, 'epoch': 1.0, 'num_input_tokens_seen': 176334
005}
100%|███████████████████████████████████▊| 8278/8316 [11:00:40<07:35, 11.99s/it]
language loss:  0.6642982363700867
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
language loss:  0.6371344327926636
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
{'loss': 0.9819, 'grad_norm': 4.6054339613826025, 'learning_rate': 2.07672620053
72265e-10, 'flos': 10502975669880.0, 'epoch': 1.0, 'num_input_tokens_seen': 1763
51240}
100%|███████████████████████████████████▊| 8279/8316 [11:00:51<07:16, 11.80s/it]
language loss:  0.7315943837165833
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
language loss:  0.5494574308395386
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
{'loss': 0.9749, 'grad_norm': 4.444848436961993, 'learning_rate': 1.965989510346
322e-10, 'flos': 13623112344240.0, 'epoch': 1.0, 'num_input_tokens_seen': 176370
080}
100%|███████████████████████████████████▊| 8280/8316 [11:01:03<07:00, 11.69s/it]
language loss:  0.8655051589012146
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.4768577218055725
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
{'loss': 0.928, 'grad_norm': 22.830187652792493, 'learning_rate': 1.858286506013
4955e-10, 'flos': 14225892911880.0, 'epoch': 1.0, 'num_input_tokens_seen': 17638
7990}
100%|███████████████████████████████████▊| 8281/8316 [11:01:15<06:52, 11.79s/it]
language loss:  0.6810111999511719
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
language loss:  0.536152184009552
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
{'loss': 0.8175, 'grad_norm': 0.7829577663353872, 'learning_rate': 1.75361720387
90098e-10, 'flos': 41135749485240.0, 'epoch': 1.0, 'num_input_tokens_seen': 1764
48020}
100%|███████████████████████████████████▊| 8282/8316 [11:01:27<06:48, 12.02s/it]
language loss:  0.10510075092315674
mlp auxiliary loss:  0.11902669072151184
clip auxiliary loss:  0.11902669072151184
language loss:  0.7700405716896057
mlp auxiliary loss:  0.11903809010982513
clip auxiliary loss:  0.11903809010982513
{'loss': 0.9125, 'grad_norm': 8.731615207163939, 'learning_rate': 1.651981619819
054e-10, 'flos': 19785611332560.0, 'epoch': 1.0, 'num_input_tokens_seen': 176464
890}
100%|███████████████████████████████████▊| 8283/8316 [11:01:39<06:32, 11.90s/it]
language loss:  0.20866155624389648
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
language loss:  0.6110549569129944
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
{'loss': 0.9162, 'grad_norm': 3.8278571833134953, 'learning_rate': 1.55337976925
46257e-10, 'flos': 17084124383040.0, 'epoch': 1.0, 'num_input_tokens_seen': 1764
83345}
100%|███████████████████████████████████▊| 8284/8316 [11:01:51<06:18, 11.83s/it]
language loss:  0.7564903497695923
mlp auxiliary loss:  0.11903645843267441
clip auxiliary loss:  0.11903645843267441
language loss:  0.8042489290237427
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
{'loss': 1.0512, 'grad_norm': 6.672603665658187, 'learning_rate': 1.457811667140
4296e-10, 'flos': 13255954474800.0, 'epoch': 1.0, 'num_input_tokens_seen': 17650
1345}
100%|███████████████████████████████████▊| 8285/8316 [11:02:02<06:03, 11.72s/it]
language loss:  0.659926176071167
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
language loss:  0.7060515880584717
mlp auxiliary loss:  0.11903320252895355
clip auxiliary loss:  0.11903320252895355
{'loss': 0.9436, 'grad_norm': 4.750096370321738, 'learning_rate': 1.365277327975
9777e-10, 'flos': 14199825037800.0, 'epoch': 1.0, 'num_input_tokens_seen': 17651
7715}
100%|███████████████████████████████████▊| 8286/8316 [11:02:14<05:52, 11.74s/it]
language loss:  0.21402987837791443
mlp auxiliary loss:  0.11905111372470856
clip auxiliary loss:  0.11905111372470856
language loss:  0.718335747718811
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
[2024-09-20 12:02:30,461] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 0.852, 'grad_norm': 4.838603971618781, 'learning_rate': 1.2757767657989
305e-10, 'flos': 23612830732440.0, 'epoch': 1.0, 'num_input_tokens_seen': 176541
225}
100%|███████████████████████████████████▊| 8287/8316 [11:02:28<05:59, 12.38s/it]
language loss:  0.9438936114311218
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
language loss:  0.635784387588501
mlp auxiliary loss:  0.1190478503704071
clip auxiliary loss:  0.1190478503704071
{'loss': 1.092, 'grad_norm': 5.27152616548598, 'learning_rate': 1.18930999418509
48e-10, 'flos': 16426356542160.0, 'epoch': 1.0, 'num_input_tokens_seen': 1765598
40}
100%|███████████████████████████████████▉| 8288/8316 [11:02:40<05:41, 12.19s/it]
language loss:  0.5102301836013794
mlp auxiliary loss:  0.11905273795127869
clip auxiliary loss:  0.11905273795127869
language loss:  0.6647230982780457
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
{'loss': 0.9918, 'grad_norm': 3.8025425185134405, 'learning_rate': 1.10587702625
2866e-10, 'flos': 16323556800720.0, 'epoch': 1.0, 'num_input_tokens_seen': 17657
7890}
100%|███████████████████████████████████▉| 8289/8316 [11:02:51<05:26, 12.09s/it]
language loss:  0.8383119106292725
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
language loss:  0.5477363467216492
mlp auxiliary loss:  0.1190478503704071
clip auxiliary loss:  0.1190478503704071
{'loss': 0.9475, 'grad_norm': 3.6986800287917987, 'learning_rate': 1.02547787465
65663e-10, 'flos': 9321182791680.0, 'epoch': 1.0, 'num_input_tokens_seen': 17659
2885}
100%|███████████████████████████████████▉| 8290/8316 [11:03:03<05:09, 11.90s/it]
language loss:  0.8686684370040894
mlp auxiliary loss:  0.11903483420610428
clip auxiliary loss:  0.11903483420610428
language loss:  0.6166637539863586
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
{'loss': 0.958, 'grad_norm': 3.096712331801289, 'learning_rate': 9.4811255159532
59e-11, 'flos': 10345679240160.0, 'epoch': 1.0, 'num_input_tokens_seen': 1766106
65}
100%|███████████████████████████████████▉| 8291/8316 [11:03:16<05:03, 12.14s/it]
language loss:  0.9166135787963867
mlp auxiliary loss:  0.11905273795127869
clip auxiliary loss:  0.11905273795127869
language loss:  0.6050919890403748
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
{'loss': 1.0303, 'grad_norm': 10.04786518254313, 'learning_rate': 8.737810688064
228e-11, 'flos': 18313821714120.0, 'epoch': 1.0, 'num_input_tokens_seen': 176630
220}
100%|███████████████████████████████████▉| 8292/8316 [11:03:27<04:46, 11.95s/it]
language loss:  0.7599692344665527
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
language loss:  0.8064055442810059
mlp auxiliary loss:  0.11904622614383698
clip auxiliary loss:  0.11904622614383698
{'loss': 1.0147, 'grad_norm': 3.4689265025829337, 'learning_rate': 8.02483437560
8414e-11, 'flos': 15248733636120.0, 'epoch': 1.0, 'num_input_tokens_seen': 17664
8530}
100%|███████████████████████████████████▉| 8293/8316 [11:03:39<04:31, 11.80s/it]
language loss:  0.8573669791221619
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
language loss:  0.5391573905944824
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
[2024-09-20 12:03:54,988] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 0.891, 'grad_norm': 0.8248555765028567, 'learning_rate': 7.342196686788
149e-11, 'flos': 51718804680600.0, 'epoch': 1.0, 'num_input_tokens_seen': 176701
415}
100%|███████████████████████████████████▉| 8294/8316 [11:03:52<04:32, 12.41s/it]
language loss:  0.8128153085708618
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
language loss:  0.7786699533462524
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
{'loss': 0.9036, 'grad_norm': 4.777289252231573, 'learning_rate': 6.689897725142
834e-11, 'flos': 13960338983040.0, 'epoch': 1.0, 'num_input_tokens_seen': 176720
610}
100%|███████████████████████████████████▉| 8295/8316 [11:04:05<04:20, 12.43s/it]
language loss:  1.0755164623260498
mlp auxiliary loss:  0.11903483420610428
clip auxiliary loss:  0.11903483420610428
language loss:  0.7705391645431519
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
{'loss': 1.0972, 'grad_norm': 4.661327691263178, 'learning_rate': 6.067937589615
545e-11, 'flos': 11289212526000.0, 'epoch': 1.0, 'num_input_tokens_seen': 176738
405}
100%|███████████████████████████████████▉| 8296/8316 [11:04:16<04:03, 12.18s/it]
language loss:  0.5470969676971436
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
language loss:  0.6241592168807983
mlp auxiliary loss:  0.11896157264709473
clip auxiliary loss:  0.11896157264709473
{'loss': 0.8111, 'grad_norm': 0.763684909250779, 'learning_rate': 5.476316374575
241e-11, 'flos': 42916766190120.0, 'epoch': 1.0, 'num_input_tokens_seen': 176801
610}
100%|███████████████████████████████████▉| 8297/8316 [11:04:29<03:51, 12.19s/it]
language loss:  0.6679555773735046
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
language loss:  0.8973448872566223
mlp auxiliary loss:  0.11902273446321487
clip auxiliary loss:  0.11902273446321487
{'loss': 0.9545, 'grad_norm': 3.7649817966056585, 'learning_rate': 4.91503416977
23476e-11, 'flos': 15979553957160.0, 'epoch': 1.0, 'num_input_tokens_seen': 1768
21220}
100%|███████████████████████████████████▉| 8298/8316 [11:04:40<03:35, 12.00s/it]
language loss:  0.13627465069293976
mlp auxiliary loss:  0.11906087398529053
clip auxiliary loss:  0.11906087398529053
language loss:  0.6287740468978882
mlp auxiliary loss:  0.11903157830238342
clip auxiliary loss:  0.11903157830238342
[2024-09-20 12:04:55,744] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8831, 'grad_norm': 3.7359412831056362, 'learning_rate': 4.38409106033
8768e-11, 'flos': 18525860124600.0, 'epoch': 1.0, 'num_input_tokens_seen': 17684
1410}
100%|███████████████████████████████████▉| 8299/8316 [11:04:53<03:28, 12.27s/it]
language loss:  0.779621422290802
mlp auxiliary loss:  0.1190478578209877
clip auxiliary loss:  0.1190478578209877
language loss:  0.6493302583694458
mlp auxiliary loss:  0.119036465883255
clip auxiliary loss:  0.119036465883255
{'loss': 0.9531, 'grad_norm': 3.4961581261531074, 'learning_rate': 3.88348712681
0081e-11, 'flos': 16087872779400.0, 'epoch': 1.0, 'num_input_tokens_seen': 17686
0390}
100%|███████████████████████████████████▉| 8300/8316 [11:05:04<03:12, 12.01s/it]
language loss:  0.8218567967414856
mlp auxiliary loss:  0.11903320252895355
clip auxiliary loss:  0.11903320252895355
language loss:  0.9588649272918701
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
{'loss': 1.0279, 'grad_norm': 3.0755939437268784, 'learning_rate': 3.41322244516
995e-11, 'flos': 12915385725960.0, 'epoch': 1.0, 'num_input_tokens_seen': 176878
055}
100%|███████████████████████████████████▉| 8301/8316 [11:05:16<02:57, 11.83s/it]
language loss:  0.30394089221954346
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
language loss:  0.5879070162773132
mlp auxiliary loss:  0.11903808265924454
clip auxiliary loss:  0.11903808265924454
{'loss': 0.8524, 'grad_norm': 2.463591195402347, 'learning_rate': 2.973297086694
6925e-11, 'flos': 23875686444000.0, 'epoch': 1.0, 'num_input_tokens_seen': 17689
7655}
100%|███████████████████████████████████▉| 8302/8316 [11:05:28<02:45, 11.84s/it]
language loss:  0.8919674158096313
mlp auxiliary loss:  0.11903320252895355
clip auxiliary loss:  0.11903320252895355
language loss:  0.6690304279327393
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
{'loss': 1.0041, 'grad_norm': 5.332439208580629, 'learning_rate': 2.563711118175
327e-11, 'flos': 10974956943720.0, 'epoch': 1.0, 'num_input_tokens_seen': 176914
260}
100%|███████████████████████████████████▉| 8303/8316 [11:05:39<02:33, 11.78s/it]
language loss:  1.035224437713623
mlp auxiliary loss:  0.11904297024011612
clip auxiliary loss:  0.11904297024011612
language loss:  0.8333784341812134
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
{'loss': 1.0556, 'grad_norm': 3.6041369721764163, 'learning_rate': 2.18446460171
7728e-11, 'flos': 14173695840600.0, 'epoch': 1.0, 'num_input_tokens_seen': 17693
2295}
100%|███████████████████████████████████▉| 8304/8316 [11:05:51<02:20, 11.70s/it]
language loss:  0.5577715039253235
mlp auxiliary loss:  0.11903809010982513
clip auxiliary loss:  0.11903809010982513
language loss:  0.7869184017181396
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
{'loss': 0.9953, 'grad_norm': 6.60929448504815, 'learning_rate': 1.8355575948758
585e-11, 'flos': 14462159502840.0, 'epoch': 1.0, 'num_input_tokens_seen': 176950
000}
100%|███████████████████████████████████▉| 8305/8316 [11:06:03<02:08, 11.71s/it]
language loss:  0.7892119884490967
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.2867283523082733
mlp auxiliary loss:  0.1190478503704071
clip auxiliary loss:  0.1190478503704071
{'loss': 0.9595, 'grad_norm': 5.840691893364423, 'learning_rate': 1.516990150540
7424e-11, 'flos': 16870675540800.0, 'epoch': 1.0, 'num_input_tokens_seen': 17696
6785}
100%|███████████████████████████████████▉| 8306/8316 [11:06:14<01:55, 11.59s/it]
language loss:  0.5462948083877563
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.9885042905807495
mlp auxiliary loss:  0.1190478578209877
clip auxiliary loss:  0.1190478578209877
{'loss': 0.9667, 'grad_norm': 3.622337810710755, 'learning_rate': 1.228762317073
695e-11, 'flos': 17816907043920.0, 'epoch': 1.0, 'num_input_tokens_seen': 176985
335}
100%|███████████████████████████████████▉| 8307/8316 [11:06:25<01:43, 11.52s/it]
language loss:  0.7286312580108643
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
language loss:  0.9009562730789185
mlp auxiliary loss:  0.11903320252895355
clip auxiliary loss:  0.11903320252895355
{'loss': 1.0077, 'grad_norm': 5.327469860977996, 'learning_rate': 9.708741381952
99e-12, 'flos': 22302967439280.0, 'epoch': 1.0, 'num_input_tokens_seen': 1770069
65}
100%|███████████████████████████████████▉| 8308/8316 [11:06:37<01:33, 11.64s/it]
language loss:  0.8287013173103333
mlp auxiliary loss:  0.11904948204755783
clip auxiliary loss:  0.11904948204755783
language loss:  0.7204185128211975
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
[2024-09-20 12:06:52,511] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9724, 'grad_norm': 6.034658467031974, 'learning_rate': 7.433256530076
093e-12, 'flos': 13885569455520.0, 'epoch': 1.0, 'num_input_tokens_seen': 177026
640}
100%|███████████████████████████████████▉| 8309/8316 [11:06:50<01:23, 11.93s/it]
language loss:  0.7997701168060303
mlp auxiliary loss:  0.11903971433639526
clip auxiliary loss:  0.11903971433639526
language loss:  0.5725141167640686
mlp auxiliary loss:  0.11905273795127869
clip auxiliary loss:  0.11905273795127869
{'loss': 0.9794, 'grad_norm': 14.394504748957965, 'learning_rate': 5.46116896038
562e-12, 'flos': 12179322278160.0, 'epoch': 1.0, 'num_input_tokens_seen': 177040
770}
100%|███████████████████████████████████▉| 8310/8316 [11:07:02<01:12, 12.04s/it]
language loss:  0.24276623129844666
mlp auxiliary loss:  0.11905111372470856
clip auxiliary loss:  0.11905111372470856
language loss:  0.6099125742912292
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
{'loss': 0.8502, 'grad_norm': 3.9221609550880476, 'learning_rate': 3.79247897219
7699e-12, 'flos': 33235391251200.0, 'epoch': 1.0, 'num_input_tokens_seen': 17706
1075}
100%|███████████████████████████████████▉| 8311/8316 [11:07:14<01:00, 12.01s/it]
language loss:  0.8712928891181946
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
language loss:  0.5779014825820923
mlp auxiliary loss:  0.1190478503704071
clip auxiliary loss:  0.1190478503704071
{'loss': 0.9045, 'grad_norm': 4.945047910822709, 'learning_rate': 2.427186818199
0895e-12, 'flos': 10712438509320.0, 'epoch': 1.0, 'num_input_tokens_seen': 17707
7960}
100%|███████████████████████████████████▉| 8312/8316 [11:07:26<00:47, 11.95s/it]
language loss:  0.38282930850982666
mlp auxiliary loss:  0.11905110627412796
clip auxiliary loss:  0.11905110627412796
language loss:  0.7733635306358337
mlp auxiliary loss:  0.11904460191726685
clip auxiliary loss:  0.11904460191726685
{'loss': 1.029, 'grad_norm': 4.5428092056034695, 'learning_rate': 1.365292706001
4973e-12, 'flos': 8824636060200.0, 'epoch': 1.0, 'num_input_tokens_seen': 177093
275}
100%|███████████████████████████████████▉| 8313/8316 [11:07:38<00:36, 12.05s/it]
language loss:  0.5429903268814087
mlp auxiliary loss:  0.11905273795127869
clip auxiliary loss:  0.11905273795127869
language loss:  0.6337975263595581
mlp auxiliary loss:  0.1190413385629654
clip auxiliary loss:  0.1190413385629654
{'loss': 0.8703, 'grad_norm': 16.49611826232491, 'learning_rate': 6.067967965872
612e-13, 'flos': 13649701464840.0, 'epoch': 1.0, 'num_input_tokens_seen': 177112
605}
100%|███████████████████████████████████▉| 8314/8316 [11:07:50<00:23, 11.88s/it]
language loss:  0.10477446764707565
mlp auxiliary loss:  0.11903320252895355
clip auxiliary loss:  0.11903320252895355
language loss:  0.9085995554924011
mlp auxiliary loss:  0.11902669817209244
clip auxiliary loss:  0.11902669817209244
[2024-09-20 12:08:05,214] [WARNING] [stage3.py:2069:step] 1 pytorch allocator ca
che flushes since last step. this happens when there is high memory pressure and
 is detrimental to performance. if this is happening frequently consider adjusti
ng settings to reduce memory consumption. If you are unable to make the cache fl
ushes go away consider adding get_accelerator().empty_cache() calls in your trai
ning loop to ensure that all ranks flush their caches at the same time
{'loss': 0.9997, 'grad_norm': 3.420838360118696, 'learning_rate': 1.516992049754
8615e-13, 'flos': 45061236263760.0, 'epoch': 1.0, 'num_input_tokens_seen': 17713
6945}
100%|███████████████████████████████████▉| 8315/8316 [11:08:03<00:12, 12.19s/it]
language loss:  0.6895838975906372
mlp auxiliary loss:  0.1190071552991867
clip auxiliary loss:  0.1190071552991867
language loss:  0.5693322420120239
mlp auxiliary loss:  0.11897134780883789
clip auxiliary loss:  0.11897134780883789
{'loss': 0.7991, 'grad_norm': 3.9955062658103033, 'learning_rate': 0.0, 'flos':
36421455827640.0, 'epoch': 1.0, 'num_input_tokens_seen': 177185545}
100%|████████████████████████████████████| 8316/8316 [11:08:15<00:00, 12.20s/it]
/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/site-packages/torch/nn/mod
ules/module.py:1898: UserWarning: Positional args are being deprecated, use kwar
gs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.h
tml#torch.nn.Module.state_dict for details.
  warnings.warn(
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014faf06ad000ba91b'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
OSError: [Errno 16] Device or resource busy: '.nfs000000014fba0cac000ba921'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f3af4f5000ba920'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
Traceback (most recent call last):
OSError: [Errno 16] Device or resource busy: '.nfs000000014fba0caa000ba917'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014faf06b1000ba918'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fba0ca8000ba91d'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014faf06b3000ba922'
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
Traceback (most recent call last):
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
OSError: [Errno 16] Device or resource busy: '.nfs000000014f616c21000ba91a'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fba0ca9000ba91e'
OSError: [Errno 16] Device or resource busy: '.nfs000000014fa55a4f000ba91f'
OSError: [Errno 16] Device or resource busy: '.nfs000000014f3af4f4000ba91c'
OSError: [Errno 16] Device or resource busy: '.nfs000000014f616c20000ba919'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f9251c6000ba923'
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
Traceback (most recent call last):
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 300, in _run_finalizers
    finalizer()
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/multiprocessing/ut
il.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
OSError: [Errno 16] Device or resource busy: '.nfs000000014fa55a4a000ba926'
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 7
34, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
90, in _rmtree_safe_fd
    onerror(os.unlink, fullname, sys.exc_info())
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
  File "/cm/archive/anonymous/miniconda3/envs/moe/lib/python3.9/shutil.py", line 6
88, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)
OSError: [Errno 16] Device or resource busy: '.nfs000000014f3af4f2000ba924'
OSError: [Errno 16] Device or resource busy: '.nfs000000014f6be2f6000ba925'
{'train_runtime': 40388.5302, 'train_samples_per_second': 8.236, 'train_steps_pe
r_second': 0.206, 'train_loss': 0.38708581090575517, 'epoch': 1.0, 'num_input_to
kens_seen': 177185545}
100%|████████████████████████████████████| 8316/8316 [11:13:08<00:00,  4.86s/it]
[2024-09-20 12:13:43,903] [INFO] [launch.py:348:main] Process 2129702 exits succ
essfully.
[2024-09-20 12:13:46,907] [INFO] [launch.py:348:main] Process 2129701 exits succ
essfully.
[2024-09-20 12:13:47,908] [INFO] [launch.py:348:main] Process 2129703 exits succ
essfully.
[2024-09-20 12:14:16,937] [INFO] [launch.py:348:main] Process 2129700 exits succ
essfully.

